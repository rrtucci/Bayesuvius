\chapter{Do Calculus proofs}
\label{ch-do-calc-proofs}

In Chapter \ref{ch-do-calc},
we explained Do Calculus,
but referred to this
chapter for proofs
of claims that
use Do Calculus.
In this chapter, we've
aggregated
 all proofs, from
throughout the book,
of claims that use Do Calculus.

Note that even though the 3
rules of Do Calculus
are great for proving
adjustment formulae
for general classes of DAGs,
they are sometimes overkill
for proving
 adjustment formulae
for a single specific DAG.
Indeed, since the
 3 rules of Do Calculus
are a consequence
of the d-separation theorem, it follows that
all adjustment
formulae should be
provable from first principles,
assuming only
the d-separation theorem
and the standard rules of
probability theory.
In this chapter, we present
a technique that allows us to do just that: prove 
adjustment formulae from first principles,
without using the 
3 Rules of Do Calculus.


In this chapter, we use the
 following conventions for bnet diagrams.

\bnetInstantiations

\hiddenNodes

Selection diagrams
with bnet-switches
 are discussed
in Chapter \ref{ch-transport}.
\selectionGraphs

We often write

\beq
\begin{array}{c}
P(a)\xymatrix {a\ar[r]&\sum b\ar[r]&c}
\quad=\quad
P(a) \xymatrix {a\ar[r]&c}
\\
\sum_b P(c|b)P(b|a)P(a)=P(c,a)
\label{eq-promi-dep}
\end{array}
\eeq
because  the left hand side summed over $a,c$ yields 1. However,
this equation
is only true if the $P(c,a)$
on the right hand side
assumes a particular promise, namely
\beq
P(c,b, a)=P(c|b)P(b|a)P(a)
\eeq
An alternative way of saying the same thing
is to say that Eq.(\ref{eq-promi-dep})
 assumes that $P()=P_G()$ where 
$G$ is the bnet $\rva\rarrow\rvb\rarrow\rvc$.
We say that Eq.(\ref{eq-promi-dep})
is {\bf bnet promise dependent}.


Next we will
give some {\bf node summation identities}
that are used in this chapter.
Note that these identities are {\bf bnet promise independent}; i.e., they are true
for any bnet promise. 

\hrule
\begin{enumerate}
\item

\beq
\sum_a
P(a|x_1, x_2)=1
\eeq

\beq
\xymatrix@R=.3pc{
x_1\ar[dr]
\\
&\sum a&=1
\\
x_2\ar[ur]
}
\label{eq-collider-sum}
\eeq
\hrule
\item
\beq
\sum_a P(y|a, x_1, x_2)P(a|x_1, x_2)P(x_2|x_1)P(x_1)
= P(y,x_1, x_2)
\;.
\eeq

\beq
\xymatrix{\\P(x_1)}
\xymatrix{
x_1\ar[rd]\ar[drr]\ar[dd]
\\
&\sum a\ar[r]
&y
\\
x_2\ar[ru]\ar[rru]
}
\xymatrix
{\\ =P(x_1)}
\xymatrix{
x_1\ar[rrd]\ar[dd]
\\
&&y
\\
x_2\ar[urr]
}
\label{eq-univ-bdoor}
\;.
\eeq
This identity can be interpreted
geometrically by noting that the 
bnet on the left hand side (LHS) is fully connected 
so one can reverse the direction of the 
arrow from $\sum a \rarrow y$ without changing the numerical value of the LHS. Then the sum over $a$
yields 1 so the $\sum a$ vertex 
with its 3 incoming arrows can be deleted.

The following identity has a similar geometrical interpretation

\beq
\sum_a P(y|x_2, a, x_1)P(x_2|a, x_1)P(x_1|a)P(a)
= P(y,x_1, x_2)
\;.
\eeq

\beq
\xymatrix{
x_1\ar[drr]\ar[dd]
\\
&\EE a\ar[r]\ar[ul]\ar[dl]
&y
\\
x_2\ar[rru]
}
\xymatrix
{\\ = P(x_1)}
\xymatrix{
x_1\ar[rrd]\ar[dd]
\\
&&y
\\
x_2\ar[urr]
}
\label{eq-univ-bdoor-ee}
\;.
\eeq


\hrule
\item
\beq
\sum_a P(y|a, x)P(a|x)P(x)=P(x,y)
\eeq

\beq
\xymatrix@R=1pc{
\\P(x)}
\xymatrix@R=1pc{
&\sum a\ar[rd]
\\
x\ar[ru]\ar[rr]
&&y
&=&P(x)\quad
x\ar[r]
&y}
\label{eq-med-sum}
\eeq
Once again, the bnet on the LHS is fully
connected so the arrow $\sum a\rarrow y$ can be reversed and then the sum over $a$ yields 1.

The following identity has a similar geometrical interpretation

\beq
\sum_a P(y|x,a)P(x|a)P(a)=P(x,y)
\eeq

\beq
\xymatrix@R=1pc{
&\EE a\ar[rd]\ar[ld]
\\
x\ar[rr]
&&y
&=&P(x)\quad
x\ar[r]
&y}
\label{eq-med-sum-ee}
\eeq

\hrule

\item
\beq
\sum_a P(y|a)P(a)=P(y)
\eeq

\beq
\xymatrix{
\EE a\ar[r]
&y
&=&P(y)
}
\label{eq-diff-priors}
\eeq
Furthermore,

\beq
\sum_a P(a|y)P(y)=P(y)
\eeq

\beq
\xymatrix{
P(y)\quad
\sum a
&y\ar[l]
&=&P(y)
}
\label{eq-diff-priors-ee}
\eeq

\end{enumerate}
\hrule

The moral of the above summation identities is that
if one has a fully connected bnet, or a bnet clique
(i.e., fully connected subgraph of a bnet)\footnote{Note that in order to use the node summation identities on a clique,
the clique's nodes can have NO incoming arrows (i.e., arrows pointing from outside the clique into the clique). Outgoing arrows are allowed though. I like to call such a clique an {\bf exploding clique}.},
one can remove completely (including its arrows) a sum vertex $\sum a$ or an average
vertex $\EE a$ of the clique. Not only that! We can also 
replace a sum vertex $\sum a$  of the clique by a different  sum vertex $\sum b$. Likewise, we can replace
an average vertex $\EE a$  of the clique by a different  average vertex $\EE b$.

%An important caveat concerning the above
%summation identities is that 
%the instantiations that they produce depend on the
%original bnet whence they come from. 
%This caveat is illustrated in Fig.\ref{fig-dep-on-origing-bnet}.
%This caveat is especial relevant when dealing 
%with do operators. In that case, the unamputated
%graph and the amputated graph are different bnets, with
%different promises.
%
%\begin{figure}[h!]
%$$
%\begin{array}{c|c}
%\sum_a P_{G}(x_2|a)P_{G}(a|x_1)=P_{G}(x_2|x_1)
%&
%\sum_a P_{G'}(x_2|a, x_1)P_{G'}(a|x_1)=P_{G'}(x_2|x_1)
%\\
%\xymatrix@C=1.5pc{
%x_1\ar[r]
%&\sum a\ar[r]
%&x_2
%&=&
%x_1\ar[r]
%&x_2}
%&
%\xymatrix@C=1.5pc{
%x_1\ar[r]\ar@/_1pc/[rr]
%&\sum a\ar[r]
%&x_2
%&=&
%x_1\ar[r]
%&x_2}
%\end{array}
%$$
%\caption{Note that there are cases when $P_G(x_2|x_1)
%\neq P_{G'}(x_2|x_1)$. Hence,
% $P(x_2|x_1)$ depends on the bnet of origin.
%}
%\label{fig-dep-on-origing-bnet}
%\end{figure}
%
%




We define a {\bf do-adjustment formula}
for {\bf do-query}
$P(y|\cald\rvx=x)$
to be an algebraic expression  
constructed from the TPMs for {\it observed} (i.e., not hidden) nodes of the original bnet $G$.
If a do-adjustment formula
exists for a
particular do-query,
then we say the do-query is
 {\bf do-identifiable (DI)}.
% \footnote{
%To prove that a do-query $P(y|\cald\rvx=x, z)$
%is do-identifiable
%for a graph $G$,
%just prove that $\rvy\perp\rvx|\rvz$
%in $\call_\rvx G$.
%This is called Rule 2 of Do Calculus,
%but it is easy to understand just from
%the d-separation theorem.
%Info can be transmitted
%between $\rvy$ and $\rvx$ by
%either (1) paths
%in $\cald_\rvx G$
%or (2) paths in $\call_\rvx G$.
%$P(y|\cald\rvx=x, z)=
%P(y|x, z)$
%means the info is being transmitted only
%by (1).
%So the Rule 2 premise is checking
%that no info is being transmitted
%by (2). }
A {\bf do-transport formula}
is a relationship between 2  do-queries.
This chapter deals with both
do-adjustment and do-transport
formulae.

Let $G$ be a graph before
amputation
of the arrows entering node $\rvx$,
and let $G_{amp(x)}=\cald_{\rvx=x}G$
be the same graph after amputation
at $\rvx$.
Also let $P_G()$ be
the promise
for graph $G$, and $P_{G_{amp(x)}}()$
be that for $G_{amp(x)}$.
In general, the
following is always true,
for bnets
with or bnets without hidden nodes:\footnote{Note that $P_{G_{amp(x)}}(y|x)\neq P_G(y|x)$
in general.
In fact,
$P_{G_{amp(x)}}(y|x)= P_G(y|x)$
iff there is no confounding,
so $P_{G_{amp(x)}}(y|x)\neq P_G(y|x)$
indicates confounding.
}

\beq
P_{G_{amp(x)}}(y|\cald\rvx=x)=P_{G_{amp(x)}}(y|x)\neq P_G(y|x)
\;
\eeq
However, $P_{G_{amp(x)}}(y|x)$ is
not a valid adjustment formula
for this query
because it's not expressed in terms
of observed TPMs of $G$.

To make matters even more confusing, let $G|x$
be the bnet $G$ conditioned on $\rvx=x$. If $G$ has promise $P_G()$,
then  $P_{G|x)}(\cdot) = P_G(\cdot|x)$.
 
Note that, in general,  $P_G$, $P_{G_{amp(x)}}$ and $P_{G|x}$ are
different probability distributions. 

IMPORTANT: For any adjustment formula, we assume
a hypothesis bnet $G_{hypo}$.
Then for a query
$P(y|\cald\rvx=x)$, we express the adjustment
formula in terms of the promise $P()=P_{G_{hypo}|x}()=P_{G_{hypo}}(\cdot|x)$.


%Note that
%it's always true that $P(y|\cald \rvx = x)=P(y|x)$.
%However, if the bnet contains more nodes than just $\rvx$ and $\rvy$, then the bnet is identifiable
%iff it can be stratified (i.e., 
%expressed as $P(y|x)=\sum_{z.}P(y|x, z.)P(z.)$
%for some observed multinode $\rvz.$).
%Thus, an alternative  name for identifiability is
%{\bf stratifiability}. Note also that there mightt be more than
%one adjustment formula, but they must all 
%be numerically equal to $P(y|x)$ for all $x$ and $y$.


The following 2 step heuristic technique appears to be equivalent to Pearl's 3 rules of Do Calculus.\footnote{Two rules to match his three :)  This heuristic technique was invented solely by me. I haven't proven that it is equivalent to Pearl's 
3 rules of Do Calculus. I believe it is,
but it might not be. It 
correctly produces the backdoor and frontdoor adjustment formulae.}

\begin{mdframed}[hidealllines=true,backgroundcolor=blue!10]

\begin{enumerate}
\item 
Write down a bnet instantiation
that has a DAG structure identical
to the DAG structure of $G$,
except that arrows entering node
$\rvx$ have been amputated.
All nodes  of that instantiation, except
nodes  $x$ and $y$,
are summed (indicated by $\sum$) or averaged
(indicated by $\EE$) over.



\item Use the node summation identities to
remove all sums and averages over hidden nodes.
Each sum or average over a hidden node may be 
removed completely or replaced by a sum or average over an observed node. 
\end{enumerate}
Perform these steps until you remove all sums and averages  over hidden variables 
and obtain a valid adjustment formula.
If such an outcome cannot be achieved,
declare the  query not-identifiable.

\end{mdframed}


\begin{figure}[h!]
$$
\begin{array}{ccc}
\xymatrix{
&\rvz\ar[ld]\ar[rd]
\\
\rvx\ar[rr]&&\rvy
}
&\quad\quad&
\xymatrix{
&*++[F-o]{\rvz}\ar[ld]\ar[rd]
\\
\rvx\ar[rr]&&\rvy
}
\\
(a) \text{$P(y|\cald\rvx=x)$ is DI}
&&(b) \text{$P(y|\cald\rvx=x)$ is not-DI}
\end{array}
$$
\caption{Examples of
bnets for which
the do-query $P(y|\cald\rvx=x)$
is
DI
and not-DI.
}
\label{fig-iden-noniden}
\end{figure}
See Fig.\ref{fig-iden-noniden}
for some simple
examples of
bnets for which
the do-query $P(y|\cald\rvx=x)$
is
DI
and not-DI.
Using our heuristic technique,
we can easily see why the
query $P(y|\cald\rvx=x)$
is DI
for bnet $(a)$ 
and not-DI for bnet $(b)$
of Fig.\ref{fig-iden-noniden}.

For bnet $(a)$, after amputating arrow
$z\rarrow x$ and averaging over node $z$,
we get

\beqa
P(y|\cald\rvx=x)
&=&
\xymatrix{
\EE z\ar[dr]
\\
x\ar[r]
&y
}
\label{eq-simplest-identifiable}
\eeqa
The right hand
side of Eq.(\ref{eq-simplest-identifiable})
is a valid
adjustment formula
because it's expressed in terms
of observed TPMs of $G$.

For bnet $(b)$,
if we amputate arrow
$z\rarrow x$
and average over node $z$,
we get

\beqa
P(y|\cald \rvx =x)
&=&
\xymatrix{
*++[F-o]{\EE z}\ar[rd]
\\
x\ar[r]
&y
}
\label{eq-simplest-not-DI}
\eeqa
The right hand
side of Eq.(\ref{eq-simplest-not-DI})
is not a valid
adjustment formula
because
if we replace $\EE z$ by $\EE x' $, we get 
$\sum_{x'}P(y|x, x')P(x')$,
and $P(y|x, x')$ is undefined.






\begin{claim}
\label{cl-decBackDoor}
\decBackDoor
\end{claim}

\proof

{\bf * proof 1:}
%\beq
%P(y|\cald\rvx=x, z)
%=
%\xymatrix{
%z\ar[dr]
%\\
%x\ar[r]
%&y
%}
%\eeq

\beq
P(y|\cald\rvx=x)
=
\xymatrix{
\EE z\ar[dr]
\\
x\ar[r]
&y
}
\eeq


{\bf * proof 2:}
\begin{longtable}{l}
\color{red}
$P(y|\cald\rvx=x)
=
\sum_z
P(y|\cald\rvx=x, z)
P(z|\cald\rvx=x)$
\\
\quad by Probability Axioms
\\
\color{red}
$=\sum_z
P(y|x, z)
P(z|\cald\rvx=x)$
\\
\quad $P(y|\cald \rvx=x, z)\rarrow
P(y|x, z)$
\\
\quad  by Rule 2:
\ruletwoshort
\\
\quad
$\rvy\perp\rvx|\rvz$ in
$\call_\rvx\cald_\emptyset G:
\xymatrix{
\rvz\ar[d]\ar[rd]
\\
\rvx
&\rvy
}$
\\
\color{red}
$=\sum_z
P(y|x, z)
P(z)$
\\
\quad $P(z|\cald \rvx=x)\rarrow
P(z)$
\\
\quad  by Rule 3:
\rulethreeshort
\\
\quad
$\rvz\perp \rvx$ in
$\cald_\rvx \cald_\emptyset G:
\xymatrix{
\rvz\ar[rd]
\\
\rvx\ar[r]
&\rvy
}
$
\end{longtable}
\qed




\begin{claim}
\label{cl-decFrontDoor}
\decFrontDoor
\end{claim}

\proof


{\bf * proof 1:}

\begin{longtable}{l}
$P(y|\cald\rvx=x) =\sum_c\sum_m P(y|m,c)P(m|x)P(c)$
\\
\hspace{2cm}
\xymatrix{
&\EE c\ar[dr]
\\
x\ar[r]&\sum m\ar[r]
&y
}
\\
\quad$=\sum_m \left[\sum_c\frac{1}{P(m|c)} P(y|m,c)P(m|c)P(c)\right]P(m|x)$
\\
\hspace{2cm}
\xymatrix{\\
\sum_m\sum_c\frac{P(c)}{P(m|c)}
}
\xymatrix{
&&c\ar[dr]\ar[d]
\\
x\ar[r]&m&m\ar[r]
&y
}
\\
\quad$=\sum_m \frac{1}{P(m)}\left[\sum_c P(y|m,c)P(m|c)P(c)\right]P(m|x)$ \\
\hspace{2cm}(because $P(m|c)=P(m)$ by d-separation
on $G_{hypo}|x$)
\\
\hspace{2cm}
\xymatrix{\\
\sum_m\frac{1}{P(m)}
}
\xymatrix{
&&\EE c\ar[dr]\ar[d]
\\
x\ar[r]&m&m\ar[r]
&y
}
\\
\quad$=\sum_m P(y|m)P(m|x)$ 
\\
\hspace{2cm}
(because square bracketed expression equals $P(y,m)$)
\\
\hspace{2cm}
\xymatrix{
x\ar[r]&\sum m \ar[r]& y
}\quad (this simplifies frontdoor AF)
\\
\quad
$=\sum_{x'}\sum_m P(y|m,x')P(m|x)P(x')$
\\
\hspace{2cm}
(because can reverse steps that led to last expression)
\\
\hspace{2cm}
\xymatrix{
&\EE x'\ar[dr]
\\
x\ar[r]&\sum m\ar[r]
&y
}
\end{longtable}

{\bf * proof 2:}
\begin{longtable}{l}
$\color{red}
P(y|\cald\rvx=x)=
\sum_m
P(y|\cald\rvx=x, m)
P(m|\cald\rvx=x)$
\\
\quad by Probability Axioms
\\
$\color{red}=
\sum_m
P(y|\cald\rvx=x, \cald\rvm=m)
P(m|\cald\rvx=x)$
\\
\quad $P(y|\cald\rvx=x, m)\rarrow
P(y|\cald\rvx=x, \cald m=m)$
\\
\quad by Rule 2:
\ruletwoshort
\\
\quad $\rvy\perp \rvm|\rvx$ in
$\call_\rvm\cald_\rvx G:$
$\xymatrix{
&*++[F-o]{\rvc}\ar[rd]
\\
\rvx\ar[r]
&\rvm
&\rvy
}$
\\
$\color{red}=
\sum_m
P(y|\cald\rvx=x, \cald\rvm=m)
P(m| x)$
\\
\quad $P(m|\cald\rvx=x)\rarrow P(m|x)$
\\
\quad by Rule 2:
\ruletwoshort
\\
\quad
$\rvm\perp \rvx$ in
$\call_\rvx\cald_\emptyset G:$
$\xymatrix{
&*++[F-o]{\rvc}\ar[ld]\ar[rd]
\\
\rvx
&\rvm\ar[r]
&\rvy
}$
\\
$\color{red}=
\sum_m
P(y|\cald\rvm=m)
P(m|x)$
\\
\quad $P(y|\cald\rvx=x, \cald\rvm=m)
\rarrow
 P(y|\cald\rvm=m)$
\\
\quad by Rule 3:
\rulethreeshort
\\
\quad
$\rvy\perp\rvx|\rvm$ in
$\cald_\rvx\cald_\rvm G:$
$\xymatrix{
&*++[F-o]{\rvc}\ar[rd]
\\
\rvx
&\rvm\ar[r]
&\rvy
}$
\\
$\color{red}=
\sum_{x'}
\sum_m
P(y|\cald\rvm=m, x')
P(x'|\cald\rvm=m)
P(m|x)$
\\
\quad by Probability Axioms
\\
$\color{red}=
\sum_{x'}
\sum_m
P(y|m, x')
P(x'|\cald\rvm=m)
P(m|x)$
\\
\quad $P(y|\cald\rvm=m, x')
\rarrow
\quad P(y|m, x')$
\\
\quad by Rule 2:
\ruletwoshort
\\
\quad
$\rvy\perp\rvm|\rvx$ in
$\call_\rvm \cald_\emptyset G:$
$\xymatrix{
&*++[F-o]{\rvc}\ar[rd]\ar[ld]
\\
\rvx\ar[r]
&\rvm
&\rvy
}$
\\
$\color{red}=
\sum_{x'}
\sum_m
P(y|m, x')
P(x')
P(m|x)$
\\
\quad $P(x'|\cald\rvm=m)
\rarrow
P(x')$
\\
\quad by Rule 3:
\rulethreeshort
\\
\quad
$\rvx\perp\rvm$ in
$\cald_\rvm \cald_\emptyset G:$
$\xymatrix{
&*++[F-o]{\rvc}\ar[rd]\ar[ld]
\\
\rvx
&\rvm\ar[r]
&\rvy
}$
\end{longtable}
\qed



\begin{claim}
\label{cl-decNapkin}
\decNapkin
\end{claim}
\proof
\\
\begin{align}
P(y|\cald\rvx=x)
&=
\xymatrix{
&*++[F-o]{\EE u_1}\ar[ddl]\ar[ddrr]
&&
\\
&*++[F-o]{\EE u_2}\ar[dl]
&&
\\
\sum w\ar[r]
&\sum z
&x\ar[r]
&y
}
\\
&=
\xymatrix{
*++[F-o]{\EE u_1}\ar[dr]
\\
x \ar[r]&  y}
\end{align}
The last diagram is not identifiable.
If we try to add  the arrow $P(x|u_1)$ 
in order to eliminate vertex $\EE u_1$, 
we run into problems,
because $P(x|u_1)\neq P(x)$ by d-separation on $G_{hypo}|x$.
Since the last diagram is not identifiable, the Napkin problem is not identifiable either.\footnote{Pearl claims, 
in his {\it The Book of Why} book, that the Napkin problem
is identifiable, but he doesn't give an adjustment formula for it there. I myself
have vacillated during the writing of this book on whether 
it is identifiable or not.}
\qed


\begin{claim}
\label{cl-decWhy}
\decWhy
\end{claim}
\proof
\beqa
P(y|\cald\rvz=z, x)
&=&
\xymatrix{
x\ar[d]\ar[drr]
\\
 \sum w
&z\ar[r]
&y
\\
*++[F-o]{\EE u}\ar[u]\ar[urr]
}
\\
&=&
\xymatrix{
x\ar[drr]
\\
&z\ar[r]
&y
\\
*++[F-o]{\EE u}\ar[urr]
}
\\
&=&
\xymatrix{
x\ar[drr]
\\
\EE a\ar@/_1pc/[rr]
&z\ar[r]
&y}
\eeqa
The last line follows because 
\beqa
\sum_u P(y|u,x,z)P(u)&=&
\sum_u P(y|u,x,z) P(u|x, z)
\\
&=&
P(y|x,z)
\\
&=&
\sum_a P(y|a,x,z) P(a|x, z)
\eeqa
where $a\in\{x,w,z\}$
\qed

\begin{claim}
\label{cl-decTransportTrivial}
\decTransportTrivial
\end{claim}
\proof
\begin{align}
P^*(y|\cald \rvx=x, z) &=
P(y|\cald\rvx=x, z, \rvs=1)
\\
&=
\left[\frac{1}{\sum_{u_1}P(z|u_1)P(u_1)}\right]
\xymatrix@R=.5pc{
\EE u_1\ar@{-->}[r]
&z\ar[rd]
&s=1\ar[d]
\\
x\ar[rr]
&&y
\\
&\EE u_2\ar@{-->}[ul]\ar@{-->}[ur]
}
\\
& \text{(the factor in square brackets is necessary for normalization)}\nonumber
\\
&=
\xymatrix@R=.5pc{
&z\ar[rd]
&s=1\ar[d]
\\
x\ar[rr]
&&y
\\
&\EE u_2\ar@{-->}[ul]\ar@{-->}[ur]
}
\\
&=
\xymatrix{
&z\ar[rd]
&s=1\ar[d]
\\
x\ar[rr]
&&y}
\\
&=
P^*(y|x,z)
\end{align}
Let $\xi=(x,z,s)$. Above, we used 
\begin{align}
\sum_{u_2}P(y|u_2,\xi)P(u_2)&=
\sum_{u_2}P(y|u_2,\xi)P(u_2|\xi)&=
P(y|\xi)
\;\text{(because $\rvu_2\perp \ul{\xi}$ in $G_{hypo}|x$)}
\end{align}

\qed

\begin{claim}
\label{cl-decTransportDirect}
\decTransportDirect
\end{claim}
\proof
\begin{align}
P^*(y|\cald x=x,z) &=
P(y|\cald x=x,z, \rvs=1)
\\
&=
\frac{1}{\sum_{u_1}P(z|s=1, u_1)P(u_1)}
\xymatrix@R=.2pc{
\EE u_1\ar@{-->}[dr]
\\
s=1\ar[r]
&z\ar[rd]
&
\\
x\ar[rr]
&&y
\\
&\EE u_2\ar@{-->}[ur]\ar@{-->}[ul]
}\\
& \text{(the factor in square brackets is necessary for normalization)}\nonumber
\\
&=
\xymatrix@R=.2pc{
&z\ar[rd]
&
\\
x\ar[rr]
&&y
\\
&\EE u_2\ar@{-->}[ur]\ar@{-->}[ul]
}
\\
&=
\xymatrix{
&z\ar[rd]
&
\\
x\ar[rr]
&&y
}
\end{align}
\qed

\begin{claim}
\label{cl-decTransportBox}
\decTransportBox
\end{claim}
\proof
\begin{align}
P^*(y|\cald x=x, a) &=
P(y|\cald x=x,a,\rvs=1)
\\
&=
\left[\frac {1}{P(a|z)P(z|s=1)P(z|u_1)P(u_1)}\right]
\xymatrix@R=.5pc{
\EE u_1\ar@{-->}[dr]
\\
s=1\ar[r]
&*++[F-o]{\sum z}\ar[r]
&a \ar[d]
\\
x\ar[rr]
&&y
\\
&\EE u_2\ar@{-->}[ur]\ar@{-->}[ul]
}
\\
&=
\xymatrix@R=.75pc{
&&a\ar[d]
&
\\
x\ar[rr]
&&y
\\
&\EE u_2\ar@{-->}[ur]\ar@{-->}[ul]
}
\\
&=
\xymatrix{
&&a\ar[d]
&
\\
x\ar[rr]
&&y
}
\end{align}
Finally, multiply both sides times $P^*(a)$
and sum over $a$.
\qed

%\decTransportOne merged
%with decDirectTransport

\begin{claim}
\label{cl-decTransportNon}
\decTransportNon
\end{claim}
\proof

Obvious
\qed


\begin{claim}.
\label{cl-decTransportTwo}
\decTransportTwo
\end{claim}
\proof
\beqa
P^*(y|\cald \rvx=x)
&=&P(y|\cald \rvx=x, \rvs=1)
\\
&& \nonumber
\\
&=&
\xymatrix@R=.5pc{
s=1\ar@/^1.5pc/[rrr]
&\EE u_1\ar@{-->}[r]
&*++[F-o]{\sum h}\ar[dr]\ar[r]
&\sum z
\\
&x\ar[rr]
&
&y\\\
&&\EE u_2\ar@{-->}[ur]\ar@{-->}[ul]
}
\\
&& \nonumber
\\
&=&
\xymatrix@R=.5pc{
&\EE u_1\ar@{-->}[r]
&*++[F-o]{\sum h}\ar[dr]
\\
&x\ar[rr]
&
&y\\\
&&\EE u_2\ar@{-->}[ur]\ar@{-->}[ul]
}\\
&=& \text{independent of $s$}
\eeqa
\qed
\begin{claim}
\label{cl-decTransportThree}
\decTransportThree
\end{claim}
\proof

\beqa
P^*(y|\cald \rvx=x)
&=&P(y|\cald \rvx=x, \rvs=1)
\\
&=&
\xymatrix@R=.5pc{
s=1\ar[drr]
&\EE u_1\ar@{-->}[r]
&*++[F-o]{\su h}\ar[dr]
\\
&x\ar[r]
&\sum z\ar[r]
&y\\
&&\EE u_2\ar@{-->}[ur]\ar@{-->}[ul]
}
\\
&=&
\xymatrix
{\\
\sum_z}
\xymatrix{
s=1\ar[dr]
\\
x\ar[r]
&z
}
\xymatrix@R=.5pc{
\EE u_1\ar@{-->}[r]
&*++[F-o]{\su h}\ar[dr]
\\
x
&z\ar[r]
&y\\
&\EE u_2\ar@{-->}[ur]\ar@{-->}[ul]
}
\\
&=&
\sum_z
P(y|\cald \rvx=x, \cald \rvz=z)P^*(z|x)
\eeqa
\qed

\begin{claim}
\label{cl-decMediationSimple}
\decMediationSimple
\end{claim}
\proof
Obvious.
\qed

\begin{claim}
\label{cl-decMediationPlus}
\decMediationPlus
\end{claim}
\proof
\beqa
P(y|\cald\rvd=d,\cali\rvd=d')&=&
\xymatrix{
&&*++[F-o]{\EE u}\ar[ddr]
\ar[dl]\ar[d]
\\
d'\ar@/_1pc/[rr]
&\sum\xi\ar[rrd]\ar[r]
&\sum m\ar[rd]
\\
d\ar[rrr]
&&&y}
\\
&=&
\xymatrix{
d'\ar@/_1pc/[rr]
&\EE\xi\ar[rrd]\ar[r]
&\sum m\ar[rd]
\\
d\ar[rrr]
&&&y}
\eeqa
\qed

\begin{claim}
\label{cl-decSeqBackDoor}
\decSeqBackDoor
\end{claim}
\proof

Obvious, once one realizes that 
\beq
[\cald \rvx^3 = x^3]=
\quad
[\cald\rvx_0=x_0 ,
\cald\rvx_1=x_1,
\cald\rvx_2=x_2]
\eeq
\qed

\begin{claim}
\label{cl-decSBBackDoor}
\decSBBackDoor
\end{claim}
\proof
\beqa
P(y|\cald\rvx =x)
&=& P(y|\cald\rvx =x, \rvs=0)
\\
&=&
\xymatrix{
\rvs=0\ar[r]\ar@/^1pc/[rr]
&\sum z^-\ar[d]\ar[r]
&\sum z^+
\\
x\ar[rru]\ar[r]
&y
}
\\
&=&
\xymatrix{
\rvs=1\ar[r]
&\sum z^-\ar[d]
\\
x\ar[r]
&y
}
\\
&=&
\xymatrix{
\rvs=1\ar[r]\ar@[red][rd]
&\sum z^-\ar[d]
\\
x\ar[r]
&y
}
\\
\nonumber
&&
\text{(because $P(y|x, z^-, s)=
P(y|x, z^-)$ in $G_{hypo}|x$)}
\\
&=&
\xymatrix{
\rvs=1\ar[r]\ar@[red][rd]
&\sum z^-\ar[d]
\\
x\ar[r]\ar@[red][u]\ar@[red][ur]
&y
}
\\
\nonumber
&&
\text{(because $P(s|x)=P(s)$
and $P(z^-|x)=P(z^-)$
in $G_{hypo}|x$)}
\\
&=&
\xymatrix{
\rvs=1\ar@[red][rd]
\\
x\ar[r]\ar@[red][u]
&y
}
\\
&=&
\xymatrix{
x\ar[r]
&y
}
\\
\nonumber
&&
\text{(because $P(y|x, s)=P(y|x)$
and $P(s|x)=P(s)$
in $G_{hypo}|x$)}
\eeqa
\qed


%\input{do-calc-proofs/do-calc-proofs-limbo.tex}
