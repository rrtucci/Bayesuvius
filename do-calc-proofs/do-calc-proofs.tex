\chapter{Do Calculus proofs}
\label{ch-do-calc-proofs}

In Chapter \ref{ch-do-calc},
we explained Do Calculus,
but referred to this
chapter for proofs
of claims that
use Do Calculus.
In this chapter, we've
aggregated
 all proofs, from
throughout the book,
of claims that use Do Calculus.

Note that even though the 3
rules of Do Calculus
are great for proving
adjustment formulae
for general classes of DAGs,
they are sometimes overkill
for proving
 adjustment formulae
for a single specific DAG.
Indeed, since the
 3 rules of Do Calculus
are a consequence
of the d-separation theorem, it follows that
all adjustment
formulae should be
provable from first principles,
assuming only
the d-separation theorem
and the standard rules of
probability theory.

In this chapter, we use the
 following conventions for bnet diagrams.

\bnetInstantiations

\hiddenNodes

Selection diagrams
with switch nodes
 are discussed
in Chapter \ref{ch-transport}.
\selectionGraphs

Some {\bf node summation identities}
that are used in this chapter:

\hrule
\begin{enumerate}
\item
\beq
P(y|x_1, x_2)=
\sum_a P(y|a, x_1, x_2)P(a|x_1, x_2)
\;.
\eeq

\beq
\xymatrix{
x_1\ar[rrd]
\\
&&y
\\
x_2\ar[urr]
}
\xymatrix{\\=}
\xymatrix{
x_1\ar[rd]\ar[drr]
\\
&\sum a\ar[r]
&y
\\
x_2\ar[ru]\ar[rru]
}
\label{eq-univ-bdoor}
\;.
\eeq
\hrule
\item

\beq
\sum_a
P(a|x_1, x_2)=1
\eeq

\beq
\xymatrix@R=.3pc{
x_1\ar[dr]
\\
&\sum a\ar[r]_0&&=1
\\
x_2\ar[ur]
}
\label{eq-collider-sum}
\eeq
A zeroed
arrow means the same as no arrow.
Eq.(\ref{eq-collider-sum})
can be understood as an
edge case (when $\rvy=\emptyset$)
of Eq.(\ref{eq-univ-bdoor}).
\hrule
\item
\beq
\sum_a P(x_2|a)P(a|x_1)=P(x_2|x_1)
\eeq

\beq
\xymatrix{
x_1\ar[r]
&\sum a\ar[r]
&x_2
&=&
x_1\ar[r]
&x_2}
\label{eq-med-sum}
\eeq
\hrule

\item
\beq
P(x)=\sum_a P(x|a)P(a)
\eeq

\beq
\xymatrix{
P(x)&=&
\ar[r]_0
&\EE a\ar[r]
&x
}
\label{eq-diff-priors}
\eeq
Eq.(\ref{eq-diff-priors})
can be understood as
an edge case of Eq.(\ref{eq-med-sum}).

\end{enumerate}
\hrule

An important caveat concerning the above
summation identities is that 
the instantiations that they produce depend on the
original bnet whence they come from. 
This caveat is illustrated in Fig.\ref{fig-dep-on-origing-bnet}.

\begin{figure}[h!]
$$
\begin{array}{c|c}
\sum_a P_{G}(x_2|a)P_{G}(a|x_1)=P_{G}(x_2|x_1)
&
\sum_a P_{G'}(x_2|a, x_1)P_{G'}(a|x_1)=P_{G'}(x_2|x_1)
\\
\xymatrix@C=1.5pc{
x_1\ar[r]
&\sum a\ar[r]
&x_2
&=&
x_1\ar[r]
&x_2}
&
\xymatrix@C=1.5pc{
x_1\ar[r]\ar@/_1pc/[rr]
&\sum a\ar[r]
&x_2
&=&
x_1\ar[r]
&x_2}
\end{array}
$$
\caption{Note that there are cases when $P_G(x_2|x_1)
\neq P_{G'}(x_2|x_1)$. Hence,
 $P(x_2|x_1)$ depends on the bnet of origin.
}
\label{fig-dep-on-origing-bnet}
\end{figure}

A {\bf do-adjustment
formula} expresses
a {\bf do-query} (i.e.,
a conditional probability
with do operators
in its condition) by
an equivalent expression
without do operators.
The equivalent expression must
satisfy 2 constraints
to be discussed  below.
If a do-adjustment formula
exists for a
particular do-query,
then we say the do-query is
 {\bf do-identifiable (DI)}.
% \footnote{
%To prove that a do-query $P(y|\cald\rvx=x, z)$
%is do-identifiable
%for a graph $G$,
%just prove that $\rvy\perp\rvx|\rvz$
%in $\call_\rvx G$.
%This is called Rule 2 of Do Calculus,
%but it is easy to understand just from
%the d-separation theorem.
%Info can be transmitted
%between $\rvy$ and $\rvx$ by
%either (1) paths
%in $\cald_\rvx G$
%or (2) paths in $\call_\rvx G$.
%$P(y|\cald\rvx=x, z)=
%P(y|x, z)$
%means the info is being transmitted only
%by (1).
%So the Rule 2 premise is checking
%that no info is being transmitted
%by (2). }
A {\bf do-transport formula}
is a relationship between 2  do-queries.
This chapter deals with both
do-adjustment and do-transport
formulae.

See Fig.\ref{fig-iden-noniden}
for some simple
examples of
bnets for which
the do-query $P(y|\cald\rvx=x)$
is
DI
and non-DI.

\begin{figure}[h!]
$$
\begin{array}{ccc}
\xymatrix{
&\rvz\ar[ld]\ar[rd]
\\
\rvx\ar[rr]&&\rvy
}
&\quad\quad&
\xymatrix{
&*++[F-o]{\rvz}\ar[ld]\ar[rd]
\\
\rvx\ar[rr]&&\rvy
}
\\
(a) \text{$P(y|\cald\rvx=x)$ is DI}
&&(b) \text{$P(y|\cald\rvx=x)$ is non-DI}
\end{array}
$$
\caption{Examples of
bnets for which
the do-query $P(y|\cald\rvx=x)$
is
DI
and non-DI.
}
\label{fig-iden-noniden}
\end{figure}

Let $G$ be a graph before
amputation
of the arrows entering node $\rvx$,
and let $G_x=\cald_{\rvx=x}G$
be the same graph after amputation.
Also let $P_G()=P()$ be
the full probability distribution
for graph $G$, and $P_{G_x}()$
be that for $G_x$.
In general, the
following is always true,
whether it applies to a bnet
with or without hidden nodes:\footnote{Note that $P_{G_x}(y|x)\neq P_G(y|x)=P(y|x)$.
In fact,
$P_{G_x}(y|x)= P(y|x)$
iff there is no confounding,
so $P_{G_x}(y|x)\neq P(y|x)$
indicates confounding.
}

\beq
P(y|\cald\rvx=x)=P_{G_x}(y|x)
\;
\eeq
However, the right hand side of this equation is
not a valid adjustment formula
for this query
because it's not expressed in terms
of $P()$.
We define a valid adjustment formula
for query
$P(y|\cald\rvx=x)$
to be a bnet instantiation diagram
that satisfies the
following 2 constraints:

\begin{enumerate}
\item {\bf(structural constraint)}

The adjustment formula must be
representable by a bnet instantiation
that has a DAG structure identical
to the DAG structure of $G$,
except that arrows entering node
$\rvx$ have been amputated.
All nodes  of that instatiation, except
nodes  $x$ and $y$,
must be summed over.

\item {\bf (probabilitistic constraint)}

\begin{itemize}
\item If $G$ has hidden nodes,
these must be renamed and
assigned a TPM that
can be constructed
from the {\it observable }
TPMs of $G$.
\item The observable
nodes of $G$
with hidden
parents, must also be assigned a TPM that
can be constructed
from the {\it observable }
TPMs of $G$.
\item The observable
nodes of $G$ with no hidden
parents, must be assigned
the same TPM as they have in $G$.
\end{itemize}
\end{enumerate}
The reason for
these 2 constraints
is that we want
an adjustment
formula to show
exactly how it is calculated from
the full probability distribution
$P()$.
If we don't show exactly
how it is calculated
(i.e., whether with or without
the amputated arrows),
it is impossible to distinguish
between marginals of $P()$ and $P_{G_x}()$
such as $P(y|x)$ and $P_{G_x}(y|x)$.



Based on these 2 constraints,
we can easily see why the
query $P(y|\cald\rvx=x)$
is DI (resp., non-DI)
for bnet $(a)$ (resp., bnet $(b)$)
of Fig.\ref{fig-iden-noniden}.
For bnet $(a)$, after amputating arrow
$z\rarrow x$ and summing over node $z$,
we get

\beqa
P(y|\cald\rvx=x)
&=&
\xymatrix{
\EE z\ar[dr]
\\
x\ar[r]
&y
}
\label{eq-simplest-identifiable}
\eeqa
The right hand
side of Eq.(\ref{eq-simplest-identifiable})
is a valid
adjustment formula
because it satisfies both constraints.
For bnet $(b)$,
if we amputate arrow
$z\rarrow x$
and sum over node $z$,
we get

\beqa
P(y|\cald \rvx =x)
&=&
\xymatrix{
*++[F-o]{\EE z}\ar[rd]
\\
x\ar[r]
&y
}
\label{eq-simplest-non-DI}
\eeqa
The right hand
side of Eq.(\ref{eq-simplest-non-DI})
is not a valid
adjustment formula
because it violates the
second constraint. Furthermore,
try as we may,  there
is no way to replace the sum over
hidden  node $z$ by a sum over an observed node,
such that
constraint 2 is satisfied.





\begin{claim}
\label{cl-decBackDoor}
\decBackDoor
\end{claim}

\proof

{\bf * proof 1:}
\beqa
P(y|\cald\rvx=x)
&=&
\xymatrix{
\EE z\ar[dr]
\\
x\ar[r]
&y
}
\eeqa


{\bf * proof 2:}
\begin{longtable}{l}
\color{red}
$P(y|\cald\rvx=x)
=
\sum_z
P(y|\cald\rvx=x, z)
P(z|\cald\rvx=x)$
\\
\quad by Probability Axioms
\\
\color{red}
$=\sum_z
P(y|x, z)
P(z|\cald\rvx=x)$
\\
\quad $P(y|\cald \rvx=x, z)\rarrow
P(y|x, z)$
\\
\quad  by Rule 2:
\ruletwoshort
\\
\quad
$\rvy\perp\rvx|\rvz$ in
$\call_\rvx\cald_\emptyset G:
\xymatrix{
\rvz\ar[d]\ar[rd]
\\
\rvx
&\rvy
}$
\\
\color{red}
$=\sum_z
P(y|x, z)
P(z)$
\\
\quad $P(z|\cald \rvx=x)\rarrow
P(z)$
\\
\quad  by Rule 3:
\rulethreeshort
\\
\quad
$\rvz\perp \rvx$ in
$\cald_\rvx \cald_\emptyset G:
\xymatrix{
\rvz\ar[rd]
\\
\rvx\ar[r]
&\rvy
}
$
\end{longtable}
\qed




\begin{claim}
\label{cl-decFrontDoor}
\decFrontDoor
\end{claim}

\proof


{\bf * proof 1:}
\\
\beqa
P(y|\cald\rvx=x,c)
&=&
\xymatrix{
&*++[F-o]{c}\ar[rd]
\\
x\ar[r]
&\sum m\ar[r]
&y
}
\eeqa
\beqa
P(y|\cald\rvx=x)
&=&
\xymatrix{
\EE x'\ar[r]
&*++[F-o]{\sum c}\ar[rd]
\\
x\ar[r]
&\sum m\ar[r]
&y
}
\\
&=&
\xymatrix{
&\EE x'\ar[dr]
\\
x\ar[r]&\sum m\ar[r]
&y
}
\eeqa


{\bf * proof 2:}
\begin{longtable}{l}
$\color{red}
P(y|\cald\rvx=x)=
\sum_m
P(y|\cald\rvx=x, m)
P(m|\cald\rvx=x)$
\\
\quad by Probability Axioms
\\
$\color{red}=
\sum_m
P(y|\cald\rvx=x, \cald\rvm=m)
P(m|\cald\rvx=x)$
\\
\quad $P(y|\cald\rvx=x, m)\rarrow
P(y|\cald\rvx=x, \cald m=m)$
\\
\quad by Rule 2:
\ruletwoshort
\\
\quad $\rvy\perp \rvm|\rvx$ in
$\call_\rvm\cald_\rvx G:$
$\xymatrix{
&*++[F-o]{\rvc}\ar[rd]
\\
\rvx\ar[r]
&\rvm
&\rvy
}$
\\
$\color{red}=
\sum_m
P(y|\cald\rvx=x, \cald\rvm=m)
P(m| x)$
\\
\quad $P(m|\cald\rvx=x)\rarrow P(m|x)$
\\
\quad by Rule 2:
\ruletwoshort
\\
\quad
$\rvm\perp \rvx$ in
$\call_\rvx\cald_\emptyset G:$
$\xymatrix{
&*++[F-o]{\rvc}\ar[ld]\ar[rd]
\\
\rvx
&\rvm\ar[r]
&\rvy
}$
\\
$\color{red}=
\sum_m
P(y|\cald\rvm=m)
P(m|x)$
\\
\quad $P(y|\cald\rvx=x, \cald\rvm=m)
\rarrow
 P(y|\cald\rvm=m)$
\\
\quad by Rule 3:
\rulethreeshort
\\
\quad
$\rvy\perp\rvx|\rvm$ in
$\cald_\rvx\cald_\rvm G:$
$\xymatrix{
&*++[F-o]{\rvc}\ar[rd]
\\
\rvx
&\rvm\ar[r]
&\rvy
}$
\\
$\color{red}=
\sum_{x'}
\sum_m
P(y|\cald\rvm=m, x')
P(x'|\cald\rvm=m)
P(m|x)$
\\
\quad by Probability Axioms
\\
$\color{red}=
\sum_{x'}
\sum_m
P(y|m, x')
P(x'|\cald\rvm=m)
P(m|x)$
\\
\quad $P(y|\cald\rvm=m, x')
\rarrow
\quad P(y|m, x')$
\\
\quad by Rule 2:
\ruletwoshort
\\
\quad
$\rvy\perp\rvm|\rvx$ in
$\call_\rvm \cald_\emptyset G:$
$\xymatrix{
&*++[F-o]{\rvc}\ar[rd]\ar[ld]
\\
\rvx\ar[r]
&\rvm
&\rvy
}$
\\
$\color{red}=
\sum_{x'}
\sum_m
P(y|m, x')
P(x')
P(m|x)$
\\
\quad $P(x'|\cald\rvm=m)
\rarrow
P(x')$
\\
\quad by Rule 3:
\rulethreeshort
\\
\quad
$\rvx\perp\rvm$ in
$\cald_\rvm \cald_\emptyset G:$
$\xymatrix{
&*++[F-o]{\rvc}\ar[rd]\ar[ld]
\\
\rvx
&\rvm\ar[r]
&\rvy
}$
\end{longtable}
\qed



\begin{claim}
\label{cl-decNapkin}
\decNapkin
\end{claim}
\proof
\\
\begin{align}
P(y|\cald\rvx=x, u_1, u_2)&=
\xymatrix{
&*++[F-o]{u_1}\ar[ddl]\ar[ddrr]
\\
&*++[F-o]{u_2}\ar[dl]
\\
\sum w\ar[r]
&\sum z
&x\ar[r]
&y
}
\\
P(y|\cald\rvx=x)
&=
\xymatrix{
&*++[F-o]{\sum u_1}\ar[ddl]\ar[ddrr]
&&\EE z'\ar[ll]
\\
&*++[F-o]{\sum u_2}\ar[dl]
&&
\EE x'\ar[ll]
\\
\sum w\ar[r]
&\sum z
&x\ar[r]
&y
}
\\
&=
\xymatrix{
&{\EE z'}\ar[ddl]\ar[ddrr]
\\
&{\EE x'}\ar[dl]
\\
\sum w\ar[r]
&\sum z
&x\ar[r]
&y
}
\\
&=
\xymatrix{
x \ar[r]&  y}
\end{align}
\qed


\begin{claim}
\label{cl-decWhy}
\decWhy
\end{claim}
\proof
\beqa
P(y|\cald\rvz=z,x, u)
&=&
\xymatrix{
x\ar[d]\ar[drr]
\\
\sum w
&z\ar[r]
&y
\\
*++[F-o]{u}\ar[u]\ar[urr]
}
\eeqa
\beqa
P(y|\cald\rvz=z,x)
&=&
\xymatrix{
x\ar[d]\ar[drr]
\\
\sum w
&z\ar[r]
&y
\\
*++[F-o]{\sum u}\ar[u]\ar[urr]
&\EE w'\ar[l]
}
\\
&=&
\xymatrix{
x\ar[d]\ar[drr]
\\
\sum w
&z\ar[r]
&y
\\
{\EE w'}\ar[u]\ar[urr]
}
\\
&=&
\xymatrix{
x\ar[drr]
\\
\EE w\ar@/_1pc/[rr]
&z\ar[r]
&y
}
\eeqa
\qed

\begin{claim}
\label{cl-decTransportTrivial}
\decTransportTrivial
\end{claim}
\proof
\begin{longtable}{l}
\color{red}
$P(y|\cald\rvx=x, z, \rvs=1)=
P(y|x, z, \rvs=1)$
\\
\xymatrix{
&z\ar[rd]
&\rvs=1\ar[d]
\\
\cald\rvx=x\ar[rr]
&&y
}
\xymatrix{
\\=
}
\xymatrix{
&z\ar[rd]
&\rvs=1\ar[d]
\\
x\ar[rr]
&&y
}
\end{longtable}
\qed

\begin{claim}
\label{cl-decTransportDirect}
\decTransportDirect
\end{claim}
\proof
\begin{longtable}{l}
\color{red}
$P(y|\cald\rvx=x, z, \rvs=1)=
P(y|\cald\rvx=x, z)$
\\
\xymatrix{
\rvs=1\ar[r]
&z\ar[dr]
&
\\
\cald\rvx=x\ar[rr]
&&y
}
\xymatrix{\\=}
\xymatrix{
&z\ar[dr]
&
\\
\cald\rvx=x\ar[rr]
&&y
}
\begin{tabular}{l}
Because $\rvs\perp\rvy|\rvz$
\end{tabular}
\end{longtable}
Furthermore,
\begin{longtable}{l}
\color{red}
$P(y|\cald \rvx=x, \rvs=1)
=\sum_z P(y|\cald\rvx=x,z)P(z|\rvs=1)$
\\
\xymatrix{
\rvs=1\ar[r]
&\sum z\ar[rd]
\\
\cald\rvx=x\ar[rr]
&&y
}
\end{longtable}
\qed

\begin{claim}
\label{cl-decTransportBox}
\decTransportBox
\end{claim}
\proof
\begin{longtable}{l}
\color{red}$
P(y|\cald\rvx=x, \rvs=1)
=\sum_{a}
P(y|\cald\rvx=x,a)P(a|\rvs=1)$
\\
\xymatrix{
\rvs=1\ar[r]
&*++[F-o]{\sum z}\ar[r]
&\sum a\ar[d]
\\
&\cald \rvx=x\ar[r]
&y
}\xymatrix{\\=}
\xymatrix{
\rvs=1\ar[r]
&\sum a\ar[d]
\\
\cald \rvx=x\ar[r]
&y
}
\end{longtable}
\qed

%\decTransportOne merged
%with decDirectTransport

\begin{claim}
\label{cl-decTransportNon}
\decTransportNon
\end{claim}
\proof
\begin{longtable}{l}
\color{red}
$P^*(y|\cald\rvx=x)=P^*(y|\cald\rvx=x)$
\\
\\
\xymatrix{
&*++[F-o]{\EE h}\ar[dr]
&\rvs=1\ar[d]
\\
\cald\rvx=x\ar[rr]
&&y
}
\xymatrix{\\=}
\xymatrix{
&&\rvs=1\ar[d]
\\
\cald\rvx=x\ar[rr]
&&y
}
\end{longtable}
Can't replace $\cald\rvx=x$
by $x$ because
$\rvy\not\perp \rvx$ in
$\call_\rvx G$.
Hence, Rule 2 not satisfied.
\qed


\begin{claim}
\label{cl-decTransportTwo}
\decTransportTwo
\end{claim}
\proof
\begin{longtable}{l}
\color{red}
$P(y|\cald \rvx=x, \rvs=1)=
\sum_h P(y|\cald\rvx=x, h)P(h)$
\\
\\
\xymatrix{
\rvs=1\ar@/^1.5pc/[rr]
&*++[F-o]{\EE h}\ar[r]\ar[rd]
&\sum z
\\
\cald\rvx=x\ar[rr]
&&y
}
\xymatrix{\\=}
\xymatrix{
&*++[F-o]{\EE h}\ar[rd]
&
\\
\cald\rvx=x\ar[rr]
&&y
}
\\
\color{red}
$=P(y|\cald\rvx=x)$
\\
\xymatrix{=}
\xymatrix{
\cald\rvx=x\ar[rr]
&&y
}
\end{longtable}
\qed
\begin{claim}
\label{cl-decTransportThree}
\decTransportThree
\end{claim}
\proof

\begin{longtable}{l}
\color{red}
$P(y|\cald\rvx=x, \rvs=1)=
\sum_h
\sum_z P(y|h, z)P(h)
P(z|\cald\rvx=x, \rvs=1)$
\\
\\
\xymatrix{
\rvs=1\ar[rd]
&*++[F-o]{\EE \rvh}\ar[dr]
\\
\cald\rvx=x\ar[r]
&\sum z\ar[r]
&y
}
\\
\\
\color{red}
$=\sum_h\sum_z P(y|h,z)P(h|\cald\rvx=x)
P(z|x, \rvs=1)$
\\
\\
\xymatrix{\\=}
\xymatrix{
\rvs=1\ar[rd]
&*++[F-o]{\sum \rvh}\ar[dr]
&\cald\rvx=x\ar[l]
\\
x\ar[r]
&\sum z\ar[r]
&y
}
\\
\color{red}
$=\sum_z P(y|\cald\rvx=x, z)P(z|x, \rvs=1)$
\\
$\xymatrix{\\=}\xymatrix{
\rvs=1\ar[rd]
&&\cald\rvx=x\ar[d]
\\
x\ar[r]
&\sum z\ar[r]
&y
}$
\end{longtable}
\qed

\begin{claim}
\label{cl-decMediationSimple}
\decMediationSimple
\end{claim}
\proof
\begin{longtable}{l}
\color{red}
$P(y|\cald\rvd=d,\cali\rvd=d')=
\sum_{m}
P(y|d,m)P(m|d')$
\\
\\
\xymatrix{
\cali\rvd=d'\ar[r]
&\sum m\ar[rd]
\\
\cald\rvd=d\ar[rr]
&&y}
\end{longtable}
\qed

\begin{claim}
\label{cl-decMediationPlus}
\decMediationPlus
\end{claim}
\proof
\begin{longtable}{l}
\color{red}
$P(y|\cald\rvd=d,\cali\rvd=d')=
\sum_{\xi, u}
\sum_{m}
P(y|d,m,\xi, u)P(m|d', \xi, u)
\underbrace{P(\xi|u)P(u)}_{P(\xi, u)}$
\\
\\
\xymatrix{
&&*++[F-o]{\EE u}\ar[ddr]
\ar[dl]\ar[d]
\\
\cali\rvd=d'\ar@/_1pc/[rr]
&\sum\xi\ar[rrd]\ar[r]
&\sum m\ar[rd]
\\
\cald\rvd=d\ar[rrr]
&&&y}
\\
\color{red}
$=
\sum_{\xi}
\sum_{m}
P(y|d,m,\xi)P(m|d', \xi)
P(\xi)$
\\
\xymatrix{\\=}
\xymatrix{
\cali\rvd=d'\ar@/_1pc/[rr]
&\EE\xi\ar[rrd]\ar[r]
&\sum m\ar[rd]
\\
\cald\rvd=d\ar[rrr]
&&&y}
\begin{tabular}{l}
We switch from averaging over\\ the
prior of $\xi, u$\\
to averaging over the\\
prior of $\xi$.
\end{tabular}
\end{longtable}
\qed

\begin{claim}
\label{cl-decSeqBackDoor}
\decSeqBackDoor
\end{claim}
\proof
\begin{longtable}{l}
\color{red}
$P(y|\cald\rvx^3=x^3)=
\calq(y|x^3)$
\\
\xymatrix@C=.5pc{
\EE z_0\ar[r]\ar@/^1pc/[rr]
\ar[drrr]
&\sum z_1\ar[r]
\ar[drr]
&\sum z_2
\ar[dr]
\\
&&&y
\\
\cald\rvx_0=x_0\ar[uur]\ar[uurr]
\ar[urrr]
&\cald\rvx_1=x_1\ar[uur]
\ar[urr]
&\cald\rvx_2=x_2
\ar[ur]
}
\xymatrix{\\=}
\xymatrix@C=.9pc{
\EE z_0\ar[r]\ar@/^1pc/[rr]
\ar[drrr]
&\sum z_1\ar[r]
\ar[drr]
&\sum z_2
\ar[dr]
\\
&&&y
\\
x_0\ar[uur]\ar[uurr]
\ar[urrr]
&x_1\ar[uur]
\ar[urr]
&x_2
\ar[ur]
}
\begin{tabular}{l}
We can replace\\
$\cald\rvx_i=x_i$
by $x_i$
\\once all nodes
\\in bnet are
\\observed nodes.
\end{tabular}
\end{longtable}
\qed

\begin{claim}
\label{cl-decSBBackDoor}
\decSBBackDoor
\end{claim}
\proof
\begin{longtable}{l}
\color{red}
$P(y|\cald\rvx =x, \rvs=1)=
\sum_z
P(y|\cald\rvx=x, z)P(z^{<\rvx}|\rvs=1)
P(z^{>\rvx}|x,z^{<\rvx}, \rvs=1)$
\\
\\
\xymatrix{
\rvs=1\ar[r]\ar@/^1pc/[rr]
&\sum z^{<\rvx}\ar[d]\ar[r]
&\sum z^{>\rvx}
\\
\cald\rvx=x\ar[rru]\ar[r]
&y
}
\\
\color{red}
$= \sum_{z^{<\rvx}} P(y|\cald\rvx=x, z^{<\rvx})
P(z^{<\rvx}|\rvs=1)$
\\
\xymatrix{\\=}
\xymatrix{
\rvs=1\ar[r]
&\sum z^{<\rvx}\ar[d]
&
\\
\cald\rvx=x\ar[r]
&y
}
\\
\color{red}
$= \sum_z P(y|x, z)
P(z|\rvs=1)$
\\
\xymatrix{\\=}
\xymatrix{
\rvs=1\ar[r]
&\sum z\ar[d]
&
\\
x\ar[r]
&y
}
\begin{tabular}{l}
$\cald$ can be removed because there are\\
no sums over unobserved nodes.
\end{tabular}
\\
\color{red}
$= \sum_z P(y|x, z)
P(z)$
\\
\xymatrix{\\=}
\xymatrix{
&\EE z\ar[d]
&
\\
x\ar[r]
&y
}
\begin{tabular}{l}
$\rvs=1$ node can be removed\\ because
this expression must\\ equal
$P(y|x, \rvs=1)$. Furthermore,\\
$\rvy\perp\rvs|(\rvx,\rvz)$
in the hypothesis bnet.\\
Hence,  this expression must also
\\ equal $P(y|x)$.
\end{tabular}
\end{longtable}
\qed


%\input{do-calc-proofs/do-calc-proofs-limbo.tex}
