\chapter{Basic Curve Fitting
Using Gradient Descent}
\label{ch-basic-fit}

\begin{figure}[h!]
\centering
$$\xymatrix{
&\vec{\rvx}\ar[d]\ar[r]&\ul{\vecy}
\ar[d]&\\
\phi\ar[r]\ar@/_1pc/[rrr]&
\vec{\hat{\rvy}}\ar[r]&\cale\ar[r]&\phi'
}$$
\caption{Basic curve fitting bnet.}
\label{fig-bfit}
\end{figure}


Samples 
$(x^\s, y^\s)\in S_\rvx\times S_\rvy$
are given. $N(\vecx)=nsam(\vecy)$.

Estimator function 
$\haty(x; \phi)$
for $x\in S_\rvx$ and $\phi\in\RR$
is given.

Let 
\beq
P_{\rvx, \rvy}(x,y)=
\frac{1}{nsam(\vecx)}
\sum_\s \indi(x=x^\s, y=y^\s)
\;.
\eeq


Let 
\beq
\cale(\vecx, \vecy, \phi)=
\frac{1}{nsam(\vec{y})}
\sum_\s
|y^\s-\haty(x^\s; \phi)|^2
\;
\eeq
$\cale$ is called the mean square error.

Best fit is parameters $\phi^*$
such that

\beq 
\phi^*= \argmin_\phi
\cale(\vecx, \vecy, \phi)
\;.
\eeq

The TPMs, printed in blue, for
the basic curve fitting bnet
 Fig.\ref{fig-bfit}, are
as follows.

\beq\color{blue}
P(\phi) \text{ = given}
\;.
\eeq
The first time
it is used, $\phi$ is arbitrary.
After the first time, it is determined 
by previous stage.

\beq\color{blue}
P(\vecx)=\prod_\s P_\rvx(x^\s)
\eeq

\beq\color{blue}
P(\vecy|\vecx)=\prod_\s P_{\rvy|\rvx}(y^\s\cond x^\s)
\eeq

\beq\color{blue}
P(\haty^\s|\phi, \vecx)=
\delta(\haty^\s, \haty(x^\s;\phi))
\eeq


\beq\color{blue}
P(\cale|\vec{\haty}, \vecy)=
\delta(\cale,\frac{1}{nsam(\vecx)}
\sum_\s |y^\s-\haty^\s|^2)
\;.
\eeq


\beq\color{blue}
P(\phi'|\phi, \cale)=
\delta(\phi',
\phi-\eta\partial_\phi\cale)
\eeq
$\eta>0$ is the descent rate.
If $\Delta \phi=\phi'-\phi=-\eta 
\frac{\partial\cale}
{\partial \phi}$, then
 $\Delta \cale=\frac{-1}{\eta}
(\Delta\phi)^2<0$  so this will
minimize the error
$\cale$.
This is called ``gradient descent".