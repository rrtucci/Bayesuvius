\chapter{Basic Curve Fitting
Using Gradient Descent}
\label{ch-basic-fit}

\begin{figure}[h!]
\centering
$$\xymatrix{
&\vec{\rvx}\ar[d]\ar[r]&\ul{\vecy}
\ar[d]&\\
\phi\ar[r]\ar@/_1pc/[rrr]&
\vec{\hat{\rvy}}\ar[r]&\cale\ar[r]&\phi'
}$$
\caption{Basic curve fitting bnet.}
\label{fig-bfit}
\end{figure}


Samples 
$(x[\sigma], y[\sigma])\in S_\rvx\times S_\rvy$
are given. $nsam(\vecx)=nsam(\vecy)$.

Estimator function 
$\hat{y}(x; \phi)$
for $x\in S_\rvx$ and $\phi\in\RR$
is given.

Let 
\beq
P_{\rvx, \rvy}(x,y)=
\frac{1}{nsam(\vecx)}
\sum_\sigma \indi(x=x[\sigma], y=y[\sigma])
\;.
\eeq


Let 
\beq
\cale(\vecx, \vecy, \phi)=
\frac{1}{nsam(\vec{y})}
\sum_\sigma
|y[\sigma]-\hat{y}(x[\sigma]; \phi)|^2
\;
\eeq
$\cale$ is called the mean square error.

Best fit is parameters $\phi^*$
such that

\beq 
\phi^*= \argmin_\phi
\cale(\vecx, \vecy, \phi)
\;.
\eeq

The node TPMs for
the basic curve fitting bnet
 Fig.\ref{fig-bfit} are
printed below in blue.

\beq\color{blue}
P(\phi) \text{ = given}
\;.
\eeq
The first time
it is used, $\phi$ is arbitrary.
After the first time, it is determined 
by previous stage.

\beq\color{blue}
P(\vecx)=\prod_\sigma P_\rvx(x[\sigma])
\eeq

\beq\color{blue}
P(\vecy|\vecx)=\prod_\sigma P_{\rvy|\rvx}(y[\sigma]\cond x[\sigma])
\eeq

\beq\color{blue}
P(\hat{y}[\sigma]|\phi, \vecx)=
\delta(\hat{y}[\sigma], \hat{y}(x[\sigma];\phi))
\eeq


\beq\color{blue}
P(\cale|\vec{\hat{y}}, \vecy)=
\delta(\cale,\frac{1}{nsam(\vecx)}
\sum_\sigma |y[\sigma]-\hat{y}[\sigma]|^2)
\;.
\eeq


\beq\color{blue}
P(\phi'|\phi, \cale)=
\delta(\phi',
\phi-\eta\partial_\phi\cale)
\eeq
$\eta>0$ is the descent rate.
If $\Delta \phi=\phi'-\phi=-\eta 
\frac{\partial\cale}
{\partial \phi}$, then
 $\Delta \cale=\frac{-1}{\eta}
(\Delta\phi)^2<0$  so this will
minimize the error
$\cale$.
This is called ``gradient descent".