\chapter{Diffusion Models (COMING SOON)}
\label{ch-diffusion}
\newcommand{\prodalp}[0]{\pi_1^t\alp}

This paper is based on
 Ref.\cite{weng-diffusion-model}

Diffusion Model (DM)

\section{Bnet for DM}

For $t\in 1,2, \ldots , T$, let
\beq
0< \alp^t\
< 1, \; \beta^t= 1-\alp^t
\eeq

\beq
\prodalp= \prod_{\tau=1}^t \alp^\tau
\eeq

Fig.\ref{fig-diffusion1} shows a
bnet for DM, and  
\ref{fig-diffusion2}
shows the same bnet 
in more detail.\footnote{
Ref.\cite{weng-diffusion-model} uses $\rvz^{t}$ 
instead of $\rvw^{t}$
for white noise.
Note that 
the time index
of 
$\rvw^{t}$
often does not matter
because 
$\rvw^{t}\sim \caln(0, I)$
for all $t$.
This does not mean
that we can drop the
time index of $\rvw^t$,
because $\rvw^{t_1}$
and $\rvw^{t_2}$
are uncorrelated for $t_1\neq t_2$,
so we can get into 
trouble if we assume
$\rvw^{t_1}=\rvw^{t_2}$ .
}

\begin{figure}[h!]
$$
\xymatrix@C=5pc@R=3pc{
\\
\text{Original}
\\
\text{Fake}
}
\xymatrix@C=5pc@R=3pc{
\rvw^0\ar[dr]^{\sqrt{\beta^1}}
&\rvw^1\ar[dr]^{\sqrt{\beta^2}}
&\rvw^2\ar[dr]^{\sqrt{\beta^3}}
\\
\rvx^0\ar[r]_{\sqrt{\alp^1}}
&\rvx^1\ar[r]_{\sqrt{\alp^2}}
&\rvx^2\ar[r]_{\sqrt{\alp^3}}
&\rvx^3(= \TIL{\rvx}^3)\ar[dl]
\\
\TIL{\rvx}^0
&\TIL{\rvx}^1\ar[l]
&\TIL{\rvx}^2\ar[l]
&
}
\xymatrix@C=5pc@R=3pc{
\\
\text{Normal Dist.}
}
$$
\caption{Bnet for DM with $T=3$.}
\label{fig-diffusion1}
\end{figure}


The TPMs, printed in blue,
for the bnet of Fig.\ref{fig-diffusion1}
are as follows:

\beq \color{blue}
P(x^0)=\delta(x^0, X^0)
\quad \text{ (original image) }
\eeq

\beq \color{blue}
P(x^t|x^{t-1}, w^{t})=
\indi(\quad x^t=\sqrt{\alp^t}\;x^{t-1}
+\sqrt{\beta^t}\;w^{t-1}\quad)
\eeq

\beq \color{blue}
P(w^t) = \caln(w^t; \mu=0,
 \s^2 =  I ) \quad \text{(white noise)}
\eeq

\beqa \color{blue}
P(\TIL{\rvx}^{t-1}=x^{t-1}|\TIL{\rvx}^{t}=x^{t})&=&\color{blue}
\tilPT(x^{t-1}|x^{t})
\\
&=&\color{blue}
\caln(x^{t-1}; \mu=M^{t-1}_\theta(x^t),
\s^2 =\Sigma_\theta^{t-1}(x^t))
 \eeqa
We will assume,
that 

\beq
\Sigma_\theta^{t-1}(x^t)
=
(\s^{t-1})^2 I
\eeq
where $\s^{t-1}\in \RR$.
 
 \begin{figure}[h!]
 $$
 \xymatrix@C=4pc@R=3pc{
 \rvw^0\ar[dr]^{\sqrt{\beta^1}}
 &\rvw^1\ar[dr]^{\sqrt{\beta^2}}
 &\rvw^2\ar[dr]^{\sqrt{\beta^3}}
 \\
 \rvx^0\ar[r]_{\sqrt{\alp^1}}
 &\rvx^1\ar[r]_{\sqrt{\alp^2}}
 &\rvx^2\ar[r]_{\sqrt{\alp^3}}
 &\rvx^3(= \TIL{\rvx}^3)\ar[ddl]\ar[dddl]
 \\
 \TIL{\rvx}^0
 &\TIL{\rvx}^1\ar[dl]\ar[ddl]
 &\TIL{\rvx}^2\ar[dl]\ar[ddl]
 \\
 \rva^0\ar[u]^{1}
 &\rva^1\ar[u]^{1}
 &\rva^2\ar[u]^{1}
 \\
 \rvn_\theta^0\ar@/_1pc/[uu]_{-1}
 &\rvn_\theta^1\ar@/_1pc/[uu]_{-1}
 &\rvn_\theta^2\ar@/_1pc/[uu]_{-1}
 }
 $$
 \caption{The bnet of Fig.\ref{fig-diffusion1}
 shown in more detail.}
 \label{fig-diffusion2}
 \end{figure}
 
 
 The TPMs, printed in blue,
 for the bnet of Fig.\ref{fig-diffusion2}
 are as follows:
 \beq \color{blue}
 P(n_\theta^t) = \caln(n_\theta^t; \mu=M^t_\theta(x^{t+1}),
  \s =  \s^t )
 \eeq
 
\beq \color{blue}
 P(a^t|\TIL{\rvx}^{t+1}=x^{t+1}) = \indi(\quad
 a^t= M^t(x^{t+1})
 \quad)
 \eeq
 
\beq \color{blue}
 P(\TIL{\rvx}^t=x^t|
 a_t,
 n_\theta^t
 ) = \indi(\quad
 x^t= 
 a^t-
  n_\theta^t
 \quad)
 \eeq
 
 Note that these TPMs for the
 bnet Fig.\ref{fig-diffusion2}
 imply that
 \beq
\underbrace{ P(
\TIL{\rvx}^t=x^t|
 \TIL{\rvx}^{t+1}=x^{t+1}
 )
 }_{ \text{call this }\tilPT(x^t|x^{t+1})}
 =\quad
 \caln(x^t;
 \mu= M^t(x^{t+1})
 -M^t_\theta(x^{t+1}),
 \s=\s^t)
 \eeq
 
 \section{Mean Values $M^{t-1}(x^t)$ 
 and $M^{t-1}_\theta(x^t)$ }
 \begin{claim}
 \beq
 \boxed{
 \rvx^t = \sqrt{\prodalp}\; \rvx^{0}
 +\sqrt{1-\prodalp}\;\rvw^t}
 \label{eq-xt-x0-w}
 \eeq
 
\beq
 \xymatrix@C=5pc@R=3pc{
 &\rvw^t\ar[d]^{\sqrt{1-\prodalp}}
 \\
 \rvx^0\ar[r]_{\sqrt{\prodalp}}
 & \rvx^t
 }
 \eeq

 \end{claim}
 \proof
 
 Suppose $\rvx_1$ and $\rvx_2$
 are independent
 random variables with
 variances $V_1$ and $V_2$,
 respectively.
 Then the variance $V$ of 
 $\rvx=\rvx_1+\rvx_2$
 is 
 
 \beqa
 V &=& \av{\rvx,\rvx}
 \\
 &=& 
 \av{\rvx_1 + \rvx_2,\rvx_1 + \rvx_2}
 \\
 &=&
 \av{\rvx_1, \rvx_1}
 +
 \av{\rvx_2, \rvx_2}
 \\
 &=&
 V_1 + V_2
 \eeqa
 By similar reasoning, the 
 mean of $\rvx$
 equals the sum
 of the means of $\rvx_1$
 and $\rvx_2$.
 It's also true
 that if both $\rvx_1$
 and $\rvx_2$  are normally
 distributed, then $\rvx$ is too.
 We will refer to a sum of independent normals as a SIN.
 
 Let $\rvw \sim \caln(0,I)$.
 
 
 \beqa
 \rvx^t 
 &=& 
 \sqrt{\alp^t} \rvx^{t-1}
 + \sqrt{1-\alp^t}\;\rvw^{t-1}
 \\
 &=&
 \sqrt{\alp^t} 
 [\sqrt{\alp^{t-1}} \rvx^{t-2}
  + \sqrt{1-\alp^{t-1}}\;\rvw^{t-2}]
  + \sqrt{1-\alp^t}\;\rvw^{t-1}
  \\
  &=&
\sqrt{\alp^t\alp^{t-1}}\; \rvx^{t-2}
+ 
[ \sqrt{\alp^t(1-\alp^{t-1})}\;\rvw^{t-2}
  + \sqrt{1-\alp^t}\;\rvw^{t-1}]
  \\
  &=&
 \sqrt{\alp^t\alp^{t-1}}\; \rvx^{t-2}
 + 
 \sqrt{1-\alp^t\alp^{t-1}}\;\rvw
 \quad \text{(because it's a SIN)}
 \\
 &=& \cdots
 \\
 &=&
 \sqrt{\prodalp}\; \rvx^{0}
  + 
  \sqrt{1-\prodalp}\;\rvw
 \eeqa
 Now replace
 $\rvw$ by $\rvw^t$.
 This is justified
 because they are 
 both $\caln(0,I)$,
 and,
 when $t_1\neq t_2$,  we want the
 $\rvw$ for $\rvx^{t_1}$
 to
 be independent
 from the $\rvw$
 for $\rvx^{t_2}$.
 
 \qed
 
  Solving Eq.(\ref{eq-xt-x0-w}) 
  for $\rvx^0$, we get
  \beq
  \boxed{
  \rvx^0 = \frac{1}{\sqrt{\prodalp}}
  \rvx^t
  -
  \frac{\sqrt{1-\prodalp}}
  {\sqrt{\prodalp}}\rvw^t
  }
  \label{eq-x0-xt-w}
  \eeq

 \beq
 \xymatrix@C=5pc@R=3pc{
 &\rvw^t\ar[dl]
 _{\frac{\sqrt{1-\prodalp}}
   {\sqrt{\prodalp}}}
 \\
 \rvx_0
 & \rvx^t\ar[l]^{\frac{1}{\sqrt{\prodalp}}}
 }
 \eeq
  
 
 \begin{claim}\label{cl-diff-bayes}
 \beq
 P(x^{t-1}|x^{t}, x^0)=
 P(x^t|x^{t-1})
 \frac
 {P(x^{t-1}|x^0)}
 {P(x^t|x^0)}
 \eeq
 \end{claim}
 \proof
 \beq
 P(x^{t-1}|x^{t}, x^0)P(x^t|x^0)
 =
 \overbrace{
  P(x^t|x^{t-1}, x^0)}
  ^{P(x^t|x^{t-1})}P(x^{t-1}|x^0)
 \eeq
 \beq
 \begin{array}{l}
 \xymatrix{
 &\sum w^0\ar[d]
 &\cdots
 &\sum w^{t-2}\ar[d]
 &\sum w^{t-1}\ar[d]
 \\
 x^0\ar[r]
 &\sum x^1\ar[r]
 &\cdots\ar[r]
 &x^{t-1}\ar[r]
 &x^t
 }
 \\
 \\
 =
 \xymatrix{
 x^0\ar[r]
 &x^{t-1}\ar[r]
 &x^{t}
 }\text{(diagram A)}
  \\
  \\
  =
  \xymatrix{
  x^0\ar[r]\ar@/_1pc/[rr]_0
  &x^{t-1}\ar[r]
  &x^{t}
  }\quad\text{(make fully connected)}
 \\
 \\
 =
 \xymatrix{
 x^0\ar[r]\ar@/_1pc/[rr]
 &x^{t-1}
 &x^{t}\ar[l]
 }\quad\text{(reverse arrow)  (Diagram B)}
 \end{array}
 \label{eq-diff-bayes}
 \eeq
 
 \beq
 \underbrace{P(x^{t-1}|x^{t}, x^0)P(x^t|x^0)
 }_{\text{diagram B of Eq.\ref{eq-diff-bayes}}}
 =
\underbrace{ P(x^t|x^{t-1})P(x^{t-1}|x^0)
 }_{\text{diagram A of Eq.\ref{eq-diff-bayes}}
 }
 \eeq
 \qed
 
 \beq
 M^{t-1}(x^t)=
 \sum_{x^{t-1}}
 x^{t-1}P(x^{t-1}|x^t, x^0)
 \eeq
 
 \begin{claim}
 \label{cl-m-t-1}
 \beq
 \boxed{
 M^{t-1}(x^t)
 =
 \frac{\sqrt{\alp^t}
 (1-\pi_1^{t-1}\alp)}
 {1-\prodalp}
 x^t
 +
 \frac{\sqrt{\pi_1^{t-1}\alp}\;\beta^t}
  {1-\prodalp}
  x^0
  }
  \label{eq-m-t-1}
 \eeq
 \end{claim}
 \proof
 
 By Claim \ref{cl-diff-bayes},
 we know that $\calq =P(x^{t-1}|x^t, x^0)$
 can be expressed as a product
 of two Gaussians
 $P(x^t|x^{t-1})$
 and $P(x^{t-1}|x^0)$,
 divided by a third Gaussian
 $P(x^t|x^0)$,
 and all 3 of
 these Gaussians
 have been calculated 
 in closed form
 previously
in this chapter. So
it's just a matter
of algebra to express
$\calq$ as a Gaussian,
complete the square
inside the exponent
of $\calq$,
and thus obtain its mean value 
$M^{t-1}(x^t)$.
We leave the algebra to the
reader. The algebra is also found 
in Ref.\cite{weng-dif-mod}.
 \qed
 


\begin{claim}
\beq
\boxed{
M^{t-1}(x^t)=
 \frac{1}{\sqrt{\alp^t}}
 \left(
 x^t-
 \frac{\beta^t}
 {\sqrt{1-\prodalp}}
 w^t
 \right)
 }\label{eq-m-t-1-xt}
 \eeq
 \end{claim}
 \proof
 
 Use Eq.(\ref{eq-x0-xt-w})
 to replace $x^0$ in 
 Eq.(\ref{eq-m-t-1}).
 \qed


 
 Parameterize $M^{t-1}_\theta(x^t)$ by
 
 \beq
 \boxed{
 M^{t-1}_\theta(x^t)=
  \frac{1}{\sqrt{\alp^t}}
  \left(
  x^t-
  \frac{\beta^t}
  {\sqrt{1-\prodalp}}
  n^{t-1}_\theta(x^t)
  \right)}
 \eeq
 
 Let
 \beq
 C^t=
 \frac{-\beta^t}
   {\sqrt{\alp^t(1-\prodalp)}}
   \eeq
 Then 
 
 \begin{align}
 M^{t-1}(x^t)- M^{t-1}_\theta(x^t)
 &=
C^t
[w^t-n^{t-1}_\theta(x^t)]
\\
&=
C^t
[w^t-n^{t-1}_\theta(
\sqrt{\prodalp}\; x^{0}
 +\sqrt{1-\prodalp}\;w^t
)]
\label{eq-diff-M}
\end{align}

\section{Loss function ${\cal L}$}
\beqa
0&\leq&
 D_{KL}(P(x^{1:T}|x^0)
\parallel \tilPT(x^{1:T}|x^0))
\\
&=&
\sum_{x^{1:T}}
P(x^{1:T}|x^0)
\ln
\frac{P(x^{1:T}|x^0)}
{\tilPT(x^{1:T}|x^0)}
\\
&=&
\ln \tilPT(x^0)
+
\sum_{x^{1:T}}
P(x^{1:T}|x^0)
\ln
\frac{P(x^{1:T}|x^0)}
{\tilPT(x^{0:T})}
\eeqa

\beq
\underbrace{
-\sum_{x^0}P(x^0)\ln\tilPT(x^0)
}_{-E_{x^0}[\ln \tilPT(x^0)]}
\leq
\underbrace{
\sum_{x^{0:T}}
P(x^{0:T})
\ln
\frac{P(x^{1:T}|x^0)}
{\tilPT(x^{0:T})}}_{
E_{x^{0:T}}\left[\ln
\frac{P(x^{1:T}|x^0)}
{\tilPT(x^{0:T})}
\right]=\call(\theta)}
\eeq

Henceforth, expected values $E_{x^{0:T}}$
are to be understood as being with respect
to $P(x^{0:T})$.

\begin{claim}
\beq
\call = \sum_{t=0}^T\call^t
\eeq

\beq
\call^0 =
-\ln \tilPT(x^0|x^1)
\eeq

\beq
\call^{t-1}=
E_{x^{0:T}}\left[\ln
\frac{P(x^{t-1}|x^t,x^0)}
{\tilPT(x^{t-1}|x^t)}
\right]
\quad \text{ for } t=2, 3, \ldots T
\eeq

\beq
\call^T =
E_{x^{0:T}}\left[\ln
\frac{P(x^T|x^0)}
{\tilPT(x^T)}
\right]
\eeq

\end{claim}
\proof
\beqa
\call
&=&
E_{x^{0:T}}
\left[\ln 
\frac{P(x^{1:T}|x^0)}
{\tilPT(x^{0:T})}
\right]
\\
&=&
E_{x^{0:T}}
\left[\ln 
\frac{\prod_{t=1}^T P(x^t|x^{t-1})}
{\tilPT(x^T)
\prod^T_{t=1}\tilPT(x^{t-1}|x^t)}
\right]
\\
&=&
E_{x^{0:T}}
\left[\ln 
\frac{P(x^1|x^{0})\prod_{t=2}^T \overbrace
{P(x^t|x^{t-1})}
^{P(x^t|x^{t-1},x^0)}
}
{\tilPT(x^T)\tilPT(x^0|x^1)
\prod^T_{t=2}\tilPT(x^{t-1}|x^t)}
\right]
\\
&=&
E_{x^{0:T}}
\left[\ln 
\frac{P(x^1|x^{0})\prod_{t=2}^T 
P(x^{t-1}|x^t, x^0)
\frac{P(x^t|x^0)}
{P(x^{t-1}|x^0)}
}
{\tilPT(x^T)\tilPT(x^0|x^1)
\prod^T_{t=2}\tilPT(x^{t-1}|x^t)}
\right]
\\
&=&
E_{x^{0:T}}
\left[\ln 
\frac{
\cancel{P(x^1|x^{0})}
\frac{P(x^T|x^0)}
{\cancel{P(x^1|x^0)}}
\prod_{t=2}^T 
P(x^{t-1}|x^t, x^0)
}
{\tilPT(x^T)\tilPT(x^0|x^1)
\prod^T_{t=2}\tilPT(x^{t-1}|x^t)}
\right]
\\
&=&
E_{x^{0:T}}
\left[\ln 
\frac{1}
{\tilPT(x^0|x^1)}
\cdot
\frac{P(x^T|x^0)}
{\tilPT(x^T)}
\cdot
\frac{
\prod_{t=2}^T 
P(x^{t-1}|x^t, x^0)
}{
\prod^T_{t=2}\tilPT(x^{t-1}|x^t)
}
\right]
\eeqa
\qed


\begin{claim}
\beqa
\call^{t-1}
&=&
E_{x^t}
\left[
\frac{1}{2(\s^{t-1})^2}
[M^{t-1}(x^t)-M^{t-1}_\theta(x^t)]^2
\right]
\label{eq-loss-diff-ms}
\\
&=&
E_{x^0, w^t}
\left[
\frac{(C^t)^2}{2(\s^{t-1})^2}
\left
[w^t-n^{t-1}_\theta(
\sqrt{\prodalp}\; x^{0}
 +\sqrt{1-\prodalp}\;w^t
)\right]^2
\right]
\label{eq-from-diff-M}
\eeqa
\end{claim}
\proof

Eq.(\ref{eq-from-diff-M})
 follows trivially from
Eq.(\ref{eq-diff-M}).

Recall that

\beq
\call^{t-1}=
E_{x^{0:T}}\left[\ln
\frac{P(x^{t-1}|x^t,x^0)}
{\tilPT(x^{t-1}|x^t)}
\right]
\eeq

The denominator
$\tilPT(x^{t-1}|x^t)$
is one of the TPMs for
the bnet Fig.\ref{fig-diffusion2} ,
and it's a normal distribution
with mean $M^{t-1}_\theta(x^t)$.
The numerator
$P(x^{t-1}|x^t,x^0)$ is
a normal distribution too;
it was calculated
in the proof
of Claim \ref{cl-m-t-1},
where its mean was called
$M^{t-1}(x^t)$.
Hence, after
some algebra
which is left to the
reader,
one can show Eq.(\ref{eq-loss-diff-ms}). 


\qed

\beq
\call_{simple} =
E_{x^0, w^t}
\left[
\left[w^t-n^{t-1}_\theta(
\sqrt{\prodalp}\; x^{0}
 +\sqrt{1-\prodalp}\;w^t
)\right]^2
\right]
\label{eq-diff-loss-simple}
\eeq

\section{Algorithms for training
and sampling DM}

\begin{algorithm}
	\DontPrintSemicolon
    \SetKwInOut{KwIn}{Input}
    \SetKwInOut{KwOut}{Output}
	\KwIn{$\{\beta^t\}_{t=1}^T$, 
	$\eps>0$, $n^t_\theta(x)$ function,
	$P(x^0)$, $T$, $\eta>0$}
    \KwOut{$\theta_{new}=$ optimal $\theta$}
    
    \While{$|\theta-\theta_{new}| > \eps$}{
    Choose $x^0\sim P(x^0)$\;
    Choose $t\sim Uniform 
    	(\{1,2, \ldots, T\})$\;
    Choose $w \sim \caln(0, I)$\;
    \tcp{Gradient descent for simple loss 
    function given by Eq.(\ref{eq-diff-loss-simple}).}
    $\theta_{new}= \theta +\eta
    \partial_\theta 
    \left[w-n^{t-1}_\theta(
    \sqrt{\prodalp}x^{0}
     +\sqrt{1-\prodalp}w
    )\right]^2$\;
    }    
	\KwRet{$\theta_{new}$}
    \caption{Algorithm for training DM (i.e., finding optimum $\theta$)}
\end{algorithm}

\begin{algorithm}
	\DontPrintSemicolon
    \SetKwInOut{KwIn}{Input}
    \SetKwInOut{KwOut}{Output}
	\KwIn{$n^t_\theta(x)$, 
	where $\theta$ is optimal,
	$T$, $\{\alp^t\}_{t=1}^T$,
	$\{\s^t\}_{t=1}^T $
	}
    \KwOut{$x^0=$ fake image}
    Choose $x^T\sim\caln(0, I)$\;
    \For{$t=T, T-1, \ldots, 2,1$}{
    	Choose $w\sim \caln(0,I)$ if $t>1$,
    	else $w=0$\;
    	\tcp{See Eq.\ref{eq-m-t-1-xt}}
    	$x^{t-1}=
      	\frac{1}{\sqrt{\alp^t}}
      	\left(
      	x^t-
      	\frac{\beta^t}
      	{\sqrt{1-\prodalp}}
      	n^{t-1}_\theta(x^t)
      	\right) + \s^t w$
      	\;
    }
	\KwRet{$x^0$}
    \caption{Algorithm for sampling DM (i.e., finding fake image $x^0$)}
\end{algorithm}