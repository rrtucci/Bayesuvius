\chapter{Diffusion Models}
\label{ch-diffusion}
\newcommand{\prodalp}[0]{\pi_1^t\alp}

This paper is based on
 Ref.\cite{weng-diffusion-model}

Diffusion Models (DM)
are a way of generating 
fake images from an
original image.
They are a competitor 
to GANs (see Chapter \ref{ch-gan}),
and are used in DALL-E 
(OpenAI's 
computer program
that generates images from text).


DM works by
subjecting
each degree of freedom 
of the image 
to a forward Markov
chain
of transformations
labeled $t=0, 1, 2, \ldots, T$
and a reverse Markov
chain of transformations
labeled $t=T-1, T-2, \ldots,
0$.
Each step
of the forward chain
multiplies each degree
of freedom
of the image by a constant
and adds white noise
to it.
The last  image  ($t=T$)
of the forward chain is
changed to the
point that it looks
like a normal distribution.
The reverse chain tries
to undo,
as well as possible, the alterations
to the original image
done by the forward chain.
Full, faithful restoration
is impossible
because the
forward chain
is an irreversible 
process.

Of course, a DM
is only a small part of the magic of DALL-E. I haven't
studied DALL-E's algorithm,
but my guess is that it works roughly as follows.
Given a text description
of an image, such as  
``A hedgehog using a calculator
in the style of Vincent van Gogh",
it uses a neural net
trained on a vast corpus of 
words and images, to match
separate words in the description
 with an image
 for each word.
Then it uses a second neural net
to create a pastiche from
the set of images created in the first
stage. Then, it 
probably modifies
that pastiche
by passing it
through a stylistic filter
that can be specified
in the initial description.
Finally, it uses a DM
to smooth the transitions of the pastiche.



\section{Bnet for DM}

For $t\in 1,2, \ldots , T$, let
\beq
0< \alp^t\
< 1, \; \beta^t= 1-\alp^t
\eeq

\beq
\prodalp= \prod_{\tau=1}^t \alp^\tau
\eeq
Let 

$\rvx^t, \TIL{\rvx}^t\in \RR^{df}$ for $t=0, 1, 2,
\ldots, T$ be column vectors
describing an image. Here $df$
is the number of degrees of freedom 
of the image. $\rvx^0$ 
will denote the initial Original image,
$\rvx^T=\TIL{\rvx}^T$
will denote an image which 
is very close to a 
normal distribution,
and $\TIL{\rvx}^0$
will denote the final Fake image.

$I\in \RR^{df\times df}$ is the 
identity matrix

$\rvw^t, \rva^t,
\rvn^t_\theta \in\RR^{df}$



Fig.\ref{fig-diffusion1} shows a
bnet for DM, and  
\ref{fig-diffusion2}
shows the same bnet 
in more detail.\footnote{
Ref.\cite{weng-diffusion-model} uses $\rvz^{t}$ 
instead of $\rvw^{t}$
for white noise.
Note that 
the time index
of 
$\rvw^{t}$
often does not matter
because 
$\rvw^{t}\sim \caln(0, I)$
for all $t$.
This does not mean
that we can drop the
time index of $\rvw^t$,
because $\rvw^{t_1}$
and $\rvw^{t_2}$
are uncorrelated for $t_1\neq t_2$,
so we can get into 
trouble if we assume
$\rvw^{t_1}=\rvw^{t_2}=\rvw$ .
}

\begin{figure}[h!]
$$
\xymatrix@C=5pc@R=3pc{
\\
\text{Original}
\\
\text{Fake}
}
\xymatrix@C=5pc@R=3pc{
\rvw^0\ar[dr]^{\sqrt{\beta^1}}
&\rvw^1\ar[dr]^{\sqrt{\beta^2}}
&\rvw^2\ar[dr]^{\sqrt{\beta^3}}
\\
\rvx^0\ar[r]_{\sqrt{\alp^1}}
&\rvx^1\ar[r]_{\sqrt{\alp^2}}
&\rvx^2\ar[r]_{\sqrt{\alp^3}}
&\rvx^3(= \TIL{\rvx}^3)\ar[dl]
\\
\TIL{\rvx}^0
&\TIL{\rvx}^1\ar[l]
&\TIL{\rvx}^2\ar[l]
&
}
\xymatrix@C=5pc@R=3pc{
\\
\text{Normal Dist.}
}
$$
\caption{Bnet for DM with $T=3$.
See Chapter \ref{ch-linear-sys}
 for an explanation of LDEN notation.}
\label{fig-diffusion1}
\end{figure}


The TPMs, printed in blue,
for the bnet of Fig.\ref{fig-diffusion1}
are as follows:

\beq \color{blue}
P(x^0)=\delta(x^0, X^0)
\quad \text{ (original image) }
\eeq

\beq \color{blue}
P(x^t|x^{t-1}, w^{t})=
\indi(\quad x^t=\sqrt{\alp^t}\;x^{t-1}
+\sqrt{\beta^t}\;w^{t-1}\quad)
\eeq

\beq \color{blue}
P(w^t) = \caln(w^t; \mu=0,
 \s^2 =  I ) \quad \text{(white noise)}
\eeq

\beqa \color{blue}
P(\TIL{\rvx}^{t-1}=x^{t-1}|\TIL{\rvx}^{t}=x^{t})&=&\color{blue}
\tilPT(x^{t-1}|x^{t})
\\
&=&\color{blue}
\caln(x^{t-1}; \mu=M^{t-1}_\theta(x^t),
\s^2 =\Sigma_\theta^{t-1}(x^t))
 \eeqa
We will assume
that 

\beq
\Sigma_\theta^{t-1}(x^t)
=
(\s^{t-1})^2 I
\eeq
where $\s^{t-1}\in \RR$.
 
 \begin{figure}[h!]
 $$
\xymatrix@C=4pc@R=3pc{
\\
\text{Original}
\\
\text{Fake}
}
 \xymatrix@C=4pc@R=3pc{
 \rvw^0\ar[dr]^{\sqrt{\beta^1}}
 &\rvw^1\ar[dr]^{\sqrt{\beta^2}}
 &\rvw^2\ar[dr]^{\sqrt{\beta^3}}
 \\
 \rvx^0\ar[r]_{\sqrt{\alp^1}}
 &\rvx^1\ar[r]_{\sqrt{\alp^2}}
 &\rvx^2\ar[r]_{\sqrt{\alp^3}}
 &\rvx^3(= \TIL{\rvx}^3)\ar[ddl]\ar[dddl]
 \\
 \TIL{\rvx}^0
 &\TIL{\rvx}^1\ar[dl]\ar[ddl]
 &\TIL{\rvx}^2\ar[dl]\ar[ddl]
 \\
 \rva^0\ar[u]^{1}
 &\rva^1\ar[u]^{1}
 &\rva^2\ar[u]^{1}
 \\
 \rvn_\theta^0\ar@/_1pc/[uu]_{-1}
 &\rvn_\theta^1\ar@/_1pc/[uu]_{-1}
 &\rvn_\theta^2\ar@/_1pc/[uu]_{-1}
 }
 \xymatrix@C=4pc@R=3pc{
 \\
 \text{Normal Dist.}
 }
 $$
 \caption{The bnet of Fig.\ref{fig-diffusion1}
 shown in more detail. }
 \label{fig-diffusion2}
 \end{figure}
 
 
 The TPMs, printed in blue,
 for the bnet of Fig.\ref{fig-diffusion2}
 are as follows:
 \beq \color{blue}
 P(n_\theta^t) = \caln(n_\theta^t; \mu=M^t_\theta(x^{t+1}),
  \s =  \s^t )
 \eeq
 
\beq \color{blue}
 P(a^t|\TIL{\rvx}^{t+1}=x^{t+1}) = \indi(\quad
 a^t= M^t(x^{t+1})
 \quad)
 \eeq
 
\beq \color{blue}
 P(\TIL{\rvx}^t=x^t|
 a_t,
 n_\theta^t
 ) = \indi(\quad
 x^t= 
 a^t-
  n_\theta^t
 \quad)
 \eeq
 
 Note that these TPMs for the
 bnet Fig.\ref{fig-diffusion2}
 imply that
 \beq
\underbrace{ P(
\TIL{\rvx}^t=x^t|
 \TIL{\rvx}^{t+1}=x^{t+1}
 )
 }_{ \text{call this }\tilPT(x^t|x^{t+1})}
 =\quad
 \caln(x^t;
 \mu= M^t(x^{t+1})
 -M^t_\theta(x^{t+1}),
 \s=\s^t)
 \eeq
 
 \section{Mean Values $M^{t-1}(x^t)$ 
 and $M^{t-1}_\theta(x^t)$ }
 \begin{claim}
 \beq
 \boxed{
 \rvx^t = \sqrt{\prodalp}\; \rvx^{0}
 +\sqrt{1-\prodalp}\;\rvw^t}
 \label{eq-xt-x0-w}
 \eeq
 
\beq
 \xymatrix@C=5pc@R=3pc{
 &\rvw^t\ar[d]^{\sqrt{1-\prodalp}}
 \\
 \rvx^0\ar[r]_{\sqrt{\prodalp}}
 & \rvx^t
 }
 \eeq

 \end{claim}
 \proof
 
 Suppose $\rvx_1$ and $\rvx_2$
 are independent
 random variables with
 variances $V_1$ and $V_2$,
 respectively.
 Then the variance $V$ of 
 $\rvx=\rvx_1+\rvx_2$
 is 
 
 \beqa
 V &=& \av{\rvx,\rvx}
 \\
 &=& 
 \av{\rvx_1 + \rvx_2,\rvx_1 + \rvx_2}
 \\
 &=&
 \av{\rvx_1, \rvx_1}
 +
 \av{\rvx_2, \rvx_2}
 \\
 &=&
 V_1 + V_2
 \eeqa
 By similar reasoning, the 
 mean of $\rvx$
 equals the sum
 of the means of $\rvx_1$
 and $\rvx_2$.
 It's also true
 that if both $\rvx_1$
 and $\rvx_2$  are normally
 distributed, then $\rvx$ is too.
 We will refer to a sum of independent normals as a SIN.
 
 Let $\rvw \sim \caln(0,I)$.
 
 
 \beqa
 \rvx^t 
 &=& 
 \sqrt{\alp^t} \rvx^{t-1}
 + \sqrt{1-\alp^t}\;\rvw^{t-1}
 \\
 &=&
 \sqrt{\alp^t} 
 [\sqrt{\alp^{t-1}} \rvx^{t-2}
  + \sqrt{1-\alp^{t-1}}\;\rvw^{t-2}]
  + \sqrt{1-\alp^t}\;\rvw^{t-1}
  \\
  &=&
\sqrt{\alp^t\alp^{t-1}}\; \rvx^{t-2}
+ 
[ \sqrt{\alp^t(1-\alp^{t-1})}\;\rvw^{t-2}
  + \sqrt{1-\alp^t}\;\rvw^{t-1}]
  \\
  &=&
 \sqrt{\alp^t\alp^{t-1}}\; \rvx^{t-2}
 + 
 \sqrt{1-\alp^t\alp^{t-1}}\;\rvw
 \quad \text{(because it's a SIN)}
 \\
 &=& \cdots
 \\
 &=&
 \sqrt{\prodalp}\; \rvx^{0}
  + 
  \sqrt{1-\prodalp}\;\rvw
 \eeqa
 Now replace
 $\rvw$ by $\rvw^t$.
 This is justified
 because they are 
 both $\caln(0,I)$,
 and,
 when $t_1\neq t_2$,  we want the
 $\rvw$ for $\rvx^{t_1}$
 to
 be independent
 from the $\rvw$
 for $\rvx^{t_2}$.
 
 \qed
 
  Solving Eq.(\ref{eq-xt-x0-w}) 
  for $\rvx^0$, we get
  \beq
  \boxed{
  \rvx^0 = \frac{1}{\sqrt{\prodalp}}
  \rvx^t
  -
  \frac{\sqrt{1-\prodalp}}
  {\sqrt{\prodalp}}\rvw^t
  }
  \label{eq-x0-xt-w}
  \eeq

 \beq
 \xymatrix@C=5pc@R=3pc{
 &\rvw^t\ar[dl]
 _{-\;\frac{\sqrt{1-\prodalp}}
   {\sqrt{\prodalp}}}
 \\
 \rvx_0
 & \rvx^t\ar[l]^{\frac{1}{\sqrt{\prodalp}}}
 }
 \eeq
  
 
 \begin{claim}\label{cl-diff-bayes}
 \beq
 P(x^{t-1}|x^{t}, x^0)=
 P(x^t|x^{t-1})
 \frac
 {P(x^{t-1}|x^0)}
 {P(x^t|x^0)}
 \eeq
 \end{claim}
 \proof

 This is a simple
 consequence of Bayes rule.
 We will prove this in
 two ways: algebraically
 and graphically.
 
 The algebraic proof goes as follows.
  \beq
 P(x^{t-1}|x^{t}, x^0)P(x^t|x^0)
 =
 \overbrace{
  P(x^t|x^{t-1}, x^0)}
  ^{P(x^t|x^{t-1})}P(x^{t-1}|x^0)
 \eeq
 
 The  graphical proof,
 although longer, is more 
 intuitive.
 Start by noticing that
 
 \beq
 \begin{array}{l}
 \xymatrix{
 &\sum w^0\ar[d]
 &\cdots
 &\sum w^{t-2}\ar[d]
 &\sum w^{t-1}\ar[d]
 \\
 x^0\ar[r]
 &\sum x^1\ar[r]
 &\cdots\ar[r]
 &x^{t-1}\ar[r]
 &x^t
 }
 \\
 \\
 =
 \xymatrix{
 x^0\ar[r]
 &x^{t-1}\ar[r]
 &x^{t}
 }\text{(diagram A)}
  \\
  \\
  =
  \xymatrix{
  x^0\ar[r]\ar@/_1pc/[rr]_0
  &x^{t-1}\ar[r]
  &x^{t}
  }\quad\text{(make fully connected)}
 \\
 \\
 =
 \xymatrix{
 x^0\ar[r]\ar@/_1pc/[rr]
 &x^{t-1}
 &x^{t}\ar[l]
 }\quad\text{(reverse arrow)  (diagram B)}
 \end{array}
 \label{eq-diff-bayes}
 \eeq
 Therefore,
 
 \beq
 \underbrace{P(x^{t-1}|x^{t}, x^0)P(x^t|x^0)
 }_{\text{diagram B of Eq.\ref{eq-diff-bayes}}}
 =
\underbrace{ P(x^t|x^{t-1})P(x^{t-1}|x^0)
 }_{\text{diagram A of Eq.\ref{eq-diff-bayes}}
 }
 \eeq
 \qed
 
 Define
 the following mean value:
 
 \beq
 M^{t-1}(x^t)=
 \sum_{x^{t-1}}
 x^{t-1}P(x^{t-1}|x^t, x^0)
 \eeq
 
 \begin{claim}
 \label{cl-m-t-1}
 \beq
 \boxed{
 M^{t-1}(x^t)
 =
 \frac{\sqrt{\alp^t}
 (1-\pi_1^{t-1}\alp)}
 {1-\prodalp}
 x^t
 +
 \frac{\sqrt{\pi_1^{t-1}\alp}\;\beta^t}
  {1-\prodalp}
  x^0
  }
  \label{eq-m-t-1}
 \eeq
 \end{claim}
 \proof
 
 By Claim \ref{cl-diff-bayes},
 we know that $\calq =P(x^{t-1}|x^t, x^0)$
 can be expressed as a product
 of two Gaussians
 $P(x^t|x^{t-1})$
 and $P(x^{t-1}|x^0)$,
 divided by a third Gaussian
 $P(x^t|x^0)$,
 and all 3 of
 these Gaussians
 have been calculated 
 in closed form
 previously
in this chapter. So
it's just a matter
of algebra to express
$\calq$ as a Gaussian,
complete the square
inside the exponent
of $\calq$,
and thus obtain its mean value 
$M^{t-1}(x^t)$.
We leave the algebra to the
reader. If in doubt, the algebra can be found 
in Ref.\cite{weng-diffusion-model}.
 \qed
 


\begin{claim}
\beq
\boxed{
M^{t-1}(x^t)=
 \frac{1}{\sqrt{\alp^t}}
 \left(
 x^t-
 \frac{\beta^t}
 {\sqrt{1-\prodalp}}
 w^t
 \right)
 }\label{eq-m-t-1-xt}
 \eeq
 \end{claim}
 \proof
 
 Use Eq.(\ref{eq-x0-xt-w})
 to replace $x^0$ in 
 Eq.(\ref{eq-m-t-1}).
 \qed

 Let us
 parameterize $M^{t-1}_\theta(x^t)$ by
 
 \beq
 \boxed{
 M^{t-1}_\theta(x^t)=
  \frac{1}{\sqrt{\alp^t}}
  \left(
  x^t-
  \frac{\beta^t}
  {\sqrt{1-\prodalp}}
  n^{t-1}_\theta(x^t)
  \right)}
 \eeq
 
If we define
 \beq
 C^t=
 \frac{-\beta^t}
   {\sqrt{\alp^t(1-\prodalp)}}
   \;,
   \eeq
 then 
 
 \begin{align}
 M^{t-1}(x^t)- M^{t-1}_\theta(x^t)
 &=
C^t
[w^t-n^{t-1}_\theta(x^t)]
\\
&=
C^t
[w^t-n^{t-1}_\theta(
\sqrt{\prodalp}\; x^{0}
 +\sqrt{1-\prodalp}\;w^t
)]
\label{eq-diff-M}
\end{align}

\section{Loss function ${\cal L}$}

Note that\footnote{$D_{KL}$
is the Kullback-Leibler divergence.
It's defined in Chapter \nameref{ch0-conventions}.}
\beqa
0&\leq&
 D_{KL}(P(x^{1:T}|x^0)
\parallel \tilPT(x^{1:T}|x^0))
\\
&=&
\sum_{x^{1:T}}
P(x^{1:T}|x^0)
\ln
\frac{P(x^{1:T}|x^0)}
{\tilPT(x^{1:T}|x^0)}
\\
&=&
\ln \tilPT(x^0)
+
\sum_{x^{1:T}}
P(x^{1:T}|x^0)
\ln
\frac{P(x^{1:T}|x^0)}
{\tilPT(x^{0:T})}
\eeqa

\beq
\underbrace{
-\sum_{x^0}P(x^0)\ln\tilPT(x^0)
}_{-E_{x^0}[\ln \tilPT(x^0)]}
\leq
\underbrace{
\sum_{x^{0:T}}
P(x^{0:T})
\ln
\frac{P(x^{1:T}|x^0)}
{\tilPT(x^{0:T})}}_{
E_{x^{0:T}}\left[\ln
\frac{P(x^{1:T}|x^0)}
{\tilPT(x^{0:T})}
\right]=\call(\theta)=\text{ loss function}}
\label{eq-pos-leq-l}
\eeq

Henceforth, expected values $E_{x^{0:T}}$
are to be understood as being with respect
to $P(x^{0:T})$.

The left hand side of
the inequality Eq.(\ref{eq-pos-leq-l})
is expected to be a
small positive constant,
because we expect $\tilPT(x^0)\approx P(x^0)\approx \delta(x^0, X^0)$.
Thus, we are justified
in defining the right 
hand side of 
 Eq.(\ref{eq-pos-leq-l})
 as loss function $\call=\call(\theta)$
 to be minimized.

\begin{claim}
\beq
\call = \sum_{t=0}^T\call^t
\eeq

\beq
\call^0 =
-\ln \tilPT(x^0|x^1)
\eeq

\beq
\call^{t-1}=
E_{x^{0:T}}\left[\ln
\frac{P(x^{t-1}|x^t,x^0)}
{\tilPT(x^{t-1}|x^t)}
\right]
\quad \text{ for } t=2, 3, \ldots T
\eeq

\beq
\call^T =
E_{x^{0:T}}\left[\ln
\frac{P(x^T|x^0)}
{\tilPT(x^T)}
\right]
\eeq

\end{claim}
\proof
\beqa
\call
&=&
E_{x^{0:T}}
\left[\ln 
\frac{P(x^{1:T}|x^0)}
{\tilPT(x^{0:T})}
\right]
\\
&=&
E_{x^{0:T}}
\left[\ln 
\frac{\prod_{t=1}^T P(x^t|x^{t-1})}
{\tilPT(x^T)
\prod^T_{t=1}\tilPT(x^{t-1}|x^t)}
\right]
\\
&=&
E_{x^{0:T}}
\left[\ln 
\frac{P(x^1|x^{0})\prod_{t=2}^T \overbrace
{P(x^t|x^{t-1})}
^{P(x^t|x^{t-1},x^0)}
}
{\tilPT(x^T)\tilPT(x^0|x^1)
\prod^T_{t=2}\tilPT(x^{t-1}|x^t)}
\right]
\\
&=&
E_{x^{0:T}}
\left[\ln 
\frac{P(x^1|x^{0})\prod_{t=2}^T 
P(x^{t-1}|x^t, x^0)
\frac{P(x^t|x^0)}
{P(x^{t-1}|x^0)}
}
{\tilPT(x^T)\tilPT(x^0|x^1)
\prod^T_{t=2}\tilPT(x^{t-1}|x^t)}
\right]
\\
&=&
E_{x^{0:T}}
\left[\ln 
\frac{
\cancel{P(x^1|x^{0})}
\frac{P(x^T|x^0)}
{\cancel{P(x^1|x^0)}}
\prod_{t=2}^T 
P(x^{t-1}|x^t, x^0)
}
{\tilPT(x^T)\tilPT(x^0|x^1)
\prod^T_{t=2}\tilPT(x^{t-1}|x^t)}
\right]
\\
&=&
E_{x^{0:T}}
\left[\ln 
\frac{1}
{\tilPT(x^0|x^1)}
\cdot
\frac{P(x^T|x^0)}
{\tilPT(x^T)}
\cdot
\frac{
\prod_{t=2}^T 
P(x^{t-1}|x^t, x^0)
}{
\prod^T_{t=2}\tilPT(x^{t-1}|x^t)
}
\right]
\eeqa
\qed

We expect $\tilPT(x^0|x^1)$
in $\call^0$
to depend
only very weakly
on $\theta$ because 
it comes at the end of
the reverse Markov chain.
Likewise, we expect $\tilPT(x^T)$
in $\call^T$
to be approximately 
a normal distribution
independent of $\theta$,
because it comes at the beginning
of the reverse 
Markov chain.
Hence,
we are justified in
using 
$\call'= \sum_{t=2}^T \call^{t-1}$ 
as the new loss function
to me minimized with respect to $\theta$. 

\begin{claim}
\beqa
\call^{t-1}
&=&
E_{x^t}
\left[
\frac{1}{2(\s^{t-1})^2}
[M^{t-1}(x^t)-M^{t-1}_\theta(x^t)]^2
\right]
\label{eq-loss-diff-ms}
\\
&=&
E_{x^0, w^t}
\left[
\frac{(C^t)^2}{2(\s^{t-1})^2}
\left
[w^t-n^{t-1}_\theta(
\sqrt{\prodalp}\; x^{0}
 +\sqrt{1-\prodalp}\;w^t
)\right]^2
\right]
\label{eq-from-diff-M}
\eeqa
\end{claim}
\proof

Eq.(\ref{eq-from-diff-M})
 follows trivially from
Eq.(\ref{eq-diff-M})
and Eq.(\ref{eq-loss-diff-ms}).

To show Eq.(\ref{eq-loss-diff-ms}),
 recall that

\beq
\call^{t-1}=
E_{x^{0:T}}\left[\ln
\frac{P(x^{t-1}|x^t,x^0)}
{\tilPT(x^{t-1}|x^t)}
\right]
\eeq

The denominator
$\tilPT(x^{t-1}|x^t)$
is one of the TPMs for
the bnet Fig.\ref{fig-diffusion2} ,
and it's a normal distribution
with mean $M^{t-1}_\theta(x^t)$.
The numerator
$P(x^{t-1}|x^t,x^0)$ is
a normal distribution too;
it was calculated
in the proof
of Claim \ref{cl-m-t-1},
where its mean was called
$M^{t-1}(x^t)$.
Hence, after
some algebra
which is left to the
reader,
one can show Eq.(\ref{eq-loss-diff-ms}). 


\qed

Eq.(\ref{eq-from-diff-M})
for $\call^{t-1}$
is usually simplified to

\beq
\call_{simple} =
E_{x^0, w^t}
\left[
\left[w^t-n^{t-1}_\theta(
\sqrt{\prodalp}\; x^{0}
 +\sqrt{1-\prodalp}\;w^t
)\right]^2
\right]
\label{eq-diff-loss-simple}
\eeq

\section{Algorithms for training
and sampling DM}

\begin{algorithm}
	\DontPrintSemicolon
    \SetKwInOut{KwIn}{Input}
    \SetKwInOut{KwOut}{Output}
	\KwIn{$\{\beta^t\}_{t=1}^T$, 
	$0<\eps<1$, $n^t_\theta(x)$ function,
	$\theta_{in}$, $\Delta  \theta_{in}$,
	$P(x^0)$, $T$, $\eta>0$}
    \KwOut{$\theta_{next}=$ optimal $\theta$}
    $\Delta\theta = \Delta\theta_{in}$\;
    $\theta_{next} = \theta_{in}$\;
    \While{$|\Delta \theta| > \eps$}{
    Choose $x^0\sim P(x^0)$\;
    Choose $t\sim Uniform 
    	(\{1,2, \ldots, T\})$\;
    Choose $w \sim \caln(0, I)$\;
    $\theta = \theta_{next}$\;
    \tcp{Gradient descent for simple loss 
    function given by Eq.(\ref{eq-diff-loss-simple}).}
    $\theta_{next}= \theta +\eta
    \partial_\theta 
    \left[w-n^{t-1}_\theta(
    \sqrt{\prodalp}x^{0}
     +\sqrt{1-\prodalp}w
    )\right]^2$\;
    $\Delta \theta = \theta_{next}-\theta$\;
    }    
	\KwRet{$\theta_{new}$}
    \caption{Algorithm for training DM (i.e., finding optimum $\theta$)}
\end{algorithm}

\begin{algorithm}
	\DontPrintSemicolon
    \SetKwInOut{KwIn}{Input}
    \SetKwInOut{KwOut}{Output}
	\KwIn{$n^t_\theta(x)$, 
	where $\theta$ is optimal,
	$T$, $\{\alp^t\}_{t=1}^T$,
	$\{\s^t\}_{t=1}^T $
	}
    \KwOut{$x^0=$ fake image}
    Choose $x^T\sim\caln(0, I)$\;
    \For{$t=T, T-1, \ldots, 2,1$}{
    	Choose $w\sim \caln(0,I)$ if $t>1$,
    	else $w=0$\;
    	\tcp{See Eq.\ref{eq-m-t-1-xt}}
    	$x^{t-1}=
      	\frac{1}{\sqrt{\alp^t}}
      	\left(
      	x^t-
      	\frac{\beta^t}
      	{\sqrt{1-\prodalp}}
      	n^{t-1}_\theta(x^t)
      	\right) + \s^t w$
      	\;
    }
	\KwRet{$x^0$}
    \caption{Algorithm for sampling DM (i.e., finding fake image $x^0$)}
\end{algorithm}