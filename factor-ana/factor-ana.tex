\chapter{Factor Analysis}
\label{ch-factor-ana}


% Please add the following required packages to your document preamble:
% \usepackage[table,xcdraw]{xcolor}
% Beamer presentation requires \usepackage{colortbl} instead of \usepackage[table,xcdraw]{xcolor}
\begin{table}[h!]
\begin{tabular}{|l|l|l|}
\hline
hidden factors, X                          & observables, Y                                & weights, W \\ \hline
\cellcolor[HTML]{F4B9FB}Staff performance   & \cellcolor[HTML]{F4B9FB}waiting time       & .2         \\ \cline{2-3} 
\cellcolor[HTML]{F4B9FB}             & \cellcolor[HTML]{F4B9FB}cleanliness        & .4         \\ \cline{2-3} 
\cellcolor[HTML]{F4B9FB}             & \cellcolor[HTML]{F4B9FB}staff behavior     & .4         \\ \hline
\cellcolor[HTML]{B8FFDA}Food quality & \cellcolor[HTML]{B8FFDA}taste of food      & .4         \\ \cline{2-3} 
\cellcolor[HTML]{B8FFDA}             & \cellcolor[HTML]{B8FFDA}food temperature   & .3         \\ \cline{2-3} 
\cellcolor[HTML]{B8FFDA}             & \cellcolor[HTML]{B8FFDA}freashness of food & ,3         \\ \hline
\end{tabular}
\caption{Example of Factor Analysis (FA) expressed in tabular form.}
\label{tab-factor-ana}
\end{table}



This chapter is based on Refs.\cite{wiki-factor-ana} and
\cite{tipping-bishop}.

Fig.\ref{tab-factor-ana} is an example
of factor analysis (FA)
expressed in tabular form.

Let

$\rvY, \Delta\rvY, M, \rv{\cale}\in \RR^{na\times nc}$

$W\in \RR^{na\times nb}$

$\rvX \in  \RR^{nb\times nc}$

$\rvy_i, \Delta\rvy_i, \mu_i, \rveps_i\in \RR^{na\times 1}$ for $i=1, 2, \ldots nc$ are column vectors of $\rvY, \Delta \rvY, M, \rv{\cale}$.
The components of $y_i$ will be denoted by $(y_i)_a$ for $a=1,2, \ldots, na$.

$\rvx_i\in \RR^{nb\times 1}$ for $i=1, 2, \ldots nc$ are column vectors of $\rvX$. The components of $x_i$ will be denoted by $(x_i)_b$ for $b=1,2, \ldots, nb$.

Suppose

\begin{subequations}
\label{eq-struc-eqs-fa}
\beq
\underbrace{\rvY - M} _{\Delta \rvY}
= W \rvX + \rv{\cale}
\eeq

\beq
\underbrace{\rvy_i - \mu_i} _{\Delta\rvy_i}
= W \rvx_i + \rveps_i
\eeq
\end{subequations}


In FA, the bnet Fig.\ref{fig-one-factor-bnet}
is repeated 
for $i=1, 2, \ldots, nc$.
Henceforth, will make implicit (i.e. omit) the $i$ 
subscript from 
the column vectors $\rvy_i, \Delta \rvy_i, \rv{\mu}_i,
,\eps_i$.
and $\rvx_i$.
We will make that index explicit in situations when it's
necessary for clarity, as  when the index is being summed over.


As explained in Fig.\ref{fig-naive-v-factor},
 FA can be viewed as
a slight generalization of
Naive Bayes 
(see Chapter \ref{ch-naive}).



\begin{figure}[h!]
$$\xymatrix{
\rvx_i\ar[d]_W & \rveps_i \ar[ld]
\\
\rvy_i
}$$
\caption{In FA, this bnet is repeated
for $i=1,2,\ldots, nc$.  }
\label{fig-one-factor-bnet}
\end{figure}

\begin{figure}[h!]
$$
\begin{array}{ccc}
\xymatrix{
\rvc\ar[d] \ar[dr]
&
\\
\rvy_1 & \rvy_2
}
&&
\xymatrix{
x_1\ar[d]\ar[dr]& x_2\ar[dl]\ar[d]
\\
\rvy_1 & \rvy_2}
\\
\\
\text{Naive Bayes} & &
\text{Factor Analysis}
\end{array}
$$
\caption{Structurally speaking
(disregarding the TPMs of the nodes), if we disregard the noise nodes $\rveps_i$, FA can be viewed as a generalization of
Naive Bayes (NB). In NB we have a single root node $\rvc$, whereas in FA,
we can have multiple root nodes such as $\rvx_1$ and 
$\rvx_2$.}
\label{fig-naive-v-factor}
\end{figure}

Besides the structural equations
Eqs.(\ref{eq-struc-eqs-fa}), FA assumes


\begin{enumerate}

\item $P(x, \eps)=P(x)P(\eps)$ 
($\rvx$ and $\rveps$ are independent variables)

\item $E[\rvy]=\mu$, $\av{\rvx, \rvx^T}=1$, $\rveps\sim \caln(0, \s I)$
\end{enumerate}
Hence

\beqa
C_\rvy&=&\av{\Delta \rvy, \Delta\rvy^T}
\\
&=&
\av{W\rvx + \rv{\eps}, [W\rvx + \rv{\eps}]^T}
\\
&=&
W\av{\rvx, \rvx^T}W^T + \av{\rv{\eps}, \rv{\eps}^T}
\\
&=&
WW^T + \s^2I
\eeqa

In FA, the bnet Fig.\ref{fig-one-factor-bnet}
is given the TPMs of an LDEN\footnote{LDEN=Linear Deterministic
with External Noise. LDEN models are defined in Chapter \ref{ch-linear-sys}}.
Root node $\rvx$ 
can be given either a deterministic
or a random TPM. We next give the TPMs, printed in blue,
for these two cases:

\begin{itemize}
\item LDEN Model
with deterministic
$\rvx$ 

\beq\color{blue}
P(\eps) = \caln(\eps;0, \s=\s)= \frac{1}{(2\pi\s^2)^{na/2}}
\exp\left(- \;\frac{\eps^T \eps}{2\s^2}\right)
\eeq


\beq\color{blue}
P(x) = \delta(x, X)
\eeq

\beq\color{blue}
P(y|x, \eps)=
\indi(y=Wx +\eps)
\eeq


\item LDEN Model
with Gaussian $\rvx$

\beq\color{blue}
P(\eps) = \caln(\eps;0, \s=\s)= \frac{1}{(2\pi\s^2)^{na/2}}
\exp\left(- \;\frac{\eps^T \eps}{2\s^2}\right)
\eeq


\beq\color{blue}
P(x)=\caln(x; 0, \s=1)=\frac{1}{(2\pi)^{nb/2}}
\exp\left(- \;\frac{x^T x}{2}\right)
\eeq

\beq\color{blue}
P(y|x, \eps)=
\indi(y=Wx +\eps)
\eeq

\end{itemize}

If we assume that $\rvx$ has a 
Gaussian TPM, we get

\beqa
P(y|x) &=&\sum_\eps P(y|x, \eps)P(\eps)
\\
&=& \frac{1}{(2\pi\s^2)^{na/2}}\exp\left(-\;\frac{1}{2\s^2}
\norm{y-Wx-\mu}^2\right)
\eeqa

\beqa
P(y)&=&\sum_x P(y|x) P(x)
\\
&=&
\frac{1}{(2\pi)^{na/2}\sqrt{\det C_\rvy}}
\exp\left(-\;\frac{1}{2}\Delta y^T C_\rvy^{-1}\Delta y\right)
\eeqa

Let
\beq
\calc = W^T W + \s^2 I
\eeq

\beq
\HAT{\calc} = \calc/\s^2
\eeq

\beq
\Delta x = x -\calc^{-1}W^T\Delta y
\eeq
Note that $\calc$ is $nb\times nb$ whereas $C_\rvy=WW^T + \s^2I$ is $na\times na$.

Using Bayes rule, we get
\beqa
P(x|y) &=&\frac{ P(y|x)P(x)}{P(y)}
\\
&=&
\frac{1}{(2\pi)^{nb/2}}\sqrt{\det \HAT{\calc}}
\exp\left(-\;\frac{1}{2}\Delta x^T\HAT{\calc}\Delta x\right)
\eeqa

Let
\beq
S=\frac{1}{nc}\sum_{i=1}^{nc} \Delta y_i \Delta y^T_i
\eeq
Then the
log of the posterior probability of $\rvY$ is given by
\beqa 
\call &=& \sum_{i=1}^{nc}\ln P(y_i)
\\
&=&
-\;\frac{na\;\;nc}{2}\ln (2\pi) 
-\; \frac{nc}{2}\ln \det(C_\rvy)
-\;\frac{nc}{2} \tr (C_\rvy^{-1} S)
\eeqa
