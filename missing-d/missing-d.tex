\chapter{Missing Data, Imputation}
\label{ch-missing-d}

This chapter assumes
that the reader has
read some parts of
Chapter \ref{ch-emax}
on the Expectation
Maximization
(EM) algo
and Chapter \ref{ch-mcmc}
on Markov Chain Monte Carlo
(MCMC).

\begin{table}[h!]
\centering
\begin{tabular}{|
>{\columncolor[HTML]{ECF4FF}}l |l|l|l|l|}
\hline
 & \cellcolor[HTML]{ECF4FF}$h_0$ & \cellcolor[HTML]{ECF4FF}$x_0$ & \cellcolor[HTML]{ECF4FF}$x_1$ & \cellcolor[HTML]{ECF4FF}$x_2$ \\ \hline
1 & NA & 0 & 1 & 1 \\ \hline
2 & NA & 0 & 0 & 0 \\ \hline
3 & NA & 1 & 1 & 0 \\ \hline
4 & NA & NA & 1 & NA \\ \hline
5 & NA & 0 & NA & 1 \\ \hline
6 & NA & 0 & 0 & 1 \\ \hline
\end{tabular}
\;\;\;\;
\begin{tabular}{|
>{\columncolor[HTML]{ECF4FF}}l |l|l|l|l|l|}
\hline
 & \cellcolor[HTML]{ECF4FF}$h_0$ & \cellcolor[HTML]{ECF4FF}$x_0$
 & \cellcolor[HTML]{ECF4FF}$x_1$ & \cellcolor[HTML]{ECF4FF}$x_2$ 
& \cellcolor[HTML]{DAE8FC}$m$ \\ \hline
1 & NA & 0 & 1 & 1 & (0,0,0) \\ \hline
2 & NA & 0 & 0 & 0 & (0,0,0) \\ \hline
3 & NA & 1 & 1 & 0 & (0,0,0) \\ \hline
4 & NA & \begin{tabular}[c]{@{}l@{}}0\\ 0\\ 1\\ 1\end{tabular} & 1 & \begin{tabular}[c]{@{}l@{}}0\\ 1\\ 0\\ 1\end{tabular} & (1,0,1) \\ \hline
5 & NA & 0 & \begin{tabular}[c]{@{}l@{}}0\\ 1\end{tabular} & 1 & (0,1,0) \\ \hline
6 & NA & 0 & 0 & 1 & (0,0,0) \\ \hline
\end{tabular}
\caption{{\bf Left Table:}
Dataset 
with 
$nsam=6$ and some missing entries,
 for 4 binary variables $h_0, x_0, x_1, x_2$.
 NA=not available.
The $h_0$  column is completely missing 
because $h_0$ is an unobserved latent 
variable.
{\bf Right Table:} All possibilities
for
$x_i=NA$ cells of left table have
been enumerated. A new column
labeled $m$ has been added.
$m_i=\indi(x_i \text{ is missing})$
for $i=0,1,2$.}
\label{tab-missing-data}
\end{table}


Suppose you
have compiled a dataset
from a study.
It consists 
of $nsam$ number 
of samples (sample= row), 
and $nx$ columns (each column is a different 
feature, or observation).
Suppose that some of the
cells
in this matrix 
are empty.
Throwing
away all the incomplete
rows is okay 
if the 
number
of incomplete rows
is much smaller  than
$nsam$.
If not,
throwing 
them
away would
throw away
a substantial amount of
information 
from all the
filled cells
in those incomplete rows, 
plus it might
bias your dataset.
This chapter
deals with
how to fill
those empty cells 
with plausible
fake data.
A fancy name
for this process
is {\bf imputation}.
There is no unique
way of 
fabricating
fake data,
but
some fakes
are
better than others
by some metrics.
This chapter will
consider
two popular
ways (EM
and MCMC)
of 
filling those
empty 
cells
with
their
``most likely" values
based on the cells
of the dataset that
aren't missing,
and perhaps
also based
on some 
model (DAG)
that is 
expected to describe well the
dataset.

Notation:
$\vec{\rva}=
(\rva[\sigma])_{\sigma=0, 1, \ldots,nsam-1}
$, where $nsam$
is the number of samples.
Will
sometimes
denote
$a\sqsig$ by $a^\sqsig$.


For concreteness,
we will
apply
the concepts
of this chapter
to
the dataset
with missing data
given by Table \ref{tab-missing-data}.


\section*{Imputation via EM}

We begin by augmenting
Fig.\ref{fig-em-bnet} (the first figure
in Chapter \ref{ch-emax}).
We augment it 
to Figs.\ref{fig-mar}
and \ref{fig-mar-multi}
by adding a new node
$\vec{\rvm}$
called the
{\bf missingness variable}.
There are 
3
popular
ways of 
connecting node $\vec{\rvm}$
 to the
other nodes
in the graph.
For
doing imputation
via EM, 
we connect node
$\vec{\rvm}$
as shown in the
middle bnet (called MAR)
of Fig.\ref{fig-mar}.
Recall
that node
$\ul{\theta}$
represents
the {\bf unknown parameters},
node $\vec{\rvx}$
represents the
{\bf observed variables},
and 
node $\vec{\rvh}$
represents the {\bf latent
variables}.
Both $\ul{\theta}$
and $\vec{\rvh}$
are hidden (i.e., unobserved).


\begin{figure}[h!]
$$
\begin{array}{ccc}
\xymatrix{
&\ul{\theta}\ar[d]\ar[ld]\ar[rd]
\\
\vec{\rvm}
&\vec{\rvx}
&\vec{\rvh}\ar[l]
}
&
\xymatrix{
&\ul{\theta}\ar[d]\ar[ld]\ar[rd]
\\
\vec{\rvm}
&\vec{\rvx}\ar[l]
&\vec{\rvh}\ar[l]
}
&
\xymatrix{
&\ul{\theta}\ar[d]\ar[ld]\ar[rd]
\\
\vec{\rvm}
&\vec{\rvx}\ar[l]
&\vec{\rvh}\ar[l]\ar@/^1pc/[ll]
}
\\
\\
\text{Seldom assumed}
&\text{MAR}
&\text{not-MAR (NMAR)}
\end{array}
$$
\caption{The 
left bnet is seldom assumed. The
middle bnet 
is referred to as the MAR (missing
at random)
assumption. The right bnet
is referred to as the not-MAR  (NMAR)
assumption.}
\label{fig-mar}
\end{figure}

\begin{figure}[h!]
$$
\begin{array}{ccc}
\xymatrix{
&\ul{\theta}\ar[d]\ar[dr]\ar[dl]
\\
\vec{\rvm}
&\ul{\vecx}\ar[l]
&\ul{\vech}\ar[l]
}
&=&
\xymatrix{
&\ul{\theta}\ar[ld]\ar[ldd]\ar[lddd]
\ar[d]
\ar@/_1pc/[dd]
\ar@/_1pc/[ddd]
\ar[rd]\ar[rdd]\ar[rddd]
\\
\rvm[0]
&
\rvx[0]\ar[l]
&\rvh[0]\ar[l]
\\
\rvm[1]
&
\rvx[1]\ar[l]
&\rvh[1]\ar[l]
\\
\rvm[2]
&
\rvx[2]\ar[l]
&\rvh[2]\ar[l]
}
\end{array}
$$
\caption{MAR bnet
with $nsam=3$.}
\label{fig-mar-multi}
\end{figure}

From Fig.\ref{fig-mar}, we have
\beq
P(\vec{m}|\vec{x},
\vec{h}, \theta)=
\left\{
\begin{array}{ll}
P(\vec{m}| \theta)&\text{Seldom assumed. Called missing-CAR (MCAR)}
\\
P(\vec{m}|\vec{x}, \theta)&\text{MAR}
\\
P(\vec{m}|\vec{x}, \vec{h}, \theta)
&\text{not-MAR (NMAR)}
\end{array}
\right.
\;.
\eeq

For the example of
Table \ref{tab-missing-data},
we have
variables
$\vec{\rvm},\vec{\rvx}$
and $\vec{\rvh}$
whose values range over the 
following sets:

$\vec{\rvx}=(\vec{\rvx}_0, \vec{\rvx}_1, 
\vec{\rvx}_2)$

$\vec{\rvh}=(\vec{\rvh}_0)$

$\rvh_0[\sigma]\in\bool$,

$\rvx_i[\sigma]\in\bool$
for $i=0,1,2$,

$\rvm_i[\sigma]\in\bool$
for $i=0,1,2$.



\begin{figure}[h!]
$$\xymatrix{
\rvm[0]
&\rvx[0]\ar[l]
&\rvh[0]\ar[l]
}=
\;\;\;\;\;
\xymatrix{
\rvm[0]
&\rvx_0[0]\ar[l]\ar[d]\ar@/^1pc/[dd]
&\rvh[0]\ar[ld]
\ar@{-->}[ldd]
\ar@{-->}[l]
\\
&\rvx_1[0]\ar[ul]\ar[d]
\\
&\rvx_2[0]\ar[uul]
}
$$
\caption{Our example for imputation via
 EM
assumes this bnet
between nodes
$\rvm[\sigma],
\rvx[\sigma], \rvh[\sigma]$.}
\label{fig-miss-subnet}
\end{figure}

For
concreteness,
we will assume
that 
the Markov
chain
$\rvm[\sigma]\larrow
\rvx[\sigma]\larrow\rvh[\sigma]$
has a finer grained DAG structure
given by Fig.\ref{fig-miss-subnet}.
where we will
omit the dashed arrows.
If one
doesn't
want
to assume that the data
can be fitted
well by the
bnet
of 
Fig.\ref{fig-miss-subnet}
without
the dashed arrows,
one can include those arrows too,
at the expense of more 
unknown parameters
(i.e., degrees of freedom)
to be lumped into $\theta$.
We will parmaterize
the TPMs 
corresponding
to Fig.\ref{fig-miss-subnet}
using a 
Categorical Distribution
for each column of the
TPMs.
We will thus assume
that the bnet of 
Fig.\ref{fig-miss-subnet}
has the following TPMs,
printed in blue.

\beq\color{blue}
P(h_0^\sqsig| \theta)=
\begin{array}{l|l}
\\\hline
&1-\theta_0
\\
{\scriptstyle 1}&\theta_0
\end{array}
\eeq

\beq\color{blue}
P(x_0^\sqsig| \theta)=
\begin{array}{l|l}
\\\hline
{\scriptstyle 0}&1-\theta_1
\\
{\scriptstyle 1}&\theta_1
\end{array}
\eeq


\beq\color{blue}
P(x_1^\sqsig\cond x_0^\sqsig, h^\sqsig, \theta)=
\begin{array}{l|llll}
&{\scriptstyle 00}
&{\scriptstyle 01}
&{\scriptstyle 10}
&{\scriptstyle 11}
\\\hline
{\scriptstyle 0}
&1-\theta_2
&1-\theta_3
&1-\theta_4
&1-\theta_5
\\
{\scriptstyle 1}
&\theta_2
&\theta_3
&\theta_4
&\theta_5
\end{array}
\eeq


\beq\color{blue}
P(x_2^\sqsig\cond
 x_1^\sqsig, x_0^\sqsig, \theta)=
\begin{array}{l|llll}
&{\scriptstyle 00}
&{\scriptstyle 01}
&{\scriptstyle 10}
&{\scriptstyle 11}
\\\hline
{\scriptstyle 0}
&1-\theta_6
&1-\theta_7
&1-\theta_8
&1-\theta_9
\\
{\scriptstyle 1}
&\theta_6
&\theta_7
&\theta_8
&\theta_9
\end{array}
\eeq

\beq\color{blue}
P(m^\sqsig|x^\sqsig, \theta)=
\frac{1}{nsam}
P((x_i)_{\forall i\ni m_i=1}\cond
(x_i)_{\forall i\ni m_i=0},\theta)
\label{eq-prob-miss}
\eeq

Eq.(\ref{eq-prob-miss})
can be illustrated 
as follows.
In Table \ref{tab-missing-data-prob-m},
we added a $P(m)$ column
to Table \ref{tab-missing-data}.


\begin{table}[h!]
\centering
\begin{tabular}{|
>{\columncolor[HTML]{ECF4FF}}l |l|l|l|l|l|
>{\columncolor[HTML]{CBCEFB}}l |}
\hline
 & \cellcolor[HTML]{ECF4FF}$h_0$ & \cellcolor[HTML]{ECF4FF}$x_0$ & \cellcolor[HTML]{ECF4FF}$x_1$ & \cellcolor[HTML]{ECF4FF}$x_2$ & \cellcolor[HTML]{ECF4FF}$m$ & $P(m)$ \\ \hline
1 & NA & 0 & 1 & 1 & (0,0,0) & $\frac{1}{nsam}$ \\ \hline
2 & NA & 0 & 0 & 0 & (0,0,0) & $\frac{1}{nsam}$ \\ \hline
3 & NA & 1 & 1 & 0 & (0,0,0) & $\frac{1}{nsam}$ \\ \hline
4 & NA & \begin{tabular}[c]{@{}l@{}}0\\ 0\\ 1\\ 1\end{tabular} & 1 & \begin{tabular}[c]{@{}l@{}}0\\ 1\\ 0\\ 1\end{tabular} & (1,0,1) & $\misscellone$ \\ \hline
5 & NA & 0 & \begin{tabular}[c]{@{}l@{}}0\\ 1\end{tabular} & 1 & (0,1,0) & $\misscelltwo$ \\ \hline
6 & NA & 0 & 0 & 1 & (0,0,0) & $\frac{1}{nsam}$ \\ \hline
\end{tabular}
\caption{$P(m)$ column added to 
 Table \ref{tab-missing-data}.
Note that $\sum_m P(m)=1$.}
\label{tab-missing-data-prob-m}
\end{table}

\beq
\theta=(\theta_i)_{i=0, 1, \ldots, 9}
\eeq

\beq
P(m^\sqsig, x^\sqsig, h^\sqsig|\theta)=
P(m^\sqsig| x^\sqsig, \theta)
P(x^\sqsig| h^\sqsig, \theta)
P(h^\sqsig|\theta)
\eeq


\beq
P(x^\sqsig| h^\sqsig, \theta)=
P(x_2^\sqsig|x_1^\sqsig, x_0^\sqsig, \theta)
P(x_1^\sqsig|x_0^\sqsig, h^\sqsig, \theta)
P(x_0^\sqsig| \theta)
\eeq

\beq
P(x_1^\sqsig|x_0^\sqsig, \theta)=
\sum_h 
P(x_1^\sqsig|x_0^\sqsig, h^\sqsig, \theta)
P(h^\sqsig|\theta)
\eeq

\beq
P(x^\sqsig| \theta)=
P(x_2^\sqsig|x_1^\sqsig, x_0^\sqsig, \theta)
P(x_1^\sqsig|x_0^\sqsig,\theta)
P(x_0^\sqsig| \theta)
\eeq

\beqa
Q(\theta|\theta^{(t)})
&=&
\sum_{ \vec{m},\vec{h}}
P(\vec{m},\vec{h}\cond
\vec{x}, \theta^{(t)})
\ln P(\vec{m}, \vec{x}, \vec{h}|\theta)
\\
&=&
\sum_{\vec{m}, \vec{h}}
\left[ \prod_\sigma 
P(m^\sqsig,h^\sqsig\cond
x^\sqsig, \theta^{(t)})
\right]
\ln 
\left[
\prod_\sigma 
P(m^\sqsig, 
x^\sqsig, h^\sqsig|\theta)
\right]
\\
&=&
\sum_\sigma
\sum_{m^\sqsig, h^\sqsig}
P(m^\sqsig,h^\sqsig\cond
x^\sqsig, \theta^{(t)})
\ln P(m^\sqsig, x^\sqsig, h^\sqsig|\theta)
\\
&=&
\sum_\sigma
\sum_{m^\sqsig, h^\sqsig}
\frac{P(m^\sqsig,h^\sqsig,
x^\sqsig\cond \theta^{(t)})}
{
P(
x^\sqsig\cond \theta^{(t)})
}
\ln P(m^\sqsig, x^\sqsig, h^\sqsig|\theta)
\eeqa

Once you find optimal
parameters $\theta^*$
by recursing this $Q(\theta|\theta^{(t)})$,
you
can evaluate
numerically the
$P(m)$ 
column 
of Table \ref{tab-missing-data-prob-m}.
In Table
\ref{tab-missing-data-prob-m},
out of the 4
sub-rows for row 4,
choose the one with
the highest probability.
Similarly,
out of the  2 sub-rows for row 5,
choose the one with 
the highest probability.

\section*{Imputation via MCMC}
A simple 
and popular way to do inputation via MCMC
is described in 
Ref.\cite{taka2017}. It goes as follows.

Let
\beq
\rvH[\sigma]=(\rvh[\sigma], \rvm[\sigma])
\eeq
for $\sigma=0, 1, \ldots, nsam-1$.
Initialize $\theta^{(0)}$ 
to a random value
within the allowed ranges.
Do the following 2 steps, 
for $t=0, 1, \ldots, T-1$,
where $T$
is large enough 
that $\theta^{(t)}$ has 
reached a steady value
that is independent of $\theta^{(0)}$.
To do the
sampling, use a 
standard sampling technique such as Gibbs sampling.

\begin{subequations}
\begin{itemize}
\item{\bf STEP 1:}
For $\sigma=0, 1, \ldots, nsam-1$, find a sample

\beq
(H^\sqsig)^{(t+1)}\sim
 P(H^\sqsig|x^\sqsig, \theta^{(t)})
\;.
\eeq
\item {\bf STEP 2:} Find a sample
\beq
\theta^{(t+1)}\sim 
P^{(t+1)}(\theta)
\eeq
where

\beq
 P^{(t+1)}(\theta)=
\caln(!\theta)
\prod_\sigma
P(x^\sqsig, (H^\sqsig)^{(t+1)}|\theta)
\;.
\eeq
\end{itemize}
\label{eq-impu-mcmc}
\end{subequations}
Fig.\ref{fig-impu-mcmc} illustrates 
this two step
process using a bnet.

\begin{figure}[h!]
$$\xymatrix{
\vec{\rvx}\ar[dr]\ar[drr]\ar[drrr]
\\
\ul{\theta}^{(0)}\ar[dr]
&\ul{\theta}^{(1)}\ar[dr]
&\ul{\theta}^{(2)}\ar[dr]
&\ul{\theta}^{(3)}
\\
&\vec{\rvH}^{(1)}\ar[u]
&\vec{\rvH}^{(2)}\ar[u]
&\vec{\rvH}^{(3)}\ar[u]
\\
\vec{\rvx}\ar[ur]\ar[urr]\ar[urrr]
}$$
\caption{bnet illustrating Eqs.(\ref{eq-impu-mcmc})
for doing imputation via MCMC.
The {\bf same} node $\vec{\rvx}$
appears twice 
to make the graph
clearer.}
\label{fig-impu-mcmc}
\end{figure}
\section*{Multiple Imputations}
{\bf Multiple imputations} means calculating
$\theta^*$ (i.e.,
the optimum $\theta$) 
via any method
(such as EM or MCMC).
Doing so a large number $N$
of times,
starting from different, randomly
chosen  $\theta^{(0)}$
initial parameters,
and then calculating
the average and the variance 
of $\theta^*$.