\chapter{Hidden Markov Model}
\label{ch-hmm}

A dynamical Bayesian
network (DBN)  (see Chapter
\ref{ch-dyn-bnet})
is a generalization
of a
Hidden Markov Model (HMM), which
in turn is 
 a  generalization of a
Kalman Filter (KF) (see Chapter
\ref{ch-kalman}).

See Wikipedia article 
Ref.\cite{wiki-hmm} to learn 
about the history 
and many uses of HMMs. This
chapter is based on
Ref.\cite{nuel}.

In this
chapter,
we use the following conventions.

\bnetConventions


\begin{figure}[h!]
\centering
$$\xymatrix{
*++[F-o]{\rvx_0}\ar[d]\ar[r]&
*++[F-o]{\rvx_1}\ar[d]\ar[r]&
*++[F-o]{\rvx_2}\ar[d]\ar[r]&
*++[F-o]{\rvx_3}\ar[d]\\
\rvv_0&
\rvv_1&
\rvv_2&
\rvv_3
}$$
\caption{HMM bnet
with $n=4$.}
\label{fig-hmm}
\end{figure}

Suppose 

$\rvv^n=(\rvv_0, \rvv_1, 
\ldots, \rvv_{n-1})$
are $n$ visible nodes that
are measured,
and 

$\rvx^n=(\rvx_0, \rvx_1, 
\ldots, \rvx_{n-1})$
are the $n$ hidden, unmeasurable 
state nodes of a system
that is being monitored.



For the bnet of Fig.\ref{fig-hmm},
one has
\beq
P(x^n, v^n)=\prod_{i=0}^{n-1}
P(x_i|x_{i-1})P(v_i|x_i)
\;,
\eeq
where $x_{-1}=\emptyset$.
The goal of HMMs is to
predict $P(x^n|v^n)$, 
i.e., the probability
of a hidden sequence
$x^n$, 
given a visible sequence $v^n$.

Let
$x_{<i} =(x_0, x_1, \dots, x_{i-1})$.

For $i=0,1, \dots, n-1$, define

$\calf_i$=future measurements probability

\beqa
\calf_i(x_i)&=&
P(v_{> i}|x_i)
\\
&=&
\xymatrix{
x_i\ar[r]
&\sum x_{>i}\ar[d]
\\
&v_{>i}
}
\eeqa

$\ol{\calf}_i$= 
past and present measurements  probability

\beqa
\ol{\calf}_i(x_i)&=&
P(v_{<i},v_i, x_i)
\\
&=&
\xymatrix{\sum 
x_{<i}\ar[r]\ar[d]
&x_i\ar[d]
\\
v_{<i}&v_i
}
\eeqa

$\lam_i$=
present measurement probability

\beqa
\lam_i(x_i)&=&
P(v_i|x_i)
\\
&=&
\xymatrix{
x_i\ar[d]
\\
v_{i}
}
\eeqa



\begin{claim}
For $i\geq 0$, 
\beqa
P(x_i, v^n)&=&
\ol{\calf}_i(x_i)\calf_i(x_i)
\\
&=&
\xymatrix{\sum 
x_{<i}\ar[r]\ar[d]
&x_i\ar[d]
\\
v_{<i}&v_i
}
\xymatrix{
x_i\ar[r]
&\sum x_{>i}\ar[d]
\\
&v_{>i}
}
\;.
\eeqa
For $i>0$,

\beqa
P(x_{i-1},x_i, v^n)&=&
 \ol{\calf}_{i-1}(x_{i-1})
P(x_i|x_{i-1})\lam_i(x_i)\calf_i(x_i)
\\
&=&
\xymatrix{\sum 
x_{<i-1}\ar[r]\ar[d]
&x_{i-1}\ar[d]
\\
v_{<i-1}&v_{i-1}
}
\xymatrix{
x_{i-1}\ar[r]&x_i}
\xymatrix{
x_i\ar[d]
\\
v_{i}
}
\xymatrix{
x_i\ar[r]
&\sum x_{>i}\ar[d]
\\
&v_{>i}
}
\;.
\eeqa


\end{claim}
\proof

\beqa
P(x_i,v^n)
&=&
\sum_{x_{< i}}\sum_{x_{> i}}
P(x^n, v^n)
\\
&=&
\sum_{x_{< i}}\sum_{x_{> i}}
P(x^n, v^n|x_i)P(x_i)
\\
&=&
\sum_{x_{< i}}\sum_{x_{> i}}
P(x_{< i}, v_{< i}, v_i|x_i)
P(x_{>i}, v_{>  i}|x_i)
P(x_i)
\\
&=&
P( v_{< i}, v_i|x_i)
P(v_{>  i}|x_i)
P(x_i)
\\
&=&
\ol{\calf}_i(x_i)\calf_i(x_i)
\eeqa

\begin{align}
P(x_{i-1},x_i,v^n)
&=
\sum_{x_{< i-1}}\sum_{x_{>i}}
P(x^n, v^n)
\\
&=
\sum_{x_{< i-1}}\sum_{x_{>i}}
P(x^n, v^n|x_{i-1}, x_i)P(x_{i-1}, x_i)
\\
&=
\sum_{x_{< i-1}}\sum_{x_{>i}}
P(x_{<i-1}, v_{<i-1}, v_{i-1}|x_{i-1})
P(v_i|x_i)
P(x_{i-1}, x_i)
P(x_{>  i}, v_{> i}|x_i)
\\
&=
P( v_{<i-1}, v_{i-1}|x_{i-1})
P(v_i|x_i)
P(x_{i-1}, x_i)
P( v_{> i}|x_i)
\\
&=
 \ol{\calf}_{i-1}(x_{i-1})
\lam_i(x_i)
P(x_i|x_{i-1})
\calf_i(x_i)
\end{align}
\qed

\begin{claim}
For $i>0$, $\calf_i$ and
$\ol{\calf}_i$ can be calculated 
recursively as follows:


\beq
\ol{\calf}_i(x_{i})
=
\sum_{x_{i-1}}
\ol{\calf}_{i-1}(x_{i-1})
P(x_i|x_{i-1})\lam_i(x_i)
\eeq
which can be represented graphically by

\beq
\xymatrix{\sum 
x_{<i}\ar[r]\ar[d]
&x_{i}\ar[d]
\\
v_{<i}&v_{i}
}
=\quad\sum_{x_{i-1}}
\xymatrix{\sum 
x_{<i-1}\ar[r]\ar[d]
&x_{i-1}\ar[d]
\\
v_{<i-1}&v_{i-1}
}
\xymatrix{
x_{i-1}\ar[r]&x_i}
\xymatrix{
x_i\ar[d]
\\
v_{i}
}
\eeq
and

\beq
\calf_{ i-1}(x_{i-1})
=
\sum_{x_i}
P(x_i|x_{i-1})\lam_i(x_i)
\calf_i(x_{i})
\eeq
which can be represented graphically by

\beq
\xymatrix{
x_{i-1}\ar[r]
&\sum x_{>i-1}\ar[d]
\\
&v_{>i-1}
}
=\quad \sum_{x_i}
\xymatrix{x_{i-1}\ar[r]&x_i}
\xymatrix{x_i\ar[d]\\v_i}
\xymatrix{
x_i\ar[r]
&\sum x_{>i}\ar[d]
\\
&v_{>i}
}
\eeq

\end{claim}
\proof

\beqa
\ol{\calf}_i(x_i)\calf_i(x_i)
&=&
P(x_i, v^n)\\
&=&
\sum_{x_{i-1}}P(x_{i-1},x_i, 
v^n)\\
&=&\sum_{x_{i-1}}
\ol{\calf}_{i-1}(x_{i-1})
\lam_i(x_i)
P(x_i|x_{i-1})\calf_i(x_i)
\eeqa

\beqa
\ol{\calf}_{i-1}(x_{i-1}
)\calf_{i-1}(x_{i-1})
&=&
P(x_{i-1}, v^n)\\
&=&
\sum_{x_i}P(x_{i-1},x_i, 
v^n)\\
&=&\sum_{x_i}
\ol{\calf}_{i-1}(x_{i-1})
\lam_i(x_i)
P(x_i|x_{i-1})\calf_i(x_i)
\eeqa
\qed

\begin{claim}
\beqa
P(x_i|x_{i-1}, v^n)
&=&
\frac{P(x_i|x_{i-1})
\lam_i(x_i)
\calf_{i}(x_i)}
{\calf_{i-1}(x_{i-1})}
\\
&=&
\frac{
\xymatrix{x_{i-1}\ar[r]&x_i}
\xymatrix{x_i\ar[d]\\v_i}
\xymatrix{
x_i\ar[r]
&\sum x_{>i}\ar[d]
\\
&v_{>i}
}
}{
\xymatrix{
x_{i-1}\ar[r]
&\sum x_{i}\ar[d]\ar[r]
&\sum  x_{>i}\ar[d]
\\
&v_{i}&v_{>i}
}
}
\eeqa

\beqa
P(x_{i-1}|x_i, v^n)&=&
\frac{\ol{\calf}_{i-1}(x_{i-1})
P(x_i|x_{i-1})
\lam_i(x_i)
}
{\ol{\calf}_i(x_{i})}
\\
&=&
\frac{
\xymatrix{\sum 
x_{<i-1}\ar[r]\ar[d]
&x_{i-1}\ar[d]
\\
v_{<i-1}&v_{i-1}
}
\xymatrix{
x_{i-1}\ar[r]&x_i}
\xymatrix{
x_i\ar[d]
\\
v_{i}
}
}{
\xymatrix{
\sum x_{<i-1}\ar[r]\ar[d]
&\sum x_{i-1}\ar[r]\ar[d]
&x_{i}\ar[d]
\\
v_{<i-1}
&v_{i-1}
&v_{i}
}
}
\label{eq-update-2}
\eeqa
Note that
actually,
$P(x_i|x_{i-1}, v^n)=
P(x_i|x_{i-1}, v_{\geq i})$
and
$P(x_{i-1}|x_i, v^n)=
P(x_{i-1}|x_i, v_{\leq i})$
but we won't use this fact.
\end{claim}
\proof
\beqa
P(x_i|x_{i-1}, v^n)&=&
\frac{P(x_{i-1}, x_i, v^n)}
{P(x_{i-1}, v^n)}
\\&=&
\frac{
\ol{\calf}_{i-1}(x_{i-1})
\lam_i(x_i)
P(x_i|x_{i-1})\calf_i(x_i)
}{
\ol{\calf}_{i-1}
(x_{i-1})\calf_{i-1}(x_{i-1})
}
\eeqa
Analogous 
proof for Eq.(\ref{eq-update-2}).
\qed


\beqa
P(x^n|v^n) &=&
\prod_{i=1, \ldots, n-1, n} P(x_i|x_{i-1},
 v^n)
\quad \text{(forward propagation)}
\\
&=&
\prod_{i=n+1, n, \ldots 3, 2} 
P(x_{i-1}|x_i, v^n)
\quad\text{(backward propagation)}
\eeqa