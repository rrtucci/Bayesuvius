\chapter{Structure and Parameter Learning for bnets:
 COMING SOON}\label{ch-struc-learn}

Ref.\cite{bnlearn}

\cite{scutari2019}

\section*{Overview}



\dirtree{%
.1 Parameter Learning (PL)-post SL.
.2 missing data.
.1 Structure Learning (SL)-pre PL.
.2 tree-like structures given a priori.
.3 Naive Bayes.
.3 Chow-Liu tree.
.3 Tree Augmented Naive Bayes (TAN).
.3 ARACNE.
.2 score based.
.3 algorithms.
.4 hill climbing (HC).
.4 HC with random restarts.
.4 HC with Tabu list (Tabu).
.4 simulated annealing.
.4 genetic algorithms.
.3 scoring functions.
.4 log-likelihood (LL).
.4 predictive log-likelihood (PLL).
.4 Akaike Information Criterion (AIC).
.4 Bayesian Information Criterion (BIC).
.4 Minimum Description Length (MDL) (same as BIC).
.4 Bayesian Dirichlet (BD) family.
.5 K2 score.
.5 score equivalent Dirichlet posterior density (BDe).
.5 sparse Dirichlet posterior density (BDs).
.5 Dirichlet posterior density based on Jeffrey's prior (BDJ).
.5 modified Bayesian Dirichlet for mixtures of interventional and observational data.
.5 locally averaged BDe score (BDla).
.2 constraint based.
.3 algorithms.
.5 Inductive Causation (IC).
.4 Parents \& Children (PC) family.
.5 PC (the stable version).
.5 Max-Min Parents \& Children (MMPC).
.5 Semi-Interleaved Hiton-PC (SI-HITON-PC).
.5 Hybrid Parents \& Children (HPC).
.4 Grow-Shrink (GS).
.4 IAMB family.
.5 Incremental Association Markov Blanket (IAMB).
.5 Fast Incremental Association (Fast-IAMB).
.5 Interleaved Incremental Association (Inter-IAMB).
.5 Incremental Association with FDR Correction (IAMB-FDR).
.3 conditional independence tests.
.4 mutual information (parametric, semiparametric and permutation tests).
.4 shrinkage-estimator for the mutual information.
.4 Pearson's X2 (parametric, semiparametric and permutation tests).
.4 Jonckheere-Terpstra (parametric and permutation tests).
.4 linear correlation (parametric, semiparametric and permutation tests).
.4 Fisher's Z (parametric, semiparametric and permutation tests).
.2 hybrid.
.3 Max-Min Hill Climbing (MMHC).
.3 Hybrid HPC (H2PC).
.3 General 2-Phase Restricted Maximization (RSMAX2).
.1 parallel programming structure learning.
.1 node types.
.2 discrete.
.3 Categorical (unordered) (multinomial distribution).
.3 ordinal (ordered).
.2 continuous  (multivariate normal distribution).
.2 mixed  (conditional Gaussian distribution).
}
     
\section*{Tidbits}
\hrule\noindent
{\bf linear regression }

$\rvy=$  true value
$\hat{\rvy}=$ estimator
$\ul{\eps}=$ residual

\beq
\hat{\rvy}=
\beta_0 +\sum_{j=1}^N\beta_j \rvx_j
\eeq

\beq
\rvy = \hat{\rvy}+\ul{\eps}
\eeq

\beq
\av{\rvx_j, \ul{\eps}}=0
\eeq

For $k=1, \ldots, N$,
\beq
\av{\rvx_k, \rvy}
=
\sum_{j=1}^N\beta_j\av{\rvx_k, \rvx_j}
\eeq

$\rvx=(\rvx_1, \ldots, \rvx_N)^T$
\beq
\av{\rvx, \rvy}=
\av{\rvx, \rvx^T}\beta
\eeq

\beq
\beta=
\av{\rvx, \rvx^T}^{-1}\av{ \rvx, \rvy}
\eeq
\hrule\noindent
{\bf Categorical and Dirichlet
Distributions} 

Categorical 
distribution Ref.\cite{wiki-categorical}

Dirichlet 
distribution Ref.\cite{wiki-diri}

$q_+=\sum_i q_i$, 
$q.=(q_0, q_1, \ldots, q_{nq-1})$

\beq
Cat(x;\pi.)
=\pi_x
=\prod_k\pi_k^{\indi(k=x)}
\eeq


\beq
Dir(\pi.;\alp.)=
\indi(\pi_+=1)
\Gamma(\alp_+)
\prod_k
\frac{\pi_k^{\alp_k-1}}
{\Gamma(\alp_k)}
\eeq


\beq
Cat(x;\pi.)Dir(\pi.;\alp.)
=
\caln(!\pi.)Dir(\pi.;\alp.')
\eeq

\beq
\alp'_k = \alp_k + \indi(x=k)
\eeq

\beq
P(x|\pi.)=
Cat(x;\pi.)
\eeq

\beq
P(\pi.)=Dir(\pi.;\alp.)
\eeq

\beq
P(x|\pi.)P(\pi.)=
\caln(!\pi.)
P(\pi.|x)
\eeq

\beqa
P(\pi.|x)
&=&\caln(!\pi.)
Cat(x; \pi.) Dir(\pi.;\alp.)
\\
&=&
Dir(\pi.;\alp'.)
\eeqa



\hrule\noindent 

Discrete  nodes only for now

\beq
\xymatrix{
\rvx_i&\ul{\Theta^i}\ar[l]
}
\eeq

\beq
Cat(k;\pi_{\cdot|j}^i)=\pi_{k|j}^i
\eeq

\beq
[\Theta^i]_{k,j}=\pi^i_{k|j}
\eeq

\beq\color{blue}
P(\rvx_i=k|pa(\rvx_i)=j, \Theta^i)=
Cat(k;\pi_{\cdot|j}^i)
\eeq


\beq\color{blue}
P(\Theta^i)=
\prod_jDir(\pi^i_{.|j}; \alp^i_{.|j})
\eeq

\beq
P(\rvx_i=k|pa(\rvx_i)=j, \Theta^i)P(\Theta^i)
=
\caln(!\Theta^i)
P(\Theta^i|\rvx_i=k,pa(\rvx_i)=j)
\eeq

\beqa
P(\Theta^i|\rvx_i=k,pa(\rvx_i)=j)
&=&
\caln(!\Theta^i)
Cat(k;\pi_{\cdot|j}^i)
\prod_{j'}Dir(\pi^i_{.|j'}; \alp^i_{.|j'})
\\
&=&
\prod_{j'}Dir(\pi^i_{.|j'}; \beta^i_{.|j'})
\eeqa

\beq
 \beta^i_{k'|j'}=
 \alp^i_{k'|j'}
+\indi(k=k', j=j')
\eeq



\begin{figure}[h!]
$$
\xymatrix{
&\ul{\Theta}\ar[d]\ar[dl]
\\
\ul{D}&\ul{G}\ar[l]
}
$$
\caption{}
\label{}
\end{figure}

$\ul{G}=$ the DAG, the structure

$\ul{D}=$ samples consisting of 
states of each node

$\ul{\Theta}=$ the TPMs of all nodes, the 
parameters

PL=parameters learning

SL= structure learning


\beq
P(\Theta, G|D)=
\underbrace{P(\Theta|G,D)}_{PL \text{ once know structure $G$}}
\underbrace{P(G|D)}_{SL}
\eeq

SL scores

\beq
score_{B}
=P(G|D)
\eeq

\beq
score_{ML}
=P(D|G)
\eeq

Score each node
independently
\beq
score_{ML}=
\prod_i
P(D_i|G)
\eeq

\beq
P(D_i|G)
=
\sum_{\Theta_i}
P(D_i|G,\Theta_i)P(\Theta_i|G)
\eeq

\beq
P(D_i|G,\Theta_i)=
\caln(!D_i, !\Theta_i)P(\Theta_i)
\eeq

\beq
P(D_i|G)
=\caln(!D_i)
\sum_{\Theta_i}
P(\Theta_i)P(\Theta_i|G)
=\caln(!D_i)
E_{\ul{\Theta}_i}
[P(\Theta_i|G)]
\eeq

\beq
score_{ML}=
\prod_i
E_{\ul{\Theta}_i}
[P(\Theta_i|G)]
\eeq

$P(\Theta_i)=$
Dirichlet

$P(D_i|\Theta_i)=$ Categorical

$P(\Theta_i|D_i)=$ Dirichlet again (Dir is
conjuagte prior of Cat)

