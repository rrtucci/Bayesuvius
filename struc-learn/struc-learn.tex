\chapter{Structure and Parameter Learning for bnets:
 COMING SOON}\label{ch-struc-learn}

Ref.\cite{bnlearn}

\cite{scutari2019}

\section*{Overview}



\dirtree{%
.1 Parameter Learning.
.2 missing data.
.1 Structure Learning.
.2 tree-like structures given a priori.
.3 Naive Bayes.
.3 Chow-Liu tree.
.3 Tree Augmented Naive Bayes (TAN).
.3 ARACNE.
.2 score based.
.3 algorithms.
.4 hill climbing (HC).
.4 HC with random restarts.
.4 HC with Tabu list (Tabu).
.4 simulated annealing.
.4 genetic algorithms.
.3 scoring functions.
.4 Information Theoretic scores.
.4 Bayesian Information Criterion (BIC).
.4 Bayesian Dirichlet (BD) family.
.2 constraint based.
.3 algorithms.
.4 Parents \& Children (PC) family.
.4 Grow-Shrink (GS).
.4 Incremental Association Markov Blanket (IAMB) family.
.3 conditional independence tests.
.4 mutual information (parametric, semiparametric and permutation tests).
.4 shrinkage-estimator for the mutual information.
.2 hybrid.
.3 Max-Min Hill Climbing (MMHC).
.3 Hybrid HPC (H2PC).
.3 General 2-Phase Restricted Maximization (RSMAX2).
.1 parallel programming structure learning.
.1 node types.
.2 all-discrete.
.2 all-continuous.
.2 mixed.
}
     

\hrule
Let
\begin{itemize}
\item
PL=parameters (i.e, the TPMs) learning
\item
SL= structure (i.e., the DAG) learning
\end{itemize}

PL is easy, once the
structure is known. PL 
assuming no missing data
goes as follows.
Using the notation of Chapter 
\ref{ch-scoring}, define
\beq
\pi_\kbarmu^i=
P(\rvx_i=k\cond pa(\rvx_i)=\mu )
\;.
\eeq
Then $\pi_\kbarmu^i$
can be estimated
from the data $N^i_\kmu$ 
as follows:

\beq
\pi_\kbarmu^i
\approx 
N^i_\kbarmu
=
\frac{N^i_\kmu}{N^i_\plusmu}
\;.
\label{eq-PL-simple}
\eeq
PL
described by Eq.(\ref{eq-PL-simple})
 is only for discrete nodes
with no missing data.
bnlearn can also do PL
with missing data
and continuous 
(Gaussian linear only) nodes. 
See Chapter \ref{ch-missing-d}
on missing data
and Chapter \ref{ch-gauss-lin}
on Gaussian linear nodes.

SL
actually
does PL and SL
at the same time.  
There are 3 main 
types of SL: score based,
constraint based, and hybrid score-constraint
based.
bnlearn can perform many
 algorithms
of each of these 3 types
of SL.
Some
algorithms can
handle either all-discrete,
or all-continuous
or mixed nodes.

Score based SL methods
require
scoring bnets (with either
all-discrete, all-continuous
or mixed nodes).
See Chapter \ref{ch-scoring}
for an introduction
to scoring bnets.
The BIC score
explained
in that chapter is
very popular
and works for all-discrete,
all-continuous or mixed nodes.


Constraint based SL methods
require
estimating from the data
the conditional independence
$\rvx.\perp_P \rvy.|\rva.$ 
for any 3 disjoint
multinodes $\rvx., \rvy., \rva.$.
This can be done by
estimating the conditional
mutual information (CMI)
$H(\rvx.:\rvy.|\rva.)$.
bnlearn can calculate CMI and
other metrics
of $\rvx.\perp_P \rvy.|\rva.$.
All these metrics are very
similar; they 
all measure
how close
$P(x.|y., a.)$
and $P(x.|a.)$ are.

\section*{Tidbits}




