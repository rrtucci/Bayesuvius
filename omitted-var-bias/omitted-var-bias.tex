\chapter{Omitted Variable Bias}
\label{ch-omitted-var-bias}

This paper is
loosely based on Refs.\cite{cinelli-haz}
and \cite{cherno-cinelli}.

This chapter assumes that the
reader has read Section 
\ref{sec-conv-lr}
on Linear Regression (LR)
and Section \ref{intro-pot-out-sensitivity}
which is an introduction
to sensitivity analysis
for the Potential Outcomes (PO)
model.

We will
 use the terms {\bf ``PO Sensitivity Analysis}"
and {\bf ``Omitted Variable Bias (OVB)"}
to mean the same thing.
In this chapter,
we consider 2 types 
of OVB. LDEN bnets for
these two cases are depicted in Fig.\ref{fig-ovb-two-lden}.
In Fig.\ref{fig-ovb-two-lden}$(a)$, the omitted variable
is a confounder,
and in Fig.\ref{fig-ovb-two-lden} $(b)$,
it's a mediator.

Next we express OVB for these two cases as
a product of gains along a path
that goes through the unobserved node, and then we re-express the OVB 
in terms of correlations between the nodes.
 


\begin{figure}[h!]
$$
\begin{array}{cc}
\xymatrix{
\eps_\rvd\ar[ddd]
&\eps_\rvc\ar[d]
&\eps_\rvy\ar[ddd]
&\eps_\rvx\ar[ddll]
\\
&*++[F-o]{\rvc}\ar[ldd]_{\alp'}
\ar[rdd]^{\beta'}
\\
&\rvx
\ar[ld]^\alp
\ar[rd]_\beta
\\
\rvd\ar[rr]_\delta
&&\rvy
}
&
\xymatrix{
\eps_\rvd\ar[ddd]
&\eps_\rvm\ar[d]
&\eps_\rvy\ar[ddd]
&\eps_\rvx\ar[ddll]
\\
&*++[F-o]{\rvm}
\ar[rdd]^{\mu}
\\
&\rvx
\ar[ld]^\alp
\ar[rd]_\beta
\\
\rvd\ar[rr]_\delta
\
\ar[ruu]^\lam
&&\rvy
}
\\
(a)&(b)
\end{array}
$$
\caption{LDEN bnets used to do PO
sensitivity analysis.
Node $\rvc$ in $(a)$
is an unobserved confounder,
and node $\rvm$ in $(b)$
is an unobserved mediator.}
\label{fig-ovb-two-lden}
\end{figure}







\hrule
\noindent {\bf CASE $(a)$, confouder}

Consider the LDEN bnet of Fig.\ref{fig-ovb-two-lden}$(a)$,
whose structural equations,
printed in blue, are as follows:

\beq
\color{blue}
\left\{
\begin{array}{l}
\rvc = \rveps_\rvc
\\
\rvx = \eps_\rvx
\\
\rvd = \alp\rvx +\alp'\rvc + \eps_\rvd
\\ 
\rvy = \delta \rvd +
\beta \rvx + \beta'\rvc
+\eps_\rvy
\end{array}
\right.
\eeq

In Section \ref{intro-pot-out-sensitivity},
we showed that
for confounder case $(a)$,
\beq
\boxed{
OVB_{con}=ATE-ATE|_{\beta'=0}=\frac{\beta'}{\alp'}
}
\eeq
This result is easy to understand.
$ATE|_{\beta'=0}=\delta$ due to the
directed path $\rvd\rarrow\rvy$,
whereas $ATE = \delta +\frac{\beta'}{\alp'}$
due to the two directed paths 
$\rvd\rarrow \rvy$ and
$\rvd\rarrow \rvc\rarrow\rvy$.
Note that traveling from $\rvc$ to $\rvd$ has gain $\alp'$,
so traveling in
the opposite direction has gain $\frac{1}{\alp'}$.

Next
we express $OVB_{con}$
in terms of correlations
between the nodes of the bnet.



\begin{claim}
\beq
OVB_{con}=
\frac{\beta'}{\alp'}
=
\frac{
[\rho_{\rvy,\rvc}\s_\rvy]^{|d,x}
}{
[\rho_{\rvd,\rvc}\s_\rvd]^{|x}
}
\label{eq-bias-confounder}
\eeq
If $\s_{\eps_\rvm}=\s_{\eps_\rvd}=0$,
then $\rho_{\rvy,\rvc}=\rho_{\rvd, \rvc}=1$
and  

\beq
OVB_{con}=
\frac{
\s_\rvy^{|d,x}
}{
\s_\rvd^{|x}
}
\eeq

\end{claim}
\proof\footnote{We use ``$|x$"
at the end of a line
to mean all
averages in that
line are taken
at fixed $\rvx=x$.}

Note that

\beq
\av{\rvc, \rveps_\rvd}=0\quad\quad|x
\label{eq-first-alp-prime-eq}
\eeq
because the paths from $\rvc$ to
$\rveps_\rvd$ are blocked by colliders. 
Hence,

\beq
\av{\rvd, \rveps_\rvd}=
\av{\alp'\rvc +\rveps_\rvd, \rveps_\rvd}=
\av{\rveps_\rvd, \rveps_\rvd}\quad\quad|x
\eeq 

\beqa
\av{\rvd, \rvd}
&=&
\av{\alp'\rvc +\rveps_\rvd,
\alp'\rvc +\rveps_\rvd}=
(\alp')^2\av{\rvc,\rvc}
+
\av{\rveps_\rvd, \rveps_\rvd}
\quad\quad|x
\eeqa

\beq
\s_\rvd = |\alp'|\s_\rvc
\sqrt{1 + \left(\frac{\s_{\rveps_\rvd}}
{\alp'\s_\rvc}\right)^2}
\quad\quad|x
\eeq

\beqa
\rho_{\rvd, \rvc}
&=&
\frac{\av{\rvd,\rvc}}{\sqrt{
\av{\rvd,\rvd}\av{\rvc, \rvc}
}}
\quad\quad|x
\\
&=&
\frac{\av{\alp'\rvc +\rveps_\rvd,\rvc}}
{\sqrt{
\av{\alp'\rvc +\rveps_\rvd,
\alp'\rvc +\rveps_\rvd}\av{\rvc, \rvc}
}}
\quad\quad|x
\\
&=&
\frac{\alp'\av{\rvc,\rvc}}
{\sqrt{
((\alp')^2\av{\rvc,\rvc}
+
\av{\rveps_\rvd, \rveps_\rvd})
\av{\rvc, \rvc}
}}
\quad\quad|x
\\
&=&
\frac{\alp'\s_\rvc}
{\sqrt{
(\alp'\s_\rvc)^2
+
\s_{\rveps_\rvd}^2
}}
\quad\quad|x
\\
&=&
\frac{\sign(\alp')}
{\sqrt{1+
\left(
\frac{\s_{\rveps_\rvd}}
{\alp'\s_\rvc}\right)^2
}}
\quad\quad|x
\eeqa

\beq
\av{\rvc, \rvd}=\alp'\av{\rvc, \rvc}
\quad\quad|x
\eeq

\beq
\alp' =
\frac{\av{\rvc,\rvd}}{\av{\rvc,\rvc}}
=
\partial_\rvc\rvd 
=
\rho_{\rvc, \rvd}
\frac{\s_\rvd}{\s_\rvc}
\quad\quad|x
\eeq

All equations between Eq.(\ref{eq-first-alp-prime-eq})
and this point,
remain valid
if we make the following
replacements:
\beq
\begin{array}{lll}
\rvd&\rarrow& \rvy
\\
\rveps_\rvd&\rarrow& \rveps_\rvy
\\
\rvc&\rarrow&\rvc
\\
\alp'&\rarrow& \beta'
\\ 
|x&\rarrow& |d,x
\end{array}
\eeq
In particular,
the following are true

\beq
\s_\rvy = |\beta'|\s_\rvc
\sqrt{1 + \left(\frac{\s_{\rveps_\rvy}}
{\beta'\s_\rvc}\right)}
\quad\quad|d,x
\eeq

\beq
\rho_{\rvy,\rvc}
=
\frac{sign(\beta')}
{\sqrt{1+
\left(
\frac{\s_{\rveps_\rvy}}
{\beta'\s_\rvc}\right)^2
}}
\quad\quad|d,x
\eeq

\beq
\beta' =
\partial_\rvc\rvy =
\rho_{\rvy,\rvc}
\frac{\s_\rvy}{\s_\rvc}
\quad\quad|d,x
\eeq

Combining our newly found expressions for
for $\alp'$ and $\beta'$,  we get

\beq
\frac{\beta'}{\alp'}
=
\left[
\frac{\rho_{\rvy,\rvc}\s_\rvy}{\s_\rvc}
\right]^{|d,x}
\left[
\frac{\s_\rvc}{
\rho_{\rvd,\rvc}\s_\rvd}
\right]^{|x}
\eeq

Now note that
\beq
\s_\rvc^2 = \av{\rveps_\rvc, \rveps_\rvc}
\eeq
which is independent
of $\rvd$ and $\rvx$, so

\beq
\s_\rvc^{|x}=\s_\rvc^{|d,x}
\eeq
Therefore,

\beq
\frac{\beta'}{\alp'}
=
\frac{
[\rho_{\rvy,\rvc}\s_\rvy]^{|d,x}
}{
[\rho_{\rvd,\rvc}\s_\rvd]^{|x}
}
\eeq
\qed



\hrule
\noindent {\bf CASE $(b)$, mediator}

Consider the LDEN bnet of Fig.\ref{fig-ovb-two-lden}$(b)$,
whose structural equations,
printed in blue, are as follows:

\beq
\color{blue}
\left\{
\begin{array}{l}
\rvm = \lam\rvd +\eps_\rvm
\\
\rvx = \eps_\rvx
\\
\rvd = \alp\rvx + \eps_\rvd
\\ 
\rvy = \delta \rvd +
\beta \rvx 
+\mu \rvm
+\eps_\rvy
\end{array}
\right.
\eeq

By an argument
analogous to the one used
previously in
this chapter to 
prove that $OVB_{con}=\frac{\beta'}{\alp'}$,
we get
\beq
\boxed{
OVB_{med}=ATE-ATE|_{\mu=0}=\lam\mu}
\eeq

Next
we express $OVB_{med}$
in terms of correlations
between the nodes of the bnet.


\begin{claim}\footnote{Eq.(\ref{eq-bias-mediator})
appears in Ref.\cite{cinelli-haz},
but that paper claims it 
gives the bias for unobserved confounders instead
of unobserved mediators. But the formula for the bias for unobserved confounders is given by Eq.(\ref{eq-bias-confounder}).
}
\beq
OVB_{med}=
\lam\mu=
\frac{
[\rho_{\rvy, \rvm}\s_\rvy]^{|d,x}}
{\s_{\rvd}}
\underbrace{\left[\frac{\rho_{\rvd, \rvm}}{\sqrt{
1-\rho^2_{\rvd,\rvm}
}}\right]
}_{\tan\theta}
\label{eq-bias-mediator}
\eeq
If $|\tan\theta|\leq \eta$, 
then\footnote{Recall that,
by the Schwarz Inequality, $|\rho_{\rva, \rvb}|\leq 1$ 
for any correlation coefficient 
$\rho_{\rva,\rvb}=\frac{\av{\rva,\rvb}}
{\s_\rva \s_\rvb}$.}

\beq 
|OVB_{med}| \leq 
\left|\frac{\s_\rvy^{|d,x}}{\s_{\rvd}}
\right|
\eta
\eeq
\end{claim}
\proof

Note that
\beq
\av{\rvd, \eps_\rvm}=0
\eeq
because paths from $\rvd$ to $\rveps_\rvm$
are blocked by a collider. Hence,

\beq
\av{\rvd, \rvm}=\lam\av{\rvd, \rvd}
\eeq

\beq
\lam =
\frac{\av{\rvd,\rvm}}{\av{\rvd,\rvd}}
=
\pder{\rvm}{\rvd}=
\rho_{\rvd, \rvm}
\frac{\s_\rvm}{\s_\rvd}
\eeq

Note that also

\beq
\av{\rvm, \eps_\rvy}=0
\quad\quad |d,x
\eeq
because the paths from
$\rvm$ to $\eps_\rvy$
are blocked by a collider.
Hence,

\beq
\av{\rvm, \rvy}=\mu\av{\rvm, \rvm}
\quad\quad|d,x
\eeq

\beq
\mu 
=
\frac{\av{\rvm,\rvy}}{\av{\rvm,\rvm}}
= \pder{\rvy}{\rvm}
=
\rho_{\rvy, \rvm}
\frac{\s_\rvy}{\s_\rvm}
\quad\quad|d,x
\eeq
Combining
our newly found
expressions for $\lam$ and $\mu$,
we get

\beq
\lam\mu=
\rho_{\rvd, \rvm}
\frac{\s_\rvm}{\s_\rvd}
\left[\rho_{\rvy, \rvm}
\frac{\s_\rvy}{\s_\rvm}\right]^{|d,x}
\eeq
Let

\beq
e_\lam =
\frac{\s_{\rveps_\rvm}}{
\lam\s_\rvd}
\eeq
Then
\beqa
\s_\rvm^2
&=&
\av{\rvm, \rvm}
\\
&=&
\av{\lam\rvd+\rveps_\rvm,
\lam\rvd+\rveps_\rvm}
\\
&=&
\lam^2\av{\rvd,\rvd}
+\av{\rveps_\rvm, \rveps_\rvm}
\\
&=&
\lam^2\s_\rvd^2
\left(1 + 
e_\lam^2
\right)
\;,
\eeqa
and, since $[\s_\rvd^2]^{|d,x}=0$, we have

\beq
[\s_{\rvm}^{|d,x}]^2
=
\s^2_{\rveps_\rvm}
\eeq
Hence,

\beq
\frac{\s_\rvm}
{\s_\rvm^{|d,x}}
=
\frac{1}
{e_\lam}
\sqrt{
1 + 
e_\lam^2
}
\eeq
Furthermore,

\beqa
\av{\rvd, \rvm}
&=&
\av{\rvd, \lam\rvd +\rveps_\rvm}
\\
&=&
\lam\s_\rvd^2
\eeqa
so

\beqa
\rho_{\rvd, \rvm}
&=&
\frac{\av{\rvd, \rvm}}
{\s_\rvd\s_\rvm
}
\\
&=&
\frac{\lam\s_\rvd}{\s_\rvm}
\\
&=&
\frac{1}
{
\sqrt{
1 + 
e_\lam^2
}
}
\eeqa
Hence,

\beq
\sqrt{1-\rho^2_{\rvd,\rvm}}
=
\frac{
e_\lam}
{
\sqrt{
1 + e_\lam^2
}
}
=
\left(
\frac{\s_\rvm}
{\s_\rvm^{|d,x}}
\right)^{-1}
\eeq

\beq
\lam\mu
=
\frac{[\rho_{\rvy, \rvm}\s_\rvy]^{|d,x}}{\s_\rvd}
\frac{\rho_{\rvd, \rvm}}{\sqrt{
1-\rho^2_{\rvd,\rvm}
}}
\eeq
\qed
\hrule

Note that both $OVB_{con}$ and $OVB_{med}$
are equal to a product 
$\calf_\rvd\calf_\rvy$ of two
factors $\calf_\rvd$ and $\calf_\rvy$.
For $OVB_{con}$

\beq
\calf_\rvy = [\rho_{\rvy,\rvc}\s_\rvy]^{|d,x},
\quad \calf_\rvd =
\frac{1}{[\rho_{\rvd,\rvc}\s_\rvd]^{|x}}
\eeq
For $OVB_{med}$, 
\beq
\calf_\rvy = [\rho_{\rvy,\rvm}\s_\rvy]^{|d,x},
\quad
\calf_\rvd =
\frac{\rho_{\rvd, \rvm}}{\s_\rvd\sqrt{
1-\rho^2_{\rvd,\rvm}}}
\eeq
$\calf_\rvy$ is the same
for the confounder and mediator cases, except that the node $\rvc$ in $\calf_\rvy$ 
for the confounder case is changed to $\rvm$ in the mediator case. This is to be expected, because 
in both cases the arrow from
the unobserved node points into
$\rvy$.
On the other hand,
$\calf_\rvd$ is different
for the confounder and mediator cases,
because in the confounder case, the unobserved node $\rvc$ points into $\rvd$,
whereas  in the mediator case,
$\rvd$ points into the unobserved node $\rvm$.



%\hrule
%On reference \cite{cherno-cinelli}
%
%
%
%
%\beqa
%A_{\rvd, \rvx} &=&
%1-\rvd\partial_\rvd
%-\rvx\partial_\rvx
%\\
%&=&
%\rvc \partial_\rvc
%\\
%&=&
%1-A_\rvc
%\eeqa
%
%\beq
%A_\rvx = 
%1 -\rvx \partial_\rvx
%\eeq
%
%
%
%\beqa
%\beta' &=& 
%\partial_{A_{\rvd,\rvx}\rvc}
%A_{\rvd, \rvx}\rvy
%\\
%&=&
%\partial_\rvc
%A_{\rvd, \rvx}\rvy
%\\
%&=&
%\rho_{\rvc,A_{\rvd, \rvx}\rvy}
%\frac{\s_{A_{\rvd, \rvx}\rvy}}{\s_\rvc}
%\eeqa
%
%
%\beqa
%\alp'
%&=&
%\partial_{A_{\rvx}\rvc}
%A_{\rvx}\rvd
%\\
%&=&
%\partial_\rvc
%A_{\rvx}\rvd
%\\
%&=&
%\rho_{\rvc,A_{\rvx}\rvd}
%\frac{\s_{A_\rvx\rvd}}
%{\s_\rvc}
%\eeqa
%
%\beq
%\frac{\beta'}{\alp'}=
%\frac{\rho_{\rvc,A_{\rvd, \rvx}\rvy}}
%{\rho_{\rvc, {A_\rvx\rvd}}}
%\quad
%\frac{\s_{A_{\rvd,\rvx}\rvy}}
%{\s_{A_\rvx\rvd}}
%\eeq
%
%
%
%\beqa
%\frac{\av{A_{\rvd, \rvx}\rvc,
%A_{\rvd, \rvx}\rvc}}
%{\av{A_\rvx\rvc, A_\rvx\rvc}}
%&=&
%\frac{\av{\rvc,
%\rvc}}
%{\av{A_\rvx\rvc, A_\rvx\rvc}}
%\eeqa
%
%
%\beqa
%\rho_{A_\rvx\rvc, A_\rvx\rvd}
%&=&
%\partial_{A_\rvx\rvd}
%(A_\rvx\rvc)
%\;
%\partial_{A_\rvx\rvc}
%(A_\rvx\rvd)
%\\
%&=&
%\rho_{\rvc, A_\rvx\rvd}
%\eeqa
%
%\beqa
%1-\rho_{A_\rvx\rvc, A_\rvx\rvd}^2
%&=&
%\frac{\av{A_\rvx\rvc,A_\rvx\rvc}
%\av{ A_\rvx\rvd, A_\rvx\rvd}-
%\av{A_\rvx\rvc,A_\rvx\rvd}^2
%}
%{\av{A_\rvx\rvc,A_\rvx\rvc}
%\av{ A_\rvx\rvd, A_\rvx\rvd}}
%\\
%&=&
%\frac{
%\av{ A_\rvx\rvc, A_\rvx\rvc}
%-\frac{\av{A_\rvx\rvc,A_\rvx\rvd}^2}
%{\av{ A_\rvx\rvd, A_\rvx\rvd}}
%}
%{\av{A_\rvx\rvc,A_\rvx\rvc}}
%\\
%&=&
%\frac{\av{\rvc,\rvc}}
%{\av{A_\rvx\rvc,A_\rvx\rvc}}
%\eeqa
%
%\beq
%\Delta(d) = \indi(d=1)-\indi(d=0)
%\eeq
%
%Long $V=(d,x,c)$, Short $V^S=(d^S,x^S)$
%\beq
%A_\rvc(\rvc, \rvx, \rvd, \rvy)=
%(0, \rvx^S, \rvd^S, \rvy^S)
%\eeq
%
%\beqa
%\underbrace{\alp(V)}_
%{\rarrow\alp^S(V^S)}
%&=&
%\;\frac{-1}{d-E_{|x,u}[\rvd]}
%\\
%&=&
%\;\frac{-1}{d-\sum_{d'=0}^1d'P(d'|x,u)}
%\\
%&=&
%\underbrace{\frac{\Delta(d)}
%{P(d|x,u)}
%}_{\rarrow \frac{\Delta(d^S)}
%{P(d^S|x^S)}}
%\eeqa
%
%
%
%\beqa
%\underbrace{\caly_{|V}}_
%{\rarrow \caly^S_{|V^S}}
%&=&
%\underbrace{E_{|d,x,u}[\rvy]}_
%{\rarrow E_{|d,x}[\rvy^S]}
%\\
%&=&
%E_{|x,u}[\rvy(d)]
%\\
%&=&
%\sum_{y(d)}P(y(d)|x,u)y(d)
%\\
%&=&
%\underbrace{\caly_{d|x,u}}_
%{\rarrow \caly^S_{d^S|x^S}}
%\eeqa
%
%
%\beq
%\underbrace{\caly_{d}}_{
%\rarrow\caly_{d^S}^S}
%=
%\underbrace{\sum_{x,u}P(x,u)
%\caly_{d|x}}_{
%\rarrow
%\sum_{x^S}P(x^S)
%\caly^S_{d^S|x^S}
%}
%\eeq
%
%
%
%\begin{align}
%\underbrace{E_{\rvy,\rvV}[\rvy\alp(\rvV)]}_
%{\rarrow E_{\rvy^S,\rvV^S}[\rvy^S\alp(\rvV^S)]}
%&=
%E_{\rvV}\left[\alp(\rvV)
%E_{\rvy|\rvV}
%[\rvy]
%\right]
%\\
%&=
%E_{\rvV}\left[\alp(\rvV)
%\caly_{|\rvV}
%\right]
%\\
%&=
%E_{\rvx,\rvc}\left[E_{\rvd|\rvx,\rvc}
%\left[\alp(\rvV)
%\caly_{\rvd|\rvx,\rvc}\right]
%\right]
%\\
%&=
%\sum_{x,u} P(x,u)
%\sum_d P(d|x,u)
%\frac{\Delta(d)}
%{P(d|x,u)}
%\caly_{d|x,u}
%\\
%&=
%\underbrace{\caly_{d=1}-\caly_{d=0}}_
%{\rarrow
%\caly_{d^S=1}^S-\caly_{d^S=0}^S}
%\end{align}
%
%\beq
%\underbrace{ATE}_
%{\rarrow ATE^S} 
%=
%\underbrace{\caly_{1}-\caly_{0}}_
%{\rarrow\caly_{1}^S-\caly_{0}^S}
%\eeq
%
%\begin{claim}
%
%\beqa
%ATE-ATE^S
%&=&
%\av{\alp(V)-\alp^S(x^S),
%\caly_{|V}+\caly^S_{|V^S}}_{\rvV, \rvV^S}
%\\
%&=&
%\rho B
%\eeqa
%where
%
%\beq
%\rho = \rho_{\alp(V)-\alp^S(x^S),
%\caly_{|V}+\caly^S_{|V^S}}
%\eeq
%
%\beq
%B=B_\alp
%B_\caly
%\eeq
%
%\beq
%B_{\alp} = \s_{\alp(V)-\alp^S(V^S)}
%\eeq
%
%\beq
%B_\caly = \s_{\caly_{|V}+\caly^S_{|V^S}}
%\eeq
%
%\end{claim}
%\proof
%
%\beqa
%ATE-ATE^S
%&=&
%E_{\rvV, \rvV^S}\left[\alp(V)\caly_{|V}-
%\alp(V^S)\caly_{|V^S}^S
%\right]
%\eeqa
%
%\beqa
%\underbrace{E_\rvV[\alp(V)]}_
%{\rarrow E_{\rvV^S}[\alp(V^S)]}
%&=&
%\sum_{d,x,u}P(x,u)P(d|x,u)
%\frac{\Delta(d)}{P(d|x,u)}
%\\
%&=&
%\sum_{d,x,u}P(x,u)\Delta(d)
%\\
%&=&
%\underbrace{0}_
%{\rarrow 0}
%\eeqa
%
%\beq
%E_{\rvV, \rvV^S}[\alp(V)\caly^S_{|V^S)}]=
%E_{\rvV, \rvV^S}[\alp^S(V^S)
%\caly_{|V}]=0
%\eeq
%\qed
%
%
