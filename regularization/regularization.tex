\chapter{Regularization of Cost Functions}
\label{ch-regularization}


 The topic
 of this chapter is Regularization of Cost functions (RC).

RC is the practice of adding 
a convex function  $\calr:\RR^n\rarrow \RR$
(called the {\bf regulator function}\footnote{This function is also commonly called a {\bf penalty function}.
It can  also be thought  of, from a Bayesian perspective,
as the log of a prior probability, and the cost function can be thought of as a log likelihood function.}) to
a convex function  $\calc:\RR^n\rarrow \RR$
(called the {\bf  cost function}\footnote{This function is also commonly
called the {\bf loss function}.}), when one
wishes to minimize the cost function. In Machine Learning,
the variable being minimized over is often the weights 
of a Neural Net, so we will denote it by $w\in \RR^n$, and use $w^*\in \RR^n$
to represent the
minimum of $\calc^+(w) =\calc(w)+ \calr(w)$. 

In some cases (like in $L_p$ norm RC,
which we discuss below),
the minimum of $\calc^+(w)$
and that of $\calc(w)$
are not the same but very close. In such cases,
displacing the minimum of 
$\calc(w)$ 
might be done in order
to avoid overfitting, or
to spread out degenerate solutions, or to produce
sparse solutions. 

In other cases (like in proximal RC, which we discuss below),
the minimum of $\calc^+(w)$
and that of $\calc(w)$
are the  same. 
In such cases, RC might be done
to improve the
convergence properties
of a sequence of points
$\{w_k\}_{k=0}^\infty$
such that $w_k\rarrow w^*$
as $k
\rarrow \infty$.

There are many methods
of biasing the 
minimum of a convex function that  don't
involve adding a regulator
function.
For example, Early Stopping
of training 
and Cross Validation for Neural Nets,
or adding Latent Variables
to a Bayesian Network.
Or Constraint Optimization where hard equality and/or inequality constraints are imposed (as in Linear Programming,
Lagrange multipliers, Khun-Tucker conditions, method of simple substitution of constraints).
We won't discuss those types of
RCs in this chapter\footnote{RC that adds a regulator function (resp., doesn't add)
is sometimes called {\bf Explicit (resp., Implicit)
regularization}.}, except for a brief section on latent nodes at the end.



Cost functions commonly used in Statistics and
Machine Learning (ML) are of the form


\beq
\calc(w) = \sum_{\s=1}^{nsam}
\HAT{\calc}(\haty_\s(x_\s, w), y_\s)
\eeq
where the sum is over $nsam$ samples.
A common, more specific type of $\calc(w)$ is


\beq
\calc(w) = \frac{1}{nsam}\sum_{\s=1}^{nsam}
\norm{\haty_\s(x_\s, w)-y_\s)}_2
\eeq
This cost function is convex, so it has a minimum:

\beq
\left\{
\begin{array}{l}
w^* = \argmin_w \calc(w)
\\
Cost = \min_w \calc(w) = \calc(w^*)
\end{array}
\right.
\eeq
In RC, we add a regulator $\calr(w)$ to $\calc(w)$:

\beq
\calc^+(w) = \calc(w) + \calr(w)
\eeq



\section{$L_p$ norm RC}

See \ref{sec-p-norm}
for the definition of
$L_p$ norms.  Let

\beq
\calc^+(w) = \calc(w) + \calr(w)
\eeq
Then,
for $\lam, \lam_1, \lam_2>0$,


\begin{itemize}
\item $L_1$ norm RC (called LASSO or Basis Pursuit)
\beq
\calr(w) =\lam\norm{w}_1
\eeq

\item $L_2$ norm RC (called Ridge or Tikhonov Regression) (note that the $L_2$ norm is squared)
\beq
\calr(w) =\lam\norm{w}_2^2
\eeq

\item
$L_1 + L_2$ norm RC (called Elastic Net)
\beq
\calr(w) =\lam_1\norm{w}_1
+ \lam_2\norm{w}_2^2
\eeq
\end{itemize}

\subsection{$L_1$ norm RC can lead to sparsity}

The obvious way to induce sparsity in 
the minimum $w^*$ is to use the $L_0$ norm of $w$ as regulator.
However, calculating $\norm{w}_0$ is
hard (NP-hard, in fact). In this section, we will
show that using the $L_1$ norm
of $w$ as regulator can also induce sparsity (not as well as the
$L_0$ norm, but still significant.)
\begin{figure}[h!]
\centering
\includegraphics[width=4in]
{regularization/sparsity.png}
\caption{Pictorial explanation of
why $L_1$ norm RC can lead to sparsity
and $L_2$ norm RC can help avert it. Sometimes 
you want sparsity and sometimes you don't. $c$ in this figure is some 
fixed constant.
}
\label{fig-sparsity}
\end{figure}

Assume $\calc(w)$ is linear in $w$.\footnote{What we say here about sparsity also applies in some cases when $\calc(w)$ is not linear in $w$. For example, it applies sometimes when $\calc(w)$
is quadratic in $w$ as occurs in Least Squares.}
For example, 
\beq
\calc(w)= Xw-y
\eeq
where $w\in\RR^n$, $X\in \RR^{nsam\times n}$,  and $y\in\RR^{nsam}$.
$nsam$ is often the number of samples. That's why we name it that way.
The set $A=\{w: Xw-y=c\}$ for some fixed constant $c$  might be empty,
or contain a single point or a line or a plane, or a hyperplane. Let $dof=n-nsam$
be the degrees  of freedom in $w$.
Barring some exceptional cases, if $dof\leq 0$, we expect $A$ to be empty.
If $dof=1$, we expect $A$ to trace out a line, if $dof= 2$, we expect $A$ to
trace out a plane, and so forth.

In Fig.\ref{fig-sparsity}, we imagine
what would happen if
$w\in \RR^2$, $X\in \RR^{1\times 2}$
and $y\in \RR$ so $A$ traces a line,
represented in green.
In Fig.\ref{fig-sparsity} $(a)$,
we imagine that $\calr(w)=\norm{w}_1$
and in Fig.\ref{fig-sparsity} $(b)$, that
$\calr(w)=\norm{w}_2^2$.
For points $w$ such that $\calr(w)$
has a well defined gradient, 
minimization of 
\beq
\calc^+(w) = \calc(w) +\calr(w)
\eeq
requires that the $w$-gradients of
$\calc$ and $\calr$ be equal in magnitude but opposite in direction.

\beq
\nabla_w\calc = -\nabla_w \calr
\eeq
But when $\calr(w)$ is
the $L_1$ norm,  $\nabla_w\calr$ is not defined along the $w_1$
and $w_2$ axes. To avoid this, we can 
approximate the diamond contour $\norm{w}_1=c$
by rounding out its corners
by an infinitesimal amount. If the green line had 
slope of $-1$,
there would be a diamond contour
 $\norm{w}_1=c$ that would coincide with the green line along the 
 segment connecting points $(0,c)$
 and $(c,0)$. However, this is an exceptional case. Usually, the slope
 of the green line is not $\pm 1$.
 In that case, the only way for 
 $\calc$ and $\calr$ to have
 opposite gradients is if the diamond 
 contour and the green line 
 kiss at one of the (rounded)  corners
 of the diamond contour. Call that
 kissing point $w^*$. Note from
 Fig.\ref{fig-sparsity} $(a)$ that that kissing point $w^*=(0,c)$ would be
 sparse. In going from the Fig.\ref{fig-sparsity} $(a)$ to Fig.\ref{fig-sparsity} $(b)$,
 we have replaced the diamond contour
 $\norm{w}_1=c$ by
 a circular contour $\norm{w}_2=c$,
 but we have kept the same green line.
 Note that with the circular contour,
 the kissing point $w^*$ is no longer sparse; both of its components are non-zero.
 
 The moral of Fig.\ref{fig-sparsity}
 is that $L_1$ norm RC can lead to sparsity and $L_2$ norm RC can help avert it.
 Sometimes we want the vector of weights $w$ to be sparse so as to give
 a succinct description.
 At other times, we want solutions in set $A$ to be spread out over many dimensions instead of being sparse and clumped together in a small number of dimensions.






\subsection{$L_2$ norm RC for Least Squares}
As in Section \ref{sec-conv-lr} on Linear Regression, 
suppose $w\in\RR^n$, $X\in \RR^{nsam\times n}$,
 and  $y\in\RR^{nsam} $. Let

\beq
\calc^+(w)=\calc(w) + \calr(w)
\eeq
where

\beq
\calc(w)=
\frac{1}{n}
(Xw-y)^T(Xw-y)
\eeq
and

\beq
\calr(w) =\lam\norm{w}^2_2
\;.
\eeq
If we vary the  vector $w$ by an
infinitesimal amount $\delta w^T$, we get

\beq
0= \delta\calc^+(w)=
\delta w^T
\left[\frac{2X^T}{n}(Xw-y) +2 \lam w \right]
\eeq
Hence

\beq
X^T(Xw-y)+\lam n  w=0
\eeq

\beq
(X^TX+\lam n)w = X^T y
\eeq

\beqa
w&=&
(X^TX + \lam  nI)^{-1}X^Ty
\\
&=&
\frac{1}{\lam n}(I + 
\frac{X^T X}{\lam n})^{-1}X^Ty
\eeqa

Note that for $\lam=0$, the minimum $w^*$ of $\calc^+(w)$ is

\beq
w^*= (X^TX)^{-1}X^Ty 
\;\;\text{(valid for $\lam=0$)}
\eeq
When $\lam>>1$,
we can express $w^*$  as a Taylor expansion in
$w$. 
Recall that if $|\eps|<1$,
\beq
\frac{1}{1-\eps}=
1 + \eps + \eps^2 + \ldots
\eeq
Define
$n_0 = 1/\lam$,
and assume that
$\frac{n_0}{n}|X^TX| <1  $.
Then

\beqa
w^* &=& \frac{n_0}{n}
\sum_{i=0}^{\infty}
(-\;\frac{n_0 X^TX}{n})^i
X^Ty
\\
&=&
-\sum_{i=0}^{\infty}
\left(-\;\frac{n_0 X^TX}{n}\right)^{i+1}
(X^TX)^{-1}X^Ty
\;\;\text{(valid for $\lam=\frac{1}{n_0}>>1$)}
\eeqa
Truncating this series is itself
a kind of regularization.


\section{Proximal functions}

For $v \in \RR^n$, $w\in \WW \subset \RR^n$
and $\alp>0$, we define the
{\bf proximal function} 
$w^{prox}:\RR^n \rarrow \WW$ by

\beq
\boxed{
w^{prox}(v; \alp\calc, \WW) = \argmin_{w\in \WW}
\underbrace{
\left(
\calc(w) +
\underbrace{ \frac{1}{2\alp}\norm{w-v}^2}_{\calr(w,v)}
\right)}_{\calc^+(w, v)}}
\eeq
$w^{prox}(v)$ can be viewed as a 
projection of $v\in\RR^n$
onto $w$ in the subspace $\WW\subset \RR^n$.
Henceforth we assume $\WW=\RR^n$,
so the projected vector $v$ and
its projection $w$ are in the same
vector space $\RR^n$.

See Fig.\ref{fig-proximal-example}
for a numerical example of a 
proximal function $w^{prox}:\RR\rarrow \RR$.


\begin{figure}[h!]
\centering
\includegraphics[width=3in]
{regularization/proximal-example.png}
\caption{Example of a proximal
function. x-axis is $w$ and $v=5$. Green curve: $\calc(w)=|w|$. Red curve: $\calr(w, 5)=\frac{1}{2}(w-5)^2$, Blue curve: $\calc^+(w, 5)=\calc(w)+\calr(w,5)$. Note that we are adding 2 convex functions so the minimum of the sum is somewhere in between the minima of the two summands.
$w^{prox}(5) =\argmin_w
\calc^+(w, 5)=4$.
}
\label{fig-proximal-example}
\end{figure}

Next, we will discuss an
analytical rather a numerical
example of proximal functions.
We  begin by defining the
shrinking function\footnote{We call it a
{\bf shrinking function} because
it shrinks a neighborhood-of-zero to zero.
Another common name for this function is a
 {\bf soft-threshold function}, because
 it makes the transition from negative to positive y axis values occur over the interval of $x\in [-\alp, \alp]$ instead of $x\in [0,0]$.}
 $sh_0: \RR\rarrow \RR$ and its inverse
$sh_0^{-1}:\RR\rarrow\RR$  for any $\alp>0$:

\beq
sh_0(v; \alp) = (v-\alp)\indi(v>\alp) + (v+\alp)\indi(v<-\alp)
\eeq

\beq
sh^{-1}_0(w; \alp) = (w+\alp)\indi(w>0) + (w-\alp)\indi(w<0)
\eeq
Fig.\ref{fig-sh_0_w}
shows a plot of $sh_0$ and
$sh^{-1}_0$.


\begin{figure}[h!]
\centering
\includegraphics[width=3.7in]
{regularization/sh_0_w.png}
\caption{Plot
of the functions $sh_0(w; \alpha)$ and $sh_0^{-1}(w; \alpha)$.
}
\label{fig-sh_0_w}
\end{figure}

\begin{claim}
For $\WW=\RR$ and $w, v\in\RR$, if 
\beq
\calc(w)= |w|
\;,
\eeq
then

\beq
w^{prox}(v) = sh_0(v; \alp)
\eeq
and


\beq
\calc^+(w^{prox}(v), v)
=
|sh_0(v; \alp)| + \frac{\alp}{2}
\eeq

\end{claim}
\proof
\beq
0=\frac{d\calc^+}{dw}
=
\alp[\indi(w>0)-\indi(w<0)] + (w-v)
\eeq
So

\beqa
v &=& w+ \alp [\indi(w>0)-\indi(w<0)]
\\
&=& (w+\alp)\indi(w>0) + (w-\alp) \indi(w<0)
\\
&=& sh^{-1}_0(w; \alp)
\eeqa
Hence

\beq
w^{prox}(v;\alp) = sh_0(v; \alp)
\eeq

Note that

\beq
|w-v| = |\alp|
\eeq
so
\beqa
\calc^+(w_{prox}(v), v)&=&|w^{prox}(v;\alp)|
+ \frac{\alp}{2}
\\
&=&
|sh_0(v; \alp)| + \frac{\alp}{2}
\eeqa
\qed




\section{Proximal RC}

Proximal functions can be 
used to do RC as follows.
For $w,v\in\RR^n$
and $\alp>0$,  let

\beq
\calr(w, v) = \frac{1}{2\alp}\norm{w-v}^2_2
\eeq
and

\beq
\calc^+(w) = \calc(w) + \calr(w, w^*)
\;,
\eeq
where $w^*$ is the $w$-minimum 
of both $\calc(w)$ and $\calr(w, w^*)$:


\beq
w^* = \argmin_w \calc(w)
=\argmin_w \calr(w, w^*)
\eeq
Hence,

\beq
w^* = \argmin_w \left[ \calc(w) +
\calr(w, w^*)\right]
\;.
\eeq
Now assume the sequence of
points $\{w_k\}_{k=0}^\infty$
satisfies
$w_k \rarrow w^*$ as
 $k
\rarrow \infty$. Then

\beq
w_{k+1}=
\underbrace{\argmin_w
\left(\calc(w) + \frac{1}{2\alp}
\norm{w-w_k}^2_2\right)
}_{w^{prox}(w_k)}
\eeq
If we differentiate 
the argument of argmin()
to find its minimum, we find

\beq
0= \alp
\underbrace{\nabla_w\calc(w_{k+1})}_{\approx \nabla_w\calc(w_{k})}+ w_{k+1}-w_k
\eeq

Thus, the following 3 recursion relations\footnote{Some people use a sequence $\alp_k\in \RR_+$ instead of the constant $\alp>0$. This is called an
{\bf adaptive step size}
and can yield faster
convergence rates.}
can be used to calculate $w_k$:

\begin{subequations}

\beq
\boxed{
w_{k+1}  = w_{k} -\alp\nabla_w\calc(w_{k})}
\xymatrix{
&&\ar[d]^{-\alp\nabla_w\calc(w_k)}\\
\ar[rr]_{w_{k+1}}\ar[rru]^{w_k}&&
}
\label{eq-non-prox-steep-d}
\eeq


\beq
\boxed{
w_{k+1}=
w^{prox}(w_k)}
\eeq 


\beq
\boxed{
w_{k+1}=
w^{prox}(w_k-\alp \nabla_w\calc(w_k))}
\label{eq-prox-steep-d}
\eeq
\end{subequations}

Eq.(\ref{eq-non-prox-steep-d}) is the familiar 
recursion relation for 
gradient
descent (See Chapter \ref{ch-gradient-descent}).
Eq.(\ref{eq-prox-steep-d})
combines gradient descent
and a proximal projection,
so it is expected to 
converge faster than
simple gradient descent.

\section{Unobserved Nodes of a bnet}

Nodes of a bnet for which the CPT is unknown are called unobseved nodes.
In this book, \hiddenNodes


{\bf Unobserved nodes} (UN) represent what are called
 {\bf latent random variables} in Statistics.
 
 UN make an appearance in many
 places throughout this book.
 For example, they are essential 
 to the methods of
 Kalman Filtering (see Chapter \ref{ch-kalman}) and
 Hidden Markov Model (see Chapter \ref{ch-hmm}).
 
This being a book about bnets and a chapter about RC,
we would like to stress that UN 
can be viewed as a very natural and powerful way of
doing (implicit) RC when using bnets.

