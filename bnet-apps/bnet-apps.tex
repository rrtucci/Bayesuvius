\chapter{Visually Intuitive B Net apps}
\label{ch-bnet-apps}

The figures in this chapter were generated using
the free, open source app ``PyAgrum" (see Ref.\cite{pyagrum})


Whenever I introduce the subject of Bayesian Networks
to someone who has never seen them before, I recommend that the first thing they do is to
download a free bnet app from the internet such as PyAgrum\footnote{Alternative choices for bnet apps are Netica by www.norsys.com, or Hugin by www.hugin.com and several others.}, and play with it. It's a sure way of getting hooked
for life. That's how I got hooked, by an early bnet application called Ergo. As you can see from the pictures in this chapter,
the visual output generated by such applications is very ``explainable", intuitive and appealing.
  
In the late 1980's and early 1990's, partially fueled by the invention of
the junction tree algorithm (see Chapter \ref{ch-junc-tree}), there was a lot of startup activity around Bnets. Bill Gates was a fan of them at that time, and he dedicated a lot of Microsoft manpower to do R\& D of bnets. The first version of Clippy and the first XBox recommender were in fact Bayesian Networks.

Here are a few of these addictive visuals generated by PyAgrum\footnote{``agrum" means citric fruit in French. }
for the ``Wet Grass bnet".

First one enters the input data consisting of the structure of the bnet and a TPM (Transition Probability Matrix)\footnote{A TPM is also called
a CPT, which stands for Conditional Probability Table}
for each node of the bnet.
If asked to display the input data, PyAgrum gives Fig.\ref{fig-wet-grass-bnet}.

\begin{figure}[h!]
\centering
\includegraphics[width=5in]
{bnet-apps/wet-grass-bnet}
\caption{Structure (network, DAG) and a TPM for each node, of the ``Wet Grass" 
problem.}
\label{fig-wet-grass-bnet}
\end{figure}

\begin{figure}[h!]
\centering
\includegraphics[width=4in]
{bnet-apps/wet-grass-evidence}
\caption{Probability histograms for every node of the Wet Grass bnet,
assuming that as evidence, we observe that $\rvc=Cloudy=1$ and $\rvs=Sprinkler=1$, i.e. the sprinkler was left ON and the day was not cloudy.}
\label{fig-wet-grass-evidence}
\end{figure}

PyAgrum can also  

\begin{itemize}
\item calculate joint distributions and conditional joint distributions for multiple nodes of a bnet.
For example, it can calculate  $P(a,b|c,d)$ if $\rva, \rvb, \rvc, \rvd$ 
are 4  distinct nodes of the bnet.

\item find
the most likely value for each node. This is called the {\bf most probable explanation}.

\item calculate the Markov Blanket for any node (See Chapter \ref{ch-mblanket})

\item draw Influence diagrams and do associated calculations (see Chapter \ref{ch-inf-dia})

\item calculate Conditional Probabilities and adjustment formulae  that involve multiple do operators in the condition part. (see Chapter \ref{ch-do-calc})
\item do causal DAG discovery (what used to be called structure learning) (see Chapter \ref{ch-struc-learn})

\end{itemize}
and much more !

