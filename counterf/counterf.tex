\chapter{Counterfactual Reasoning}
\label{ch-counterf}


\section{The 3 Rungs of Causal AI}
According to
Judea Pearl,
there are 3 rungs in the
ladder of causal AI.
These are (as I see them):
\begin{enumerate}
\item
{\bf Observing Passively. Answering \enquote{What next?}:} Collecting
data
and fitting curves to it,
without any plan
designed to
investigate Nature's
causal connections. Predicting the future.
\item {\bf Doing causal
experiments. Answering \enquote{Why?}:}
Doing experiments
consciously designed to
elucidate
Nature's causal connections.
Even cats do this!, but current AI doesn't.
\item {\bf Imagining
 counterfactual situations, Analogizing. Answering \enquote{What if?}:}
Imagining gedanken experiments
to further understand
Nature's causal connections,
and to decide what future
courses of action are
more likely to succeed,
even if
those courses of action
are unprecedented, and have never been taken before.
Making
predictions about
 events that have never happened (\enquote{counterfactuals})
is a very Bayesian
concern, well out of the purview of
frequentists. Nevertheless,
humans do such
\enquote{analogizing}
all the time to great advantage.
It becomes
possible if there
is some foreign but similar
data that can be transported
(transplanted, applied)
to the situation of
interest.


\end{enumerate}

We will use the
term {\bf intervention operator (or simply \enquote{intervention})}
to refer to an operator
that maps a bnet to another bnet.
In Chapter \ref{ch-do-calc},
we introduced an intervention operator
 called the {\bf do operator}
$\cald_{\rvx=x}$ (this is our notation for what Pearl
symbolizes by $do(\rvx)=x$).
The study of counterfactuals
requires that we
introduce a new
kind of intervention
operator that we will
call an {\bf imagine operator},
and denote by
$\cali_{\rvx\rarrow\rvy}$.
These 2 types of intervention
operators
will be defined
in subsequent sections of this chapter.
Usage of the do operator 
characterizes rung 2, and usage of 
the imagine operator characterizes
rung 3.

Chapter \ref{ch-mpass}
on message passing
is about rung 1.
Chapter \ref{ch-do-calc}
on Do Calculus is about rung 2.
This chapter is dedicated to rung 3.

Judea Pearl
is fond of discussing rung 3 solely
in terms of SCM.\footnote{SCM are
what we call DEN. DEN (deterministic systems
with external noise) are discussed in
Chapter \ref{ch-linear-sys}. }
In this chapter,
we define rung 3
without using SCM, using solely
bnets.
This gives a more general
version of rung 3,
because SCM are a subset of bnets.


\section{Do operator}


\begin{figure}[h!]
\centering
\includegraphics[width=1.75in]
{counterf/rho-op.png}
\caption{Action
of \enquote{do} operator $\cald_{\rvx=5}$
on node $\rvx$.}
\label{fig-rho-op}
\end{figure}

The do operator $\cald_{\rvx=5}$
is defined graphically in Fig.\ref{fig-rho-op}.
The TPM, printed in blue,
 for node $\TIL{\rvx}$ of Fig.\ref{fig-rho-op},
is as follows.\footnote{The \enquote{$;5$}
in the distribution
indicates that $5$ 
is a frequentist parameter of the distribution.}

\beq\color{blue}
P(\TIL{x}; 5)=\delta(5, \TIL{x})
\eeq


The do operator $\cald_{\rvx=5}$
amputates
the incoming arrows of node $\rvx$
and sets the TPM
of the new root node $\TIL{\rvx}$
to a delta function $\delta(
\TIL{x}, 5)$
(or some state of $\rvx$
 other than 5).
Sometimes we call the new node
$\cald\rvx$
instead of
$\TIL{\rvx}$.

The uses of the do operator are discussed
in detail in Chapter \ref{ch-do-calc}.

\section{Imagine operator}

\begin{figure}[h!]
\centering
\includegraphics[width=4in]
{counterf/kappa.png}
\caption{Action of \enquote{imagine} operators
$\cali_{\rvx\rarrow \rvy}(5)$
and $\cali_{\rvx\rarrow \rvy}$
on arrow $\rvx\rarrow \rvy$.
In this figure, $y^{nx}=[y(x)]_{\forall x\in S_\rvx}$,
where $nx=|S_\rvx|$
and $S_\rvx$ is the set of states of node $\rvx$.
}
\label{fig-kappa}
\end{figure}

The imagine operator  $\cali_{\rvx\rarrow \rvy}(5)$
is defined graphically in Fig.\ref{fig-kappa}.
Note that Fig.\ref{fig-kappa}
actually defines two types
of imagine operators, the one
with an argument:
$\cali_{\rvx\rarrow \rvy}(5)$,
and the one without an argument:
$\cali_{\rvx\rarrow \rvy}$.
The TPMs, printed in blue,
for various nodes in
Fig.\ref{fig-kappa}, are as follows.



\begin{itemize}

\item
For $\cali_{\rvx\rarrow
\rvy}(\TIL{x})G$

\beq\color{blue}
P(y| \TIL{x}, a.)=
P(y| \rvx=\TIL{x}, a.)
\eeq

\beq\color{blue}
P(\TIL{x}; 5)=
\delta(\TIL{x}, 5)
\eeq

\item
For $\cali_{\rvx\rarrow \rvy}G$
\beq\color{blue}
P(y^{nx}| a.)=\prod_{\TIL{x}}
P(\rvy(\TIL{x})=y(\TIL{x})|a.)
\eeq

\beq\color{blue}
P(y|y^{nx}, x)=
\delta(y,y(x))
\eeq
\end{itemize}
The imagine operators
$\cali_{\rvx\rarrow\rvy}(5)$
and $\cali_{\rvx\rarrow\rvy}$
operate on an arrow
whereas the
$\cald$ operator
 operates on a node.
$\cali_{\rvx\rarrow\rvy}(5)$
deletes
arrow $\rvx\rarrow\rvy$
and
creates a new root node
$\TIL{\rvx}$
and a new arrow
$\TIL{\rvx}\rarrow \rvy$.
Sometimes we call
the new node
$\cali_\rvy\rvx$
instead of
 $\TIL{\rvx}$.
$\cali_{\rvx\rarrow\rvy}$
creates
a new node $\rvy^{nx}$
and an arrow $\rvy^{nx}\rarrow \rvy$.



\begin{figure}[h!]
$$
\begin{array}{ccccc}
\xymatrix{
&\rvx\ar@[red][ddr]\ar[ddl]
\\
&&&
\\
\rvd\ar@[red][rr]
&
&\rvy\ar@[green][r]
\ar@[green][ur]
&
}
&
\xymatrix{
&&\rvx\ar[ddll]\ar@[red][d]
\\
&&[\rvy(0),\rvy(1)]\ar[d]
&
\\
\rvd\ar@[red][rr]
&
&\rvy\ar@[green][r]
\ar@[green][ur]
&
}
\\
\\
G&G_+=\cali_{\rvd\rarrow\rvy}G
\end{array}
$$
\caption{How
 imagine operator
arises in
Potential Outcomes (PO)
theory.
}
\label{fig-counterf-G-im-y0-y1}
\end{figure}
Fig.\ref{fig-counterf-G-im-y0-y1}
shows how the
imagine operator arises
in Potential Outcomes (PO) theory.
PO theory is discussed extensively
in Chapter \ref{ch-pot-out}.
As you can see, PO theory
only uses a limited version
of the 3 rungs
of causal inference, because it
doesn't use the do-operator,
and it only uses one
of 2 possible types of
imagine operators.
Furthermore,
it assumes a
very limited triangular DAG.


\begin{figure}[h!]
\centering
\includegraphics[width=3.5in]
{counterf/rho-kappa.png}
\caption{$\cald_{\rvx=5}\cali_{\rvx\rarrow \rvy}G$
gives a connection
between do and imagine operators.
}
\label{fig-rho-kappa}
\end{figure}

Fig.\ref{fig-rho-kappa}
gives  a connection
between do and imagine
operators.
We see
from that figure that
for $\cald_{\rvx=\TIL{x}}
\cali_{\rvx\rarrow\rvy}G$, we have\footnote{In the
notation favored by Pearl, Eq.(\ref{eq-connect-do-imag})
 would be
$$P(y|do(X)=\TIL{x}, a.)=P(Y_{\TIL{x}}=y|a.)$$}
\beq
P(y|\cald\rvx=\TIL{x}, a.)=P(\rvy(\TIL{x})=y|a.)
\label{eq-connect-do-imag}
\eeq


One can define
a {\bf do-imagine-calculus}
whose
objective
is to
express
probabilities such as
$P(\rvy|\cald\rvr=r,
\cali_\rvb \rvs=s, t)$
in terms of observable
probabilities
that do not
contain
any do or imagine
operators in them.
As with
Do Calculus,
this reduction
is not
always possible,
and we say a probability is
{\bf $\cald$-identifiable},
{\bf $\cali$-identifiable}
or
{\bf $\cald\cali$-identifiable}
if it  can be
expressed without do, imagine
or both operators.

In causal inference,
we often
consider  
\enquote{counterfactual}
random variables $\rvy(0)\in \bool$
and $\rvy(1)\in\bool$. They are called
counterfactual variables because
one of the 2 variables 
refers to an event that has occurred,
whereas the other variable
refers to a \enquote{counterfactual  event},
i.e., an event that has never 
occurred. For some patients,
$\rvy(0)$ has a value
but $\rvy(1)$
doesn't.
For other 
patients,
the opposite 
is the case.
There is some
disagreement in the
community
as to which algorithms
perform rung 3 operations,
and which don't.
This is the convention
used in this book.
We will
say rung 3 operations
are being performed 
if
the counterfactual variables $\rvy(0)$
and $\rvy(1)$
are being used,
or, equivalently,
if a bnet
is being used that
includes nodes $\rvy(0)$, and $\rvy(1)$
that were produced by an imagine operator.
Potential Outcomes (PO)
theory (see Chapter \ref{ch-pot-out})
qualifies as rung 3 according
to this convention.
However, note that PO theory 
only does the bare minimum 
to reach rung 3.
In PO theory, one usually
evaluates $ATE=E[\rvy(1)]-E[\rvy(0)]$,
which entails
calculating $P(y(0))$
and $P(y(1))$.
Pearl has extended the reach of rung 3
much further 
by calculating expected
values that require knowledge of
the joint distribution $P(y(0), y(1))$.

