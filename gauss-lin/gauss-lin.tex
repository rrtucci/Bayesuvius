\chapter{Gaussian Nodes with
 Linear Dependence on Parents}
\label{ch-gauss-lin}

Bnet nodes
that 
have a Gaussian TPM
with a linear dependence
on their parent nodes (GLP)
are a very
popular way 
of modeling continuous
nodes of bnets.
A 
convenient
aspect of them
is that their
parents can be discrete
or continuous nodes,
and their
children can be discrete
or continuous nodes too.
Also,
they can be learned 
easily
from the data
because
their
parameters
can
be expressed in terms of
two node
covariances.
For these reasons, 
they are commonly
used when 
doing
structure learning of 
bnets 
with continuous nodes (see Chapter \ref{ch-struc-learn}).

\begin{figure}[h!]
$$\xymatrix{
\rvy
&\rvx_1\ar[l]
\\
&\rvx_2\ar[lu]
\\
&\rvx_3\ar[luu]
}$$
\caption{GLP node
$\rvy$ with 3 parent nodes $\rvx^3
=
(\rvx_1, \rvx_2, \rvx_3)$.}
\label{fig-glp-3}
\end{figure}

Recall our
notation
for a Gaussian distribution:
\beq
\caln(x;\mu, \sigma^2)
=
\frac{1}{\sigma\sqrt{2\pi}}
e^{\frac{-(x-\mu)^2}{2\sigma^2}}
\;,
\eeq
where 
$x, \mu\in \RR$
and $\sigma>0$.

A GLP node $\rvy$ with 
$n$ parents
 $\rvx^n=(\rvx_1, \rvx_2, \ldots, \rvx_n)$
has the following TPM:
\beq\color{blue}
P(y|x^n)=
\caln(y; \beta_0 + 
\beta^{nT}x^n, \sigma^2)
\eeq
where $\rvy, \beta_0, \in\RR$
and $\sigma^2>0$, and where
$\rvx^n, \beta^n\in \RR^n$ 
are **column vectors**.
The $T$ 
in $\beta^{nT}$ stands for transpose.
Any $\rvx_i$
can have
a discrete
set of states
as long as they are real
valued and ordinal (ordered by size).
 Fig.\ref{fig-glp-3}
shows a diagrammatic
representation
of a GLP node with 3 parents.

Note that as $\sigma\rarrow 0$,
a GLP node becomes 
deterministic.
In fact,
it
becomes a neural
net node
with a linear activation function.


An equivalent
way of defining a GLP node $\rvy$
is in terms of a random variable
equation expressing
$\rvy$ as a hyperplane
function of the parents $\rvx^n$
plus a  Gaussian noise variable.
Define a cfit $\HAT{\rvy}$
of a ``true value"
$\rvy$ by
\begin{subequations}
\beq
\HAT{\rvy}=\beta_0 + \beta^{nT}\rvx^n
\eeq
and

\beq
\rvy=\HAT{\rvy}+\ul{\eps}
\eeq
where the residual $\ul{\eps}$
satisfies 

\beq
P(\eps)=\caln(\eps; 0, \sigma^2)
\eeq
and


\beq
\av{\rvx^n, \ul{\eps}}
=0
\;.
\eeq
\end{subequations}

The notation $\av{\rvx, \rvy}$
for the covariance
of random variables
$\rvx$ and $\rvy$
is explained
in Chapter [\nameref{ch0-conventions}].

\begin{claim}
The
parameters of
a GLP node
can be expressed
in terms of 2-node
covariances.
Specifically,

\beq
\beta^n=
\av{\rvx^n, \rvx^{nT}}^{-1}
\av{\rvy, \rvx^n}
\eeq

\beq
\beta_0=
\av{\rvy}-
\beta^{nT}\av{\rvx^n}
\eeq

\beq
\sigma^2
=
\av{\rvy, \rvy}
-\beta^{nT}
\av{\rvx^n, \rvy}
\eeq
\end{claim}
\proof

Note that $\av{\rvx^n, \rvx^{nT}}^T
=\av{\rvx^n, \rvx^{nT}}$
and 
$\av{\rvy, \rvx^{nT}}^T
=\av{\rvy, \rvx^n}$.


\beq
\av{\rvy, \rvx^{nT}}
=
\beta^{nT}\av{\rvx^n, \rvx^{nT}}
\eeq

\beq
\av{\rvy, \rvx^n}=
\av{\rvx^n, \rvx^{nT}}\beta^n
\eeq

\beq
\beta^n
=
\av{\rvx^n, \rvx^{nT}}^{-1}
\av{\rvy, \rvx^n}
\eeq

\beq
\av{\rvy}=
\beta_0 + 
\beta^{nT}\av{\rvx^n}
\eeq

\beqa
\av{\rvy, \rvy}
&=&
\av{
\beta_0 + \beta^{nT}\rvx^n +
\ul{\eps},
\rvy}
\\
&=&
\beta^{nT}\av{\rvx^n,
\rvy}
+
\sigma^2
\eeqa
\qed

\hrule
Let  D=Discrete, GLP=Gaussian with  Linear
 dependence in Parents

The following arrows are possible
in a bnet.

\begin{itemize}
\item $GLP\larrow GLP$
\item $GLP\larrow D$

Pass to GLP a separate
set of regression
coefficients $\beta_0, \beta^n$
and variance $\sigma^2$
for each state 
of D. If D is called $\rvd$,
let
\beq\color{blue}
P(y|(x^n)_d, d)=
\caln(y; (\beta_0)_d + 
(\beta^{nT})_d (x^n)_d, \sigma_d^2)
\eeq
for each $d\in S_{\rvd}$.

\item $D\larrow GLP$

If D expects
a continuous parent,
no need to preprocess GLP output.
If D expects a discrete parent,
break
the interval $[a,b]$
that
contains
most of
the range
of the GLP node into
sub-intervals and 
assign a discrete label
to each subinterval.
\item $D\larrow D$
\end{itemize}