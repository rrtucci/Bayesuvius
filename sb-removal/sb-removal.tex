\chapter{Selection Bias Removal}
\label{ch-sb-removal}
This chapter
is based on Ref.\cite{bare-sb-removal}.

Selection bias (SB)
occurs when one
samples from an
atypical subset
of a
population,
producing a biased dataset.
Are such biased
datasets
useless? Not necessarily.
It is possible to
add auxiliary features
to the biased dataset, and to
sample those new features
in an unbiased way,
 from the whole population.
Then
one can merge
the original
 biased dataset with the
auxiliary, unbiased one,
to obtain an enhanced dataset.
It is sometimes
possible to do this so that the enhanced
dataset is provably
unbiased.
It's like making horizontal
the surface of a table
 that was
 not initially
horizontal.
The theory of Bayesian Networks and Causal
Inference tells us
WHEN this is possible,
and HOW to do it
when it is possible.


\section{Root and Leaf Nodes}

Root and Leaf (R-L) nodes play an important role in many facets of
bnet theory. In this section, we will review what we know
about them from previous chapters in this book, and then we will add a few new
ideas like R-L conversion and R-L selector nodes.


\begin{figure}[h!]
\centering
\includegraphics[width=1in]
{sb-removal/common-cause-effect.png}
\caption{Common Cause
and Common Effect for
treatment node $\rvd$
and outcome node $\rvy$.}
\label{fig-common-cause-effect}
\end{figure}

In Potential
Outcomes (PO) theory
 (see Chapter \ref{ch-pot-out}),
root nodes such
as $\rva$ in
Fig.\ref{fig-common-cause-effect}
are called {\bf common cause
 (a.k.a. confounder, fork) nodes}
for the treatment node $\rvd$
and outcome node $\rvy$.
In PO theory, leaf nodes such as
$\rvb$ in
Fig.\ref{fig-common-cause-effect} are
called
{\bf common effect
(a.k.a. selection bias (SB), collider) nodes
for nodes $\rvd, \rvy$}.
Hence, in PO theory,
SB is indicated
by
a leaf node,
just as we do in this chapter.

Note that
{\bf Simpson's paradox} (see Chapter
\ref{ch-simpson}) arises from an indirect effect
caused by \ul{not conditioning}
on a confounder,
whereas
{\bf Berkson's paradox}
(see Chapter \ref{ch-berkson})
arises from an indirect effect
caused by \ul{conditioning}
on a collider.

\subsection{R-L Conversion}

A fully connected subgraph of 
a graph $G$ is called a 
{\bf clique} of $G$.
I like to call a subgraph $X$  of $G$ that only has exiting arrows, an 
{\bf exploding subgraph}.
This section deals with
 {\bf exploding cliques}.


It's possible to replace a root node
by a leaf node, or vice versa, as
follows.
Suppose that we start with
a bnet $G_0$ that is \ul{fully connected}, and
we add to it a node $\rvbeta$
that is a root node that points
to all nodes of $G_0$.
Call the resulting bnet $\rvbeta\rarrow G_0$.
We can use Bayes rule to reverse the direction
of the arrows emanating from $\rvbeta$
so that they enter node $\rvbeta$
rather than exit
it.
Call the resulting bnet $\rvbeta\larrow G_0$.
In general,
Bayes rule allows us to translate
from $\rvbeta\rarrow G_0$ to
$\rvbeta\larrow G_0$,
or in the opposite direction,
without having to change the
directions of any of the arrows in $G_0$.
If $G_0$ is not fully connected, then
going from root-$\rvbeta$,
$\rvbeta\rarrow G_0$, to leaf-$\rvbeta$,
$\rvbeta\larrow G_0^{source}$,
will often require that $G_0^{source}$
have the same arrows in the same
directions as $G_0$
plus some extra arrows
new to $G_0$.
Likewise, going
from leaf-$\rvbeta$, $\rvbeta\larrow G_0$, to root-$\rvbeta$,
$\rvbeta\rarrow G_0^{target}$,
may require that $G_0^{target}$ have
the same arrows as $G_0$ plus some new arrows.

So far, we have
been intentionally
vague in specifying the graphs
$G_0^{source}$ and $G_0^{target}$.
In Fig.\ref{fig-sel-nd-reversal}
we give a trick for determining
possible candidates for
graphs $G_0^{source}$ and $G_0^{target}$.
In
Fig.\ref{fig-sel-nd-reversal},
we consider 3 panels going from left
to right, depicting
the cases where $\rvbeta$ has either 1,2 or 3 neighbors.
The top graph
 $G_{leaf}$, which has
a leaf $\rvbeta$, is converted
to a graph which is numerically
equal to it, namely
the bottom
graph
 $G_{root}$, which has
a root $\rvbeta$.
The magenta arrows represent
any number of arrows
exiting (but none entering)
a node of $G_0$.

If we start with a graph $G$ that contains a leaf node $\beta$, 
we find the biggest exploding subgraph $X$ of
$G_0 =G  -\rvbeta$.
Then we add enough
arrows to $X\cup \rvbeta$
to make it clique of $G$.
Now we can reverse the incoming
arrows to $\rvbeta$ and make them
all outgoing and call the
resulting graph $\rvbeta\rarrow G_0^{target}$.




\begin{figure}[h!]
\centering
\includegraphics[width=4in]
{sb-removal/sel-nd-reversal.png}
\caption{Converting 
a leaf node
to a root node.}
\label{fig-sel-nd-reversal}
\end{figure}

Recall that in Chapter \ref{ch-bnet-def},
we made a distinction
between a good Causal Fit (CF) bnet
a bad CF bnet, and
we pointed out
that bad CF bnets are
often useful as
numerical tools.
Recall also from Chapter
\ref{ch-obs-equi}
that two bnets can be
\qt{observationally equivalent}.
That is what is happening here.
We are faced with
the dilemma of
using
either
leaf nodes or root nodes.
Both
choices  lead to observationally
equivalent bnets.
One of the two choices
leads to a good
CF bnet,
and the other to a bad CF bnet.
Both choices are numerically
correct.

\subsection{R-L selector nodes}
{\bf R-L selector nodes} are \qt{switch} nodes
$\rvs\in \bool$ that allow us to merge 2 bnets into a single one. In this chapter they allow us 
to consider two bnets, with and without SB, at the same time.
In the case of causal-transfer (see Chapter \ref{ch-transfer-causal}), they
allow us
 to transfer causal knowledge
between experiments.


\begin{figure}[h!]
$$
\begin{array}{ccc}
\xymatrix{
&\rvbeta\ar[dl]\ar[d]\ar[dr]&\rvs\ar[l]
\\
&&
}
&\quad\quad&
\xymatrix{
\ar[dr]&\ar[d]&\ar[dl]
\\
&\rvbeta&\rvs\ar[l]
}
\\
(a)\text{ root selector node} 
&& (b)\text{ leaf selector node}
\end{array}
$$
\caption{$(a)$ (resp., $(b)$) shows a selector node $\rvs$ for a
root node (resp., leaf node) $\rvbeta$.}
\label{fig-rl-selectors}
\end{figure}

Fig.\ref{fig-rl-selectors}
$(a)$ shows a selector node $\rvs$ for a
root node  $\rvbeta$.  In this case,
the TPM, printed in blue, of node $\rvbeta$ is                        


\beq\color{blue}
P(\rvbeta=\beta|s)=
\left\{
\begin{array}{ll}
P(\beta)&\text{if $s=0$}
\\
P^*(\beta)&\text{if $s=1$}
\end{array}
\right.
\eeq

Fig.\ref{fig-rl-selectors}
$(b)$ shows a selector node $\rvs$ for a
leaf node  $\rvbeta$.  In this case,
the TPM, printed in blue, of node $\rvbeta$ is   

\beq\color{blue}
P(\rvbeta=\beta|s, pa^-(\beta))=
\left\{
\begin{array}{ll}
P(\beta)&\text{if $s=0$}
\\
P^*(\beta|pa^-(\beta))&\text{if $s=1$}
\end{array}
\right.
\eeq
where $pa(\rvbeta)$ are all the parents of $\rvbeta$ 
including $\rvs$, and $pa^-(\rvbeta)= pa(\rvbeta)-\rvs$.

\section{SB-Recoverability}

Consider the bnet
Fig.\ref{fig-bs-removal-basic}.

\begin{figure}[h!]
$$
\xymatrix{
\rvx\ar[r]\ar@{<->}[d]
&\rvy\ar@{<->}[dl]
\\
\Rect{\rva.}\ar[r]&\rvbeta&\rvs\ar[l]
}
$$
\caption{Bnet considered for
selection bias (SB) removal.
Double arrows stand for any number of arrows in
either direction.}
\label{fig-bs-removal-basic}
\end{figure}

$\rvs\in \{0,1\}$ {\bf selector node}

$\rvbeta=$ {\bf bias node}

$\rvS=(\rvbeta, \rvs)$

$\rvx=$ {\bf treatment node}.

$\rvy=$ {\bf outcome node}.

$\rva.=$ {\bf auxiliary features}.
This is a set of nodes that
may contain arrows entering
or exiting it, as indicated by the double arrows.

$\rvE=\{\rvy, \rvx\}\cup\rva.=$
{\bf Enhanced feature set}.


$\Sigma=$ population of individuals $\s$


$BD=\{({\s},\rvx^{\s},  \rvy^{\s},\rvbeta^{\s}):{\s}\in\Sigma\}=$
{\bf Biased Dataset}, dataset for $(\rvx,\rvy, \rvbeta)$ features
with $\rvs=1$.
Gives empirical
distribution $\color{red}{P(y|x,\beta, \rvs=1)}$.

$AD=\{(\s, \rvx^\s,\rva.^\s):\s\in\Sigma\}=$
{\bf Auxiliary Dataset}, dataset for $(\rvx, \rva.)$ features.
Gives empirical
distribution $\color{red}{P(a.|x)}$.


$ED=\{({\s},\rvx^{\s},  \rvy^{\s},\rva.^{\s}):{\s}\in\Sigma\}=$
{\bf Enhanced Dataset}, dataset for $(\rvx,\rvy, \rva.)$ features.
Gives empirical
distribution $\color{red}{P(y|x, a.)}$.




\subsection{Removing SB from
passive query}

We will refer to $Q=P(y|x, \beta, \rvs=1)$ 
as a {\bf  passive query}.
A {\bf passive query is
(SB-) recoverable}
via some (possibly empty) multinode $\rva.$ if $\rvy\perp\rvS|(\rvx, \rva.)$; i.e., if

\beq
P(y|x, a. , \beta,\rvs=1)=
P(y|x, a.)
\;.
\eeq

\begin{claim}\label{cl-sb-recov}
If 
 $\rvy\perp\rvS|(\rvx,\rva.)$, then
\beq
P(y|x)
=
\sum_{a.}
\underbrace{P(y|x, a.)}_
{P(y|x,a., \beta, \rvs=1)}
P(a.|x)
\eeq

\beq
\xymatrix{
\\
x\ar[r]
&y
}
\xymatrix{\\=}
\xymatrix{
\beta, \rvs=1\ar[dr]
&\sum a.\ar[d]
\\
x\ar[r]\ar[ru]
&y}
\eeq
\label{cl-sb-bdoor}
\end{claim}
\proof
Obvious.
\qed

\subsection{Removing SB from
active query}

We will refer to $Q=P(y|\cald \rvx=x, \beta, \rvs=1)$ 
as an {\bf  active query}.
An {\bf active query is
(SB-) recoverable}
via some (possibly empty) multinode $\rva.$ if $\rvy\perp\rvS|(\rvx, \rva.)$; i.e., if

\beq
P(y|\cald \rvx = x, a.,\beta, \rvs=1)=
P(y|\cald \rvx = x, a.)
\;.
\eeq
%If also $\rva.\perp \rvS|\rvx$, then
%
%\beq
%P(a.|\cald \rvx=x, \beta, \rvs=1)=P(a.|\cald\rvx=x)
%\eeq
%and we will say that
%the {\bf active query is  super-recoverable} via $\rva.$.\footnote{This definition super-recoverable active queries is my own, not in Ref.\cite{bare-sb-removal}}


\begin{claim} Same as Claim \ref{cl-sb-bdoor}
but conditioning on $\cald\rvx=x$ instead of $\rvx=x$.
\end{claim}
\proof
Same as for Claim \ref{cl-sb-bdoor}.
\qed
                                      
\subsection{Examples}
\hrule
\begin{enumerate}
\item PO Selection Bias (SB). $Q=P(y|x)$ NOT recoverable ($\rvy\not \perp\rvS|\rvx$)
\beq\xymatrix{
\rvx\ar[rr]\ar[dr]
&&\rvy\ar[dl]
\\
&\rvbeta&\rvs\ar[l]
}\eeq

\hrule\item PO Confounding. $Q=P(y|x)$ NOT recoverable ($\rvy\not \perp\rvS|\rvx$)
\beq\xymatrix{
&*++[F-o]{\rvbeta}
\ar[dl]\ar[dr]
&\rvs\ar[l]
\\
\rvx\ar[rr]
&&\rvy
}\eeq


\hrule\item SB of treatment.  $Q=P(y|x)$ recoverable. ($\rvy\perp\rvS|\rvx$)
\beq\xymatrix{
\rvx\ar[rr]\ar[dr]
&&\rvy
\\
&\rvbeta&\rvs\ar[l]
}\eeq

\hrule\item SB of outcome. $Q=P(y|x)$ NOT recoverable ($\rvy\not \perp\rvS|\rvx$)

\beq\xymatrix{
\rvx\ar[rr]
&&\rvy\ar[dl]
\\
&\rvbeta&\rvs\ar[l]
}\eeq


\hrule\item $Q=P(y|x)$ recoverable. ($\rvy\perp\rvS|\rvx$)
\beq\xymatrix{
\rvx\ar[rr]\ar[dr]
&&\rvy
\\
\rvz\ar[u]\ar[r]
&\rvbeta&\rvs\ar[l]
}\eeq

\hrule\item $Q=P(y|x)$ NOT recoverable.
($\rvy\not \perp\rvS|\rvx$)


\beq\xymatrix{
&\rvw\ar[dl]\ar[dr]
\\
\rvx\ar[rr]\ar[dr]
&&\rvy
\\
\rvz\ar[u]\ar[r]
&\rvbeta&\rvs\ar[l]
}\eeq

\hrule\item $Q=P(y|\cald x=x)$ is recoverable via $\rva.=\rvw$. ($\rvy\perp\rvS|\rvx, \rvw$)
\beq\xymatrix{
\rvx\ar[dr]\ar[rr]
&&\rvy\ar[dl]
\\
&\rvw\ar[d]
\\
&\rvbeta&\rvs\ar[l]
}\eeq

\hrule\item $Q=P(y|\cald x=x)$ is recoverable
via $\rva.=\rvw_2$. ($\rvy\perp\rvS|\rvx, \rvw_2$)
\beq\xymatrix{
\rvw_1\ar[rr]\ar[d]
&&\rvw_2\ar[d]\ar[ddl]
\\
\rvx\ar[rr]
&&\rvy
\\
&\rvbeta&\rvs\ar[l]
}\eeq

\hrule\item $Q=P(y|\cald x=x)$ is recoverable.
($\rvy\perp\rvS|\rvx$)
\beq\xymatrix{
&&*++[F-o]{\rvc}
\ar[dl]\ar[dr]
\\
\rvw_1\ar[r]\ar[d]
&\rvw_2\ar[r]
&\rvx\ar[r]
&\rvy
\\
\rvbeta&\rvs\ar[l]
}\eeq

%\hrule\item $Q=P(y|x)$ super recoverable
%via $\rva.=\rva$ ($\rvy\perp\rvS|(\rvx, \rva)$
%and $\rva\perp\rvS|\rvx$)
%\beq\xymatrix{
%\rva\ar[r]
%&\rvx\ar[rr]\ar[dr]
%&&\rvy
%\\
%&&\rvbeta&\rvs\ar[l]
%}\eeq
\end{enumerate}