\chapter{Do Calculus}\label{ch-do-calc}


The Do Calculus (DC) and associated ideas were
invented by
Judea Pearl and collaborators.
This chapter is
based on Judea Pearl's
books (see Chapter \ref{ch-nav-pearl}).

The subject of DC is complicated
so I've decided to break its presentation 
into 4 chapters:
\begin{enumerate}
\item The current chapter, entitled \nameref{ch-do-calc}. This chapter is
 the take-off point for our DC presentation.
It gives an introduction  to Pearl's DC,
without any proofs.
\item Chapter \ref{ch-do-calc-proofs}, entitled: \nameref{ch-do-calc-proofs}, is an aggregation
of proofs of all theorems in this book
that require DC to prove them.

\item Chapter \ref{ch-bdoor}, entitled: \nameref{ch-bdoor}, gives examples of the application
of the backdoor adjustment formula.

\item Chapter \ref{ch-fdoor}, entitled: \nameref{ch-fdoor}, gives examples of the application
of the frontdoor adjustment formula.

\end{enumerate}



Let
$\cald_\rvx$
be an operator called the {\bf do-operator}
that acts on a graph $G$
with a node
$\rvx$
by
deleting
all
the
arrows
entering
$\rvx$,
thus
converting
$\rvx$
into
a new
node $\cald \rvx$
that
is a root node.
Let
$\call_\rvx$
be an operator that acts
on a graph $G$
with a node
$\rvx$
by deleting
all
the
arrows
leaving
$\rvx$,
thus
converting
$\rvx$
into
a new
node $\call \rvx$
that
is a leaf node.
$\cald_\rvx$
and
$\call_\rvx$
are
depicted
in Fig.\ref{fig-do-rho-lam}.
\footnote{Pearl
uses $\cald_X G=
G_{\ol{X}}
$
and
$\call_X G=
G_{\ul{X}}$
for a random variable $X$
in a graph $G$.
The way I remember
Pearl's notation is top-in (as in topping), and
bottom-out (as in butt-out).}




\begin{figure}[h!]
\centering
\includegraphics[width=4in]
{do-calc/do-rho-lam.png}
\caption{
The do operator $\cald_\rvx$
converts node $\rvx$
into a root node $\cald \rvx$.
The leaf operator $\call_\rvx$
converts node $\rvx$
into a leaf node $\call\rvx$.
}
\label{fig-do-rho-lam}
\end{figure}


If
you don't
know yet
what we mean by
a multi-node
$\rva.$, see
Chapter \ref{ch-bnet-def}.

Given a bnet
$G$,
we define
as follows
the operators
$\cald_{\rva.}$
and
$\call_{\rva.}$
for a multi-node
$\rva.$.

\beq
\cald_{\rva.}G =
\left[\prod_j \cald_{\rva_j}\right]G
\;,\;\;\;\;
\call_{\rva.}G =
\left[\prod_j \call_{\rva_j}\right]G
\;.
\eeq

Consider a bnet
whose totality of nodes
is labeled $\rvX.$.
Recall that

\beq
P(X.)=
\prod_j P(X_j|(X_k)
_{k:\rvX_k\in pa(\rvX_j)})
\;.
\eeq

Define an
operator $\cald$
that acts as follows\footnote{As usual,
$\caln(!x)$ denotes
a constant
that is independent of $x$.}: Let
$X.-a.=(X_k)_{k:\rvX_k\notin \rva.}$.

\beqa
P(X.-a.|\cald\rva.=a.)
&=&
\caln(!(X.-a.))
\frac{P(X.)}
{
\prod_{j:\rvX_j\in \rva.}
P(X_j|(X_k)
_{k:\rvX_k\in pa(\rvX_j)})
}
\\
&=&
\caln(!(X.-a.))
\prod_{j:\rvX_j\notin \rva.}
P(X_j|(X_k)
_{k:\rvX_k\in pa(\rvX_j)})
\\
&\neq&
P(X.-a.|\rva.=a.)
\;.
\eeqa
Also,

\beq
P(\cald\rva.=a.)=
\delta(a'., a.)
\;.
\eeq
In words, we replace
the TPM for
multinode
$\rva.$ by
a delta function.

For instance, for the bnet

\beq
\xymatrix{
\rvr\ar[r]
&\rvx\ar[r]&\rvy
}
\eeq
with

\beq
P(r, x,y)=P(y|x)P(x|r)P(r)
\;,
\eeq
one has

\beq
P(r, y|\cald\rvx=x)=P(y|x)P(r)
\eeq
Hence,

\beq
P(y|\cald\rvx=x)=P(y|x)
\eeq


For the bnet

\beq
\xymatrix{
\rvc\ar[d]\ar[rd]
\\
\rvx\ar[r]&\rvy
}
\eeq
with

\beq
P(x,y, c)=P(y|x, c)P(x|c)P(c)
\;,
\eeq
one has

\beq
P(y, c|\cald\rvx=x)=P(y|x, c)P(c)
\;.
\eeq
Hence,

\beq
P(y|\cald\rvx=x)=\sum_c P(y|x,c)P(c)
\;.
\eeq
This is called {\bf adjusting $\rvc$}.
In general, adjust a variable means 
averaging over it.

For
$\rvb.\subset \rvX.-\rva.$,
define

\beq
P(b.|\cald\rva. =a.)=
\sum_{X.-a.-b.}
P(X.-a.|\cald\rva.=a.)
\;,
\eeq
and for
$\rvs.\subset \rvX.-\rva.-\rvb.$,
define

\beq
P(b.|\cald \rva.=a., s.)=
\frac{P(b., s.|\cald\rva.=a.)}
{P(s.|\cald\rva.=a.)}
\;.
\eeq

$P(b.|\cald \rva.=a., s.)$
is denoted by Pearl  by
$P(b.|do(\rva.=a.), s.)$.
I prefer to
use $\cald$
instead of $do()$.
I will still call $\cald$
a {\bf do operator}.



In $P(y|\cald \rvx=x)$,
node $\rvx$ is turned
into a root node. This guarantees
that there is
no confounding node
connecting $\rvx$ and
$\rvy$. Such
confounding nodes
are unwelcomed
when calculating
causal effects
between
the 2 variables $\rvx$ and $\rvy$
 because they
 introduce
non-causal
correlations between
the two.
This is also
what happens
in a {\bf Randomized
Controlled Trial (RCT)}.
In an RCT
 with treatment $\rvx$,
the value
of $\rvx$ for each patient
is determined by a coin toss,
effectively
turning $\rvx$ into a root node.
Hence, the do operator mimics an RCT.



For $\rvx, \rvy\in \bool$,
 the {\bf average causal effect (ACE)}
is defined as

\beq
ACE=
P(y=1|\cald \rvx=1)-
P(y=1|\cald \rvx=0)
\eeq
and the
{\bf Risk Difference (RD)}
is defined as

\beq
RD=
P(y=1|\rvx=1)-
P(y=1|\rvx=0)
\;.
\eeq

\section{Observed and Hidden Nodes}

When
doing
Do Calculus,
it is
convenient
to separate
the nodes
of a bnet
into
2  types:
{\bf observed},
and {\bf hidden (i.e., unobserved, latent,
unmeasured, non-visible)},
depending
on
whether data
describing
the
state
of that
node
is available  (i.e., measured) or not.
In this chapter, every
hidden node will
be indicated
in a bnet
diagram by
either: (1)
enclosing
its random variable
in a dashed circle or
(2) making
the arrows
coming
out of it
dashed.
Accordingly,
the
3 diagrams
in
Fig.\ref{fig-hidden-dashes}
all mean the same thing.

A {\bf confounder node} $\rvc$
for nodes $\rvx$ and $\rvy$
is a root node
with arrows
pointing
from it to
both
$\rvx$ and $\rvy$.
Thus, $\rvc$ acts as a {\bf common cause}
of $\rvx$ and $\rvy$.
In general, confounders
can be either observed
or hidden nodes.
The word \qt{confounder} itself
just means that it confuses
the analysis.
It says nothing about
whether it is hidden or not.
The node
$\rvc$
in Fig.\ref{fig-hidden-dashes})
is a {\bf hidden confounder}.

\begin{figure}[h!]
$$\xymatrix{
&*++[F-o]{\rvc}\ar[dl]\ar[dr]
\\
\rvx\ar[rr]&&\rvy
}
\;\;\;
\xymatrix{
\rvx\ar@{-->}@{<-->}@/^2pc/[rr]
\ar[rr]&&\rvy
}
\;\;\;
\xymatrix{
&\rvc\ar@{-->}[dl]\ar@{-->}[dr]
\\
\rvx\ar[rr]&&\rvy
}$$
\caption{
These 3 diagrams
are equivalent.
They
mean that node $\rvc$
is hidden.
Node $\rvc$
is implicit
in the
middle diagram.}
\label{fig-hidden-dashes}
\end{figure}

A {\bf query} $P(b.|\cald \rva.=a., s.)$
is said to be {\bf do-identifiable}
(i.e., expressible without do())
if it can be
expressed in terms of
probability distributions
that only
depend on observed
variables, and that
have no do operators
in them.\footnote{In Statistics,
one says a probability
distribution $P(x;\theta)$
of $x$ that depends on a parameter
$\theta$ is {\bf identifiable}
if  $P(x;\theta_1)=P(x;\theta_2)$
implies $\theta_1=\theta_2$.}
Note that all queries that refer to a bnet 
with no hidden nodes, are do-identifiable.



\section{3 Rules of Do Calculus}

In this section, we will state, without proof,
the 3 Rules of Do Calculus (DC). The bad news is that these rules 
are very complicated and difficult
to apply. The good news is that you only need them 
when dealing with a do query on
a bnet G that has hidden nodes. The 3 rules are totally 
unnecessary if $G$ has no hidden nodes.
Another good news is that in Chapter \ref{ch-do-calc-proofs}, we give 
a technique that is
an alternative to the 3 rules 
of do DC. This technique yields the same
results as  the 3 rules of DC, without
the tears.

Throughout
this section, suppose
$\rva., \rvb., \rvr.,
\rvs.$ are disjoint
multinodes in a bnet $G$.


All 3 rules of DC start with an
{\bf original bnet} $G$ and a {\bf hypothesis bnet}
$G_{amp}$ constructed by amputating
some arrows from $G$. Then they propose how
to transform a bnet $G_e$ to a bnet $G_{e'}$. I like to call 
$G_e$ and $G_{e'}$ the {\bf evolving graphs}. The first $G_e$ 
results from transforming the  original graph $G$.


Recall
from Chapter \ref{ch-dsep}
on d-separation,
that  $(\rvb.\perp_G \rva.|\rvr., \rvs.)$
means that
we have established
from the d-separation
rules that
all
paths in $G$
 from
$\rva.$ to
$\rvb.$
are blocked
if we condition
on $\rvr.\cup \rvs.$.
Recall also that:

\begin{itemize}
\item {\bf Rule 0: Insertion or
 deletion of condition $\rva.=a.$, in the absence
do-operator conditions.}

\rulezeroif\\
$\rulezeroP$
\\
$$%rule 0
\begin{array}{ccc}
\DLFourNode{}{}{\rva.}{}{}
&
\xymatrix{\\
\rightleftarrows
}
&
\DLFourNode{}{}{\rva.\nosignal{\rvb.}}{}{}
\\
G_e&&G_{e}
\end{array}
$$

\end{itemize}
Zeroing an arrow is the same as deleting it.
$\rva.\nosignal{\rvb.}$ is the promise that
there are no unblocked paths
from node $\rva.$ to node $\rvb.$.


The 3 rules of Do Calculus
can be presented in the same
format as Rule 0.


\begin{itemize}
\item {\bf Rule 1:
Insertion or deletion of
condition $\rva.=a.$, in the presence
of do-operator conditions.}

\ruleoneif\\
$\ruleoneP$
\\
$$%rule 1
\begin{array}{ccc}
\DLFourNode{}{}{\rva.}{}{}
&
\xymatrix{\\
\rightleftarrows
}
&
\DLFourNode{}{}{\rva.\nosignal{\rvb.}}{}{}
\\
G_e&&G_{e'}\end{array}
$$


\item {\bf Rule 2: Exchange of
$\rva.=a.$ and $\cald\rva.=a.$
conditions.}

\ruletwoif \\
$\ruletwoP$
\\
$$% rule 2
\begin{array}{ccc}
\DLFourNode{}{}{\rva.}{}{}
&
\xymatrix{\\
\rightleftarrows
}
&
\DLFourNode{0}{0}{\cald\rva.}{}{}
\\
G_e&& G_{e'}
\end{array}
$$

\item {\bf Rule 3: Insertion and
 deletion of $\cald \rva.=a.$ condition.}


\rulethreeif\\
$\rulethreeP$
\\
$$% rule 3
\begin{array}{ccc}
\DLFourNode{0}{0}{\cald\rva.}{}{}
&
\xymatrix{\\
\rightleftarrows
}
&
\DLFourNode{0}{0}{\cald\rva.\nosignal{\rvb.}}{}{}
\\
G_e&& G_{e}
\end{array}
$$


\end{itemize}

\begin{figure}[h!]
$$
\xymatrix{
&\Rect{``1"}
\ar@/_1pc/[dl]_{\text{Rule 1}}\ar@{<-}[dl]
\ar[dr]\ar@{<-}@/^1pc/[dr]^{\text{Rule 3}}
\\
\Rect{``\rva.=a."}\ar@/^1pc/[rr]
&&\Rect{``\cald\rva.=\rva."}\ar@/^1pc/[ll]^{\text{Rule 2}}
}$$
\caption{Finite State Machine describing the 3 rules 
of Do  Calculus. The arrows are commands to replace the string
at the source of the arrow by the string at the
target of the arrow.}
\label{fig-fsm-do-calc}
\end{figure}
Fig.\ref{fig-fsm-do-calc} shows a finite state machine
for the 3 rules of Do Calculus.\footnote{Finite
State Machines are discussed in Chapter \ref{ch-finite-state}.}


\begin{figure}[h!]
$$
\begin{array}{c}
\begin{array}{cc}
\xymatrix{\\\\
\text{Rule 1: }G_{amp}=\cald_{\rvr.}G:}
& 
\xymatrix@C=.2pc{
\\
&&\bullet\ar@/^2pc/@[red][rrrrrrrrr]&&&\bullet&&&&\bullet\ar@/_1pc/@[red][llll]&&\bullet&&&&
\\
&\rva.\ar@{<-}[ul]\ar@{<-}@[red][ur]\ar[dl]\ar@[red][dr]
&&&\rvb.\ar@{<-}[ul]\ar@{<-}@[red][ur]\ar[dl]\ar[dr]
&&&*++[o][F*:yellow]{\cald\rvr.}\ar@{<-}[ul]|{\color{red}0}\ar@{<-}[ur]|{\color{red}0}\ar[dl]\ar[dr]
&&&*++[o][F*:yellow]{\rvs.}\ar@{<-}@[red][ul]\ar@{<-}@[red][ur]\ar[dl]\ar[dr]
\\
&&\bullet\ar@[red][drrrrrrrrr]&&&&&&&&&&&&&
\\
&&&&&&&&&&&\bullet\ar@[red]@/_2pc/[uuu]&&&&
}
\end{array}
\\
\begin{array}{cc}
\xymatrix
{\\\\
\text{Rule 2: }G_{amp}=\call_{\rva.}\cald_{\rvr.}G:}
& 
\xymatrix@C=.2pc{
\\
&&\bullet\ar@/^2pc/@[red][rrrrrrrrr]&&&\bullet&&&&\bullet\ar@/_1pc/@[red][llll]&&\bullet&&&&
\\
&\rva.\ar@{<-}[ul]\ar@{<-}@[red][ur]\ar|{\color{red}0}[dl]\ar|{\color{red}0}[dr]
&&&\rvb.\ar@{<-}[ul]\ar@{<-}@[red][ur]\ar[dl]\ar[dr]
&&&*++[o][F*:yellow]{\cald\rvr.}\ar@{<-}[ul]|{\color{red}0}\ar@{<-}[ur]|{\color{red}0}\ar[dl]\ar[dr]
&&&*++[o][F*:yellow]{\rvs.}\ar@{<-}@[red][ul]\ar@{<-}@[red][ur]\ar[dl]\ar[dr]
\\
&&&&&&&&&&&&&&&
}
\end{array}
\\
\begin{array}{cc}
\xymatrix{\\\\
\text{Rule 3: }G_{amp}=\cald_{\rva.-an(\rvs.)}\cald_{\rvr.}G:}
&
\xymatrix@C=.2pc{
\\
&&\bullet\ar@/^2pc/@[red][rrrrrrrrr]&&&\bullet&&&&\bullet\ar@/_1pc/@[red][llll]&&\bullet&&&&
\\
&\Rect{\rva.'|\rva.''}\ar@{<-}[ul]|{\color{red}0}\ar@{<-}@[red][ur]\ar[dl]\ar@[red][dr]
&&&\rvb.\ar@{<-}[ul]\ar@{<-}@[red][ur]\ar[dl]\ar[dr]
&&&*++[o][F*:yellow]{\cald\rvr.}\ar@{<-}[ul]|{\color{red}0}\ar@{<-}[ur]|{\color{red}0}\ar[dl]\ar[dr]
&&&*++[o][F*:yellow]{\rvs.}\ar@{<-}@[red][ul]\ar@{<-}@[red][ur]\ar[dl]\ar[dr]
\\
&&\bullet\ar@[red][drrrrrrrrr]&&&&&&&&&&&&&
\\
&&&&&&&&&&&\bullet\ar@[red]@/_2pc/[uuu]&&&&
}
\end{array}
\end{array}
$$
\caption{Pictorial representation of some of the nodes in the
hypothesis graphs ($G_{amp}$) for the 3
 rules of Do Calculus. In this picture,
 yellow nodes are conditioned on.
 Furthermore, $\rva.'=\rva.-an(\rvs.)$
 and $\rva.'' =\rva.\cap an(\rvs.)$.
 Note that since
 we are conditioning on $\rvs.$,
 there are non-blocked paths,
 indicated in red, between $\rvb.$ and
 $\rva.''$ for Rule 3 (or between $\rvb.$ and
 $\rva.$ for Rules 1 and 2).
 All these paths
 pass through the conditioned collider $\rvs.$. The hypothesis
 of Rules 1,2,3 is meant to
 ban paths like those in red.
}
\label{fig-do-rules}
\end{figure}



Fig.\ref{fig-do-rules} shows
a pictorial representation of 
some of the nodes (i.e., $\rva., \rvb., \rvr., \rvs.$) of the 
hypothesis graphs for the 3 
 rules of Do Calculus.

The 3 rules of DC are
3 separate theorems that have
been proven, by Pearl and collaborators, to be correct,
consistent, and 
sufficient
for removing
all do operator conditions
from any expression
for
which it
is possible to do so.

Next we discuss
two formulae that can be
proven using
Do Calculus:
the backdoor and the
frontdoor
adjustment formulae.

The
backdoor formula
adjusts one multinode
and the
frontdoor formula adjusts two.

For any two
disjoint
multinodes $\rvx.$
and $\rvy.$,
we define a {\bf backdoor path}  (resp., {\bf frontdoor path})
from $\rvx.$ to $\rvy.$
as a path from $\rvx.$
and $\rvy.$ that
starts with an arrow pointing into (resp., 
pointing out of)
$\rvx.$.


\section{Parent Adjustment Formula}


Suppose
that $\rvx., \rvy., \rvz.$
are disjoint multinodes
and their union equals
 the
totality of all nodes of
a bnet.
Suppose we have data
available that allows us  to
estimate $P(x., y., z.)$.
Hence, all nodes of the bnet
are observed.
Furthermore,
suppose $\rvz.=pa(\rvx.)$.
In other words,
we are
considering the bnet

\beq
\xymatrix{
\rvz.\ar[d]\ar[dr]
\\
\rvx.\ar[r]&\rvy.
}
\;.
\eeq

Then

\beq
P(y., z.|\cald \rvx.=x.)
=P(y.|x., z.)P(z.)
\eeq
so

\beq
P(y.|\cald \rvx.=x.)
=\sum_{z.}P(y.|x., z.)P(z.)
\;.
\eeq
This is called
{\bf adjusting the parents}
of $\rvx.$.


We say that
we are {\bf adjusting
or controlling a node $\rva$}
if we condition
a probability on $\rva$, and
then we average
that probability over $\rva$.
More generally,
we can adjust a whole
multinode $\rva.$ jointly.

Next,
we will introduce
a generalization
of
this parent adjustment formula
called the
backdoor adjustment formula.
In a backdoor adjustment formula,
the adjusted multinode
is not necessarily
 the parents of $\rvx.$.

\section{Backdoor Adjustment Formula}

See Chapter \ref{ch-bdoor}
for examples of the use of the
backdoor adjustment formula.
In this section,
we shall mainly be
concerned with
explaining this
theorem, not using it.

The backdoor adjustment formula is a simple generalization
of the parent adjustment formula to encompass 
cases where the variables being adjusted are 
not all parents of $\rvx$.


\bdoordef

\begin{claim} (Backdoor
Adjustment Formula)

\bdoorclaim
\end{claim}
\proof

\begin{figure}[h!]
$$\xymatrix
{
\bullet\ar@[red][r]
&\rvz.\ar[dl]\ar[dr]
&\bullet\ar@[red][l]\ar@[red][d]
\\
\rvx.\ar[rr]\ar@[red][u]
&&\rvy.
\\
&\bullet\ar@[red][ul]\ar@[red][ur]
}
$$
\caption{The red arrows in this figure define paths 
that violate the 2 constraints of the backdoor adjustment criterion.
 The reason for constraint 2 is that
 if we condition on node $\rvz.$
 in accordance with constraint 1, we open the
 red  path from $\rvx.$ to $\rvy.$
 via collider $\rvz.$.}
 \label{fig-bdoor-red-paths}
\end{figure}

 Fig.\ref{fig-bdoor-red-paths}
gives some motivation for the backdoor adjustment criterion.
See Claim \ref{cl-decBackDoor}
for a proof of this claim
for the
special case of the bnet
obtained by removing the red arrows from Fig.\ref{fig-bdoor-red-paths}.
\qed

Note that the backdoor adjustment  formula
can be written as

\beqa
P(y.|\cald \rvx. =x.)
&=&
\sum_{z.}P(y.|x., z.)P(z.)
\\
&=&
\sum_{z.}\frac{P(y.,x., z.)}
{P(x.|z.)}
\eeqa
This assumes $P(x.|z.)\neq 0$
for all $x., z.$. This assumption
is referred to
as {\bf positivity},
and is violated
if $P(x.|z.)=\delta(x., x.(z.))$.
$P(x.|z.)$ is called the
{\bf propensity score}
of $x.$ given $z.$.
This
equation does
{\bf inverse probability weighting}.
One
can approximate $P(x.|z.)$
in this equation
to get
an approximation
to  $P(y|\cald\rvx=x)$.


\section{Frontdoor Adjustment Formula}
See Chapter \ref{ch-fdoor}
for examples of the use of the
frontdoor adjustment formula.
In this section,
we shall mainly be
concerned with
explaining this
theorem, not using it.

\fdoordef


\begin{claim} (Frontdoor Adjustment
Formula)

\fdoorclaim

\end{claim}
\proof
\begin{figure}[h!]
$$
\xymatrix{
&*++[F-o]{\rvc}\ar[ld]\ar[rd]
\\
\rvx\ar[r]\ar@[red][dr]
&\rvm\ar[r]
&\rvy
\\
\bullet\ar@[red][u]\ar@[red][ur]
&\bullet\ar@[red][ur]
&\bullet\ar@[red][u]\ar@[red][ul]
}
$$
\caption{The red arrows in this figure define paths 
that violate the 3 constraints of the frontdoor adjustment criterion.}
 \label{fig-fdoor-red-paths}
\end{figure}

Fig.\ref{fig-fdoor-red-paths}
gives some motivation for the frontdoor adjustment criterion.
See Claim \ref{cl-decFrontDoor}
for a proof of this claim
for the
special case of the bnet obtained from 
Fig.\ref{fig-fdoor-red-paths}
by removing  the red arrows.
See also Ref.\cite{pearl-frontdoor}
for a proof by Pearl
of the Frontdoor Adjustment Formula
without
using Do Calculus.
\qed

\section{Comparison
of Backdoor and Frontdoor
adjustment formulae}

Define a {\bf direct effect path}
for a query $P(y|\cald\rvx=x,z.)$
as a directed path that starts at $\rvx$
and ends
at $\rvy$. A backdoor path
(i.e., one that connects
$\rvx$ and $\rvy$ starting
with an arrow
pointing into $\rvx$),
is not a direct effect path;
it's an {\bf indirect effect path}.

Note that in the backdoor AF (adjustment
formula), we can find a possibly empty
observed multinode
$\rvz.$ such that if
we condition
on $\rvz.$,
all indirect effect paths are blocked.
In the frontdoor AF,
we can't find a multinode $\rvz.$
that blocks all indirect effect
paths.
Despite this,
in the frontdoor scenario,
the do-query is
identifiable and
an adjustment formula
exists.
How is that possible?
The frontdoor AF
uses the backdoor AF once,
and then it uses
the backdoor AF again,
a second time, on
the result of the first use.
The frontdoor AF
replaces a
sum over an unobserved node
by a sum over an observed one.



\section{Do operator for DEN bnets}

Recall that
the structural
equations
for a linear DEN, as
given
by Eq.(\ref{eq-mat-fully-conn})
of Chapter \ref{ch-LDEN}, are:

\beq
\rvx=A\rvx +\rvu
\;.
\label{eq-struc-pre-rho}
\eeq
Therefore,

\beq
\rvx=(1-A)^{-1}\rvu
\eeq
which
can be
represented for
both linear
and non-linear DEN
diagrams by:

\beq
\rvx_i = x_i(\rvu.)
\eeq

If now
we apply the
operator
$\cald_{\rva=a}$
to
the diagram
described by
the structural
equations Eqs.\ref{eq-struc-pre-rho},
we get the following
new
structural
equations:

\beq
\rvx^*_i =\left\{
\begin{array}{lll}
 \sum_{j<i} A_{i|j}\rvx^*_j + \rvu_i&
 \text{if $\rvx_i\neq \rva$}
\\
a&
\text{if $\rvx_i=\rva$}
\end{array}
\right.
\label{eq-ith-struc-post-rho}
\;,
\eeq
where we are
calling
$\rvx^*_i$ the
nodes
of the DEN
diagram post intervention.

Eqs.(\ref{eq-ith-struc-post-rho})
can be expressed in matrix notation
as follows.
Define $\pi_\rva$ to
be the $nx\times nx$ matrix
with all entries equal
to  zero
except for the $(i_0,i_0)$ entry, which is 1.
And define $e_\rva$
to be the column vector
with all entries zero
except for the $i_0$'th one,
which is 1.
Here
$i_0$
is
defined so that $\rvx_{i_0}=\rva$.
In other words, $\pi_\rva$ and $e_\rva$
are defined by

\beq
(\pi_\rva)_{i,j}= \indi(i=j, \rva=\rvx_i)
\;
\eeq
and

\beq
(e_\rva)_i=\indi(\rva=\rvx_i)
\;,
\eeq
for $i, j\in \{0, 1, \ldots, nx-1\}$.
Next define

\beq
\pi_{!\rva}=1-\pi_\rva
\;,
\eeq

\beq
A^*=\pi_{!\rva} A
\;,
\eeq
and

\beq
\rvu_{!\rva}=\pi_{!\rva} \rvu
\;.
\eeq
The effect
of pre-multiplying
the matrix
$A$
and the column vector $\rvu$ by
$\pi_{!\rva}$
is to leave all rows
intact except for
the $i_0$
row, which is set to zero. Here
 $i_0$ is defined by
 $\rva=\rvx_{i_0}$.
Finally,
using
all
the
variables just defined,
we can express the
structural equations
of the linear DEN bnet,
post intervention, as


\beq
\rvx^*= A^* \rvx^* + \rvu_{!\rva} +
ae_\rva
\;.
\eeq
Thus,

\beq
\rvx^*=(1-A^*)^{-1} (\rvu_{!\rva}+ae_\rva)
\;.
\eeq
which
can be
represented for
both linear
and non-linear DEN
diagrams by:

\beq
\rvx^*_i=x^*_i(\rvu_{!\rva},a)
\;.
\eeq



For any bnet,

\beq
P(\rvy=y|\rvx=x)
=
P_{G}(\rvy=y|\rvx=x)
\eeq

\beq
P(\rvy=y|\cald\rvx=x)
=
P_{\cald_{\rvx=x}G}(\rvy=y)
\eeq


\begin{claim}
For a non-linear DEN bnet,



\beq
P(y|\cald\rvx=x)=
E\left[
\delta[y, y(\rvu_{!\rvx},x)]\right]
\;.
\eeq
\end{claim}
\proof
\beqa
P(\rvy=y|\cald\rvx=x)
&=&
P_{\cald_{\rvx=x}G}(\rvy=y)
\\
&=&\sum_{u_{!\rvx}}P(u_{!\rvx})
P_{\cald_{\rvx=x}G}
(\rvy=y|u_{!\rvx})
\\
&=&\sum_{u_{!\rvx}}P(u_{!\rvx})
\delta[y, y(u_{!\rvx},x)]
\\
&=&
E_{\rvu_{!\rvx}}
[\delta[y, y(u_{!\rvx}, x)]]
\\
&=&
E[\delta[y,y(\rvu_{!\rvx}, x)]]
\eeqa
\qed

\begin{claim}
For a nonlinear DEN bnet,

\beq
E[\rvy|\cald \rvx=x]=
E[y(\rvu_{!\rvx}, x)]
\;.
\eeq
\end{claim}
\proof

\beqa
E[\rvy|\cald \rvx=x]
&=&
\sum_{y}
yP(\rvy=y|\cald\rvx=x)
\\
&=&
\sum_{y}
yE[
\delta[y, y(u_{!\rvx},x)]]
\\
&=&
E[y(\rvu_{!\rvx}, x)]
\eeqa
\qed


For any bnet
\beqa
P(y|\cald\rvx=x, z)&=&
\frac{P(y, z|\cald\rvx=x)}
{P(z|\cald\rvx=x)}
=
P_{\cald_{\rvx=x}G}(y|x, z)
\eeqa

For a nonlinear DEN bnet,
\beq
P(y, z|\cald\rvx=x)
=
\sum_{u_{!\rvx}}P(u_{!\rvx})
\delta[y, y(u_{!\rvx},x)]
\delta[z, z(u_{!\rvx},x)]
\eeq

\beq
P(z|\cald\rvx=x)=
\sum_{u_{!\rvx}}P(u_{!\rvx})
\delta[z, z(u_{!\rvx},x)]
\;.
\eeq

