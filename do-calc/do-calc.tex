\chapter{Do-Calculus}\label{ch-do-calc}


The do-calculus and associated ideas were
invented by
Judea Pearl and collaborators.
This chapter is 
based on Judea Pearl's
books (see \nameref{ch-nav-pearl}).


When
doing
do-calculus,
it is 
convenient
to separate
the nodes
of a bnet
into
2  types:
{\bf visible (observed)},
and {\bf non-visible (not observed, latent,
hidden)},
depending
on
whether data
describing
the
state 
of that
node
is available 
(visible) or not (non-visible).
In this chapter, every
hidden node will 
be indicated 
in a bnet
diagram by
either: (1)
enclosing
its random variable
in a dashed circle or
(2) making
the arrows
coming
out of it
dashed.
Accordingly, 
the 
3 diagrams 
in
Fig.\ref{fig-hidden-dashes}
all mean the same thing.

A {\bf confounder node} $\rvc$
for nodes $\rvx$ and $\rvy$
is a root node
with arrows
pointing
from it to
both
$\rvx$ and $\rvy$.
Thus, $\rvc$ acts as a common cause
of $\rvx$ and $\rvy$.
The node
$\rvc$
in Fig.\ref{fig-hidden-dashes})
is an {\bf unobserved confounder node}.

In this book,
we will refer
to a path
all of whose nodes are
observed as an {\bf opath}.



\begin{figure}[h!]
$$\xymatrix{
&*++[F-o]{\rvc}\ar[dl]\ar[dr]
\\
\rvx\ar[rr]&&\rvy
}
\;\;\;
\xymatrix{
\rvx\ar@{-->}@{<-->}@/^2pc/[rr]
\ar[rr]&&\rvy
}
\;\;\;
\xymatrix{
&\rvc\ar@{-->}[dl]\ar@{-->}[dr]
\\
\rvx\ar[rr]&&\rvy
}$$
\caption{
These 3 diagrams
are equivalent.
They
mean that node $\rvc$
is hidden.
Node $\rvc$
is implicit
in the
middle diagram.}
\label{fig-hidden-dashes}
\end{figure}



Define
an
operator
$\cald_\rvx$
that acts on
a node
$\rvx$
of a bnet
to
delete
all
the 
arrows
entering
$\rvx$,
thus
converting
$\rvx$
into
a new
node $\cald \rvx$
that
is a root node.
Define 
an analogous 
operator
$\call\rvx$
that acts on
a node
$\rvx$
of a bnet
to
delete
all
the 
arrows
leaving
$\rvx$,
thus
converting
$\rvx$
into
a new
node $\call \rvx$
that
is a leaf node.
$\cald_\rvx$
and
$\call_\rvx$
are
depicted
in Fig.\ref{fig-do-rho-lam}.



\begin{figure}[h!]
\centering
\includegraphics[width=4in]
{do-calc/do-rho-lam.png}
\caption{
The operator $\cald_\rvx$
converts node $\rvx$
into a root node $\cald \rvx$.
The operator $\call_\rvx$
converts node $\rvx$
into a leaf node $\call\rvx$.
} 
\label{fig-do-rho-lam}
\end{figure}


If
you don't
know yet
what we mean by a
a multi-node
$\rva.$, see
Chapter \nameref{ch-bnet-def}

Given a bnet
$G$,
we define
as follows
the operators
$\cald_{\rva.}$
and
$\call_{\rva.}$
for a multi-node
$\rva.$.

\beq
\cald_{\rva.}G =
\left[\prod_j \cald_{\rva_j}\right]G
\;,\;\;\;\;
\call_{\rva.}G =
\left[\prod_j \call_{\rva_j}\right]G
\;.
\eeq

Consider a bnet 
whose totality of nodes
is labeled $\rvX.$.
Recall that 

\beq
P(X.)=
\prod_j P(X_j|(X_k)
_{k:\rvX_k\in pa(\rvX_j)})
\;.
\eeq

Define an
operator $\cald$
that acts as follows\footnote{As usual,
$\caln(!x)$ denotes 
a constant 
that is independent of $x$.}: Let
$X.-a.=(X_k)_{k:\rvX_k\notin \rva.}$.

\beqa
P(X.-a.|\cald\rva.=a.)
&=&
\caln(!(X.-a.))
\frac{P(X.)}
{
\prod_{j:\rvX_j\in \rva.}
P(X_j|(X_k)
_{k:\rvX_k\in pa(\rvX_j)})
}
\\
&=&
\caln(!(X.-a.))
\prod_{j:\rvX_j\notin \rva.}
P(X_j|(X_k)
_{k:\rvX_k\in pa(\rvX_j)})
\\
&\neq&
P(X.-a.|\rva.=a.)
\;.
\eeqa
Also,

\beq
P(X.-a.,\cald\rva.=a.')=
P(X.-a.|\cald\rva.=a.)
\delta(a'., a.)
\;.
\eeq
In words, we replace
the TPM for 
multinode
$\rva.$ by
a deterministic
prior
distribution.

For instance, for the bnet

\beq
\xymatrix{
\rvx\ar[r]&\rvy
}
\eeq
with 

\beq
P(x,y)=P(y|x)P(x)
\;,
\eeq
one has 

\beq
P(y|\cald\rvx=x)=P(y|x)
\eeq
and

\beq
P(x|\cald \rvy=y)=P(x)
\;.
\eeq
This means that $\rvx$ causes $\rvy$
and $\rvy$ does not cause $\rvx$.

For the bnet

\beq
\xymatrix{
\rvc\ar[d]\ar[rd]
\\
\rvx\ar[r]&\rvy
}
\eeq
with 

\beq
P(x,y, c)=P(y|x, c)P(x|c)P(c)
\;,
\eeq
one has 

\beq
P(y, c|\cald\rvx=x)=P(y|x, c)P(c)
\;.
\eeq
Hence,

\beq
P(y|\cald\rvx=x)=\sum_c P(y|x,c)P(c)
\;.
\eeq
This is called {\bf adjusting the parents
of $\rvx$}.

For
$\rvb.\subset \rvX.-\rva.$,
define

\beq
P(b.|\cald\rva. =a.)=
\sum_{X.-a.-b.}
P(X.-a.|\cald\rva.=a.)
\;,
\eeq
and for
$\rvs.\subset \rvX.-\rva.-\rvb.$,
define

\beq
P(b.|\cald \rva.=a., s.)=
\frac{P(b., s.|\cald\rva.=a.)}
{P(s.|\cald\rva.=a.)}
\;.
\eeq

$P(b.|\cald \rva.=a., s.)$
is usually denoted instead  by
$P(b.|do(\rva.=a.), s.)$.
I prefer to 
use $\cald$
instead of $do()$.
I'll still call $\cald$
a {\bf do operator}. 

In $P(y|\cald \rvx=x)$,
node $\rvx$ is turned 
into a root node. This guarantees
that there is
no confounding node
connecting $\rvx$ and
$\rvy$. Such 
confounding nodes 
are unwelcomed 
when calculating
causal effects
between 
the 2 variables $\rvx$ and $\rvy$
 because they 
 introduce 
non-causal
correlations between
the two.
This is also 
what happens
in a {\bf Randomized 
Controlled Trial (RCT)}.
In a RCT
 with treatment $\rvx$,
the value
of $\rvx$ for each patient
is determined by a coin toss,
effectively
turning $\rvx$ into a root node.
Hence, the do operator mimics a RCT.


$P(b.|\cald \rva.=a., s.)$
is said to be {\bf identifiable}
(i.e., expressible without do())
if it can be
expressed in terms of
probability distributions
that only
depend on observed 
variables, and that
have no do operators
in them. 

For example,
if $\rvy$ has only one parent,
namely $\rvx$, then $P(y|x)$ is a TPM
of the bnet, so $P(y|\cald\rvx=x)=P(y|x)$.
Thus, in general, if $\rvy$ has only
one parent, $P(y|\cald\rvx=x)$ is 
identifiable. On the other hand, if
$\rvy$ has two parents, $\rvx$ and $\rva$, then
$P(y|x,a)$, not $P(y|x)$,
is the TPM for node $\rvy$,
so we will need more than the TPM
of node $\rvy$ to calculate $P(y|\cald\rvx=x)$.
So if $\rvy$ has more parents than node $\rvx$,
then the identifiability of $P(y|\cald\rvx=x)$
is not guaranteed.

 
As a second example,
$P(y|\cald\rvx=x)$ is identifiable
for the bnet

\beq
\xymatrix{
\rvz\ar[d]\ar[dr]
\\
\rvx\ar[r]&\rvy
}
\eeq
but it is not identifiable for the bnet

\beq
\xymatrix{
*++[F-o]{\rvz}\ar[d]\ar[dr]
\\
\rvx\ar[r]&\rvy
}
\eeq


For $\rvx, \rvy\in \bool$,
 the {\bf average controlled effect (ACE)}
is defined as

\beq
ACE=
P(y=1|\cald \rvx=1)-
P(y=1|\cald \rvx=0)
\eeq
and the 
{\bf Risk Difference (RD)} as

\beq
RD=
P(y=1|\rvx=1)-
P(y=1|\rvx=0)
\;.
\eeq

\section{Parent Adjustment Formula}


Suppose 
that $\rvx., \rvy., \rvz.$
are disjoint multinodes
and their union equals
 the
totality of all nodes of
a bnet. 
Suppose we have data
available that allows us  to
estimate $P(x., y., z.)$.
Hence, all nodes of the bnet
are observable.
Furthermore,
suppose $\rvz.=pa(\rvx.)$.
In other words,
we are 
considering the bnet

\beq
\xymatrix{
\rvz.\ar[d]\ar[dr]
\\
\rvx.\ar[r]&\rvy.
}
\;.
\eeq

Then

\beq
P(y., z.|\cald \rvx.=x.)
=P(y.|x., z.)P(z.)
\eeq
so

\beq
P(y.|\cald \rvx.=x.)
=\sum_{z.}P(y.|x., z.)P(z.)
\;.
\eeq
This is called
{\bf adjusting the parents}
of $\rvx.$.


We say that 
we are {\bf adjusting 
or controlling a variable $\rva$}
if we condition 
a probability on $\rva$ and 
then we average 
that probability over $\rva$.
More generally, 
we can adjust a whole
multinode $\rva.$ together.

Later on,
we will introduce 
a generalization
of 
this parent adjustment formula
called the 
backdoor adjustment formula.
In a backdoor adjustment formula,
the adjusted multinode
is not necessarily
 the parents of $\rvx.$, 
and $P(x., y., z.)$
need not represent the
whole bnet.



\section{3 Rules of do-calculus}
Throughout 
this section, suppose
$\rva., \rvb., \rvr., 
\rvs.$ are disjoint
multinodes in a bnet $G$.


Recall
from Chapter \ref{ch-dsep}
on d-separation,
that  $(\rvb.\perp_G \rva.|\rvr., \rvs.)$
means that 
we have established
from the d-separation
rules that 
that all 
paths in $G$
 from
$\rva.$ to
$\rvb.$
are blocked
if we condition
on $\rvr.\cup \rvs.$.
Recall also that:

\begin{itemize}
\item {\bf Rule 0:} Insertion or
 deletion of
 observations, without
do operators.
($\rva.=a. \leftrightarrow 1$ )


If 
 $(\rvb.\perp_G \rva.|\rvr.,
\rvs.)$, then
$P(b.|a., r., s.)=P(b.|r., s.)$
\end{itemize}

The 3 rules of do-calculus
can be presented in the same
format. 


\begin{color}{red}
\begin{itemize}
\item {\bf Rule 1:} 
Insertion or deletion of
 observations 
($\rva.=a. \leftrightarrow 1$ )

\ruleone

\item {\bf Rule 2:} Action or 
observation exchange 
($\cald \rva.=a. \leftrightarrow \rva.=a.$)

\ruletwo

\item {\bf Rule 3:} Insertion and
 deletion of actions
($\cald \rva.=a. \leftrightarrow 1$)

\rulethree


\end{itemize}
\end{color}

These rules have been
proven to be 
sufficient
for removing
all do operators
from an expression
for 
which it 
is possible to do so.

Next we discuss
two theorems that can be
proven using
do-calculus:
the backdoor and the
front-door
adjustment formulae.

The 
backdoor theorem 
adjusts one multinode
and the 
front-door theorem adjusts two.


\section{Backdoor Adjustment Formula}

See Chapter \ref{ch-bdoor}
for examples of the use of the 
backdoor adjustment formula.
In this section,
we shall mainly be
concerned with
proving this
theorem
using do-calculus.

For any two
disjoint
multinodes $\rvx.$
and $\rvy.$,
we define a {\bf backdoor path}
from $\rvx.$ to $\rvy.$
as a path from $\rvx.$
and $\rvy.$ that
starts with an arrow pointing into 
$\rvx.$,

\bdoordef

{\bf Motivation for BD criterion}: 
Part 1 rules out
paths 
from $\rvx$
to $\rvy$
containing a fork node (confounder)
which, if not blocked by $\rvz.$, 
 would introduce a
non-causal correlation 
(confounder bias).
Part 2 rules out
a directed path
from $\rvx$ to $\rvy$
that has a mediator node
blocked by $\rvz.$
or a collider node
unblocked by $\rvz.$.



\begin{claim} Backdoor Adjustment Formula

\bdoorclaim
\end{claim}
\proof

For simplicity,
let us omit
the dots from the
multinodes.
If
$z$
satisfies the
backdoor
criterion
relative
to
$(\rvx, \rvy)$,
then
$\rvx, \rvy, \rvz$
must 
have the following 
structure.


\beq
\xymatrix{
{\rvz}\ar[d]\ar[rd]
\\
\rvx\ar[r]&\rvy
}
\eeq
\beq
\begin{array}{lllll}
&&\color{red}
P(y|\cald\rvx=x)=
\\
&=&
\color{red}
\sum_z
P(y|\cald\rvx=x, z)
P(z|\cald\rvx=x)
\\
&&\text{by Probability Axioms}
\\
&=&\color{red}
\sum_z 
P(y|x, z)
P(z|\cald\rvx=x)
\\
&&P(y|\cald \rvx=x, z)\rarrow
P(y|x, z)
\\
&& \text{ by Rule 2: \ruletwo}
\\
&&
\rvy\perp \rvx|\rvz
\text{ in }\call_\rvx G
\;\;\;\;
\xymatrix{
{\rvz}\ar[d]\ar[rd]
\\
\rvx&\rvy
}
\\
&=&\color{red}
\sum_z 
P(y|x, z)
P(z)
\\
&&P(z|\cald \rvx=x)\rarrow
P(z)
\\
&& \text{ by Rule 3: \rulethree}
\\
&&
\rvz\perp \rvx
\text{ in }\cald_\rvx G
\;\;\;\;
\xymatrix{
{\rvz}\ar[rd]
\\
\rvx\ar[r]&\rvy
}
\end{array}
\eeq
\qed

Note that the backdoor adjustment  formula
can be written as
 
\beqa
P(y.|\cald \rvx. =x.)
&=&
\sum_{z.}P(y.|x., z.)P(z.)
\\
&=&
\sum_{z.}\frac{P(y.,x., z.)}
{P(x.|z.)}
\eeqa
This assumes $P(x.|z.)\neq 0$
for all $x., z.$. This assumption
is referred to
as {\bf positivity},
and is violated
if $P(x.|z.)=\delta(x., x.(z.))$.
$P(x.|z.)$ is called the 
{\bf propensity score}
of $x.$ given $z.$.
This
equation does 
{\bf inverse probability weighting}.
One
can approximate $P(x.|z.)$ 
in this equation
to get
an approximation
to  $P(y|\cald\rvx=x)$.


\section{Front Door Adjustment Formula}
See Chapter \ref{ch-fdoor}
for examples of the use of the 
front-door adjustment formula.
In this section,
we shall mainly be
concerned with
proving this
theorem
using do-calculus.

\fdoordef

\begin{claim} Front-Door Adjustment
Formula

\fdoorclaim

\end{claim}
\proof
(See also Ref.\cite{pearl-front-door}
for a proof
of the Front-Door Adjustment Formula
without 
using do-calculus.)

For simplicity,
let us omit
the dots from the
multinodes.
If
$\rvm$
satisfies the
front-door
criterion
relative
to
$(\rvx, \rvy)$,
then
$\rvx, \rvm, \rvy$
must 
have the following 
structure,
where
node $\rvc$
is hidden. 


\beq
\xymatrix{
&*++[F-o]{\rvc}\ar[ld]\ar[rd]
\\
\rvx\ar[r]&\rvm\ar[r]&\rvy
}
\eeq
\beq
\begin{array}{lllll}
&&\color{red}
P(y|\cald\rvx=x)=
\\
&=&
\color{red}
\sum_m 
P(y|\cald\rvx=x, m)
P(m|\cald\rvx=x)
\\
&&\text{by Probability Axioms}
\\
&=&\color{red}
\sum_m 
P(y|\cald\rvx=x, \cald\rvm=m)
P(m|\cald\rvx=x)
\\
&&P(y|\cald\rvx=x, m)\rarrow
P(y|\cald\rvx=x, \cald m=m)
\\
&& \text{ by Rule 2: \ruletwo}
\\
&&
\rvy\perp \rvm|\rvx
\text{ in }\call_\rvm\cald_\rvx G
\xymatrix{
&*++[F-o]{\rvc}\ar[rd]
\\
\rvx\ar[r]&\rvm&\rvy
}
\\
&=&\color{red}
\sum_m 
P(y|\cald\rvx=x, \cald\rvm=m)
P(m| x)
\\
&&
P(m|\cald\rvx=x)\rarrow P(m|x)
\\
&&\text{by Rule 2: \ruletwo}
\\
&&
\rvm\perp\rvx
\text{ in }
\call_\rvx G
\xymatrix{
&*++[F-o]{\rvc}\ar[ld]\ar[rd]
\\
\rvx&\rvm\ar[r]&\rvy
}
\\
&=&\color{red}
\sum_m 
P(y|\cald\rvm=m)
P(m|x)
\\
&&
P(y|\cald\rvx=x, \cald\rvm=m)
\rarrow
P(y|\cald\rvm=m)
\\
&&\text{by Rule 3: \rulethree}
\\
&&
\rvy\perp\rvx|\rvm
\text{ in }
\cald_\rvx\cald_\rvm G
\xymatrix{
&*++[F-o]{\rvc}\ar[rd]
\\
\rvx&\rvm\ar[r]&\rvy
}
\\
&=&\color{red}
\sum_{x'}
\sum_m 
P(y|\cald\rvm=m, x')
P(x'|\cald\rvm=m)
P(m|x)
\\
&&\text{by Probability Axioms}
\\
&=&\color{red}
\sum_{x'}
\sum_m 
P(y|m, x')
P(x'|\cald\rvm=m)
P(m|x)
\\
&&
P(y|\cald\rvm=m, x')
\rarrow
P(y|m, x')
\\
&& \text{by Rule 2: \ruletwo}
\\
&&
\rvy\perp\rvm|\rvx
\text{ in }
\call_\rvm G
\xymatrix{
&*++[F-o]{\rvc}\ar[rd]\ar[ld]
\\
\rvx\ar[r]& \rvm&\rvy
}
\\
&=&\color{red}
\sum_{x'}
\sum_m 
P(y|m, x')
P(x')
P(m|x)
\\
&&
P(x'|\cald\rvm=m)
\rarrow
P(x')
\\
&&\text{by Rule 3: \rulethree}
\\
&&
\rvx\perp\rvm
\text{ in }
\cald_\rvm G
\xymatrix{
&*++[F-o]{\rvc}\ar[rd]\ar[ld]
\\
\rvx&\rvm\ar[r]&\rvy
}
\end{array}
\eeq
\qed

\section{Do operator for DEN diagrams}

Recall that
the structural
equations
for a linear DEN, as
given
by Eq.(\ref{eq-mat-fully-conn})
of Chapter \ref{ch-linear-sys}, are:

\beq
\rvx=A\rvx +\rvu
\;.
\label{eq-struc-pre-rho}
\eeq
Therefore,

\beq
\rvx=(1-A)^{-1}\rvu
\eeq
which
can be 
represented for
both linear
and non-linear DEN
diagrams by:

\beq
\rvx_i = x_i(\rvu.)
\eeq 

If now
we apply the
operator
$\cald_{\rva=a}$
to 
the diagram
described by
the structural
equations Eqs.\ref{eq-struc-pre-rho},
we get the following
new
structural
equations:

\beq
\rvx^*_i =\left\{
\begin{array}{lll}
 \sum_{j<i} A_{i|j}\rvx^*_j + \rvu_i&
 \text{if $\rvx_i\neq \rva$}
\\
a&
\text{if $\rvx_i=\rva$}
\end{array}
\right.
\label{eq-ith-struc-post-rho}
\;,
\eeq
where we are
calling 
$\rvx^*_i$ the
nodes
of the DEN 
diagram post intervention.

Eqs.(\ref{eq-ith-struc-post-rho})
can be expressed in matrix notation
as follows.
Define $\pi_\rva$ to
be the $nx\times nx$ matrix 
with all entries equal
to  zero
except for the $(i_0,i_0)$ entry, which is 1.
And define $e_\rva$
to be the column vector
with all entries zero
except for the $i_0$'th one, 
which is 1. 
Here
$i_0$  
is
defined so that $\rvx_{i_0}=\rva$.
In other words, $\pi_\rva$ and $e_\rva$
are defined by

\beq
(\pi_\rva)_{i,j}= \indi(i=j, \rva=\rvx_i)
\;
\eeq
and

\beq
(e_\rva)_i=\indi(\rva=\rvx_i)
\;,
\eeq
for $i, j\in \{0, 1, \ldots, nx-1\}$.
Next define

\beq
\pi_{!\rva}=1-\pi_\rva
\;,
\eeq

\beq
A^*=\pi_{!\rva} A
\;,
\eeq
and

\beq
\rvu_{!\rva}=\pi_{!\rva} \rvu
\;.
\eeq
The effect
of pre-multiplying
the matrix
$A$ 
and the column vector $\rvu$ by
$\pi_{!\rva}$
is to leave all rows
intact except for
the $i_0$
row, which is set to zero. Here
 $i_0$ is defined by
 $\rva=\rvx_{i_0}$.
Finally,
using 
all
of the
variables just defined,
we can express the
structural equations
of the linear DEN diagram,
post intervention, as


\beq
\rvx^*= A^* \rvx^* + \rvu_{!\rva} +
ae_\rva
\;.
\eeq
Thus,

\beq
\rvx^*=(1-A^*)^{-1} (\rvu_{!\rva}+ae_\rva)
\;.
\eeq
which
can be 
represented for
both linear
and non-linear DEN
diagrams by:

\beq
\rvx^*_i=x^*_i(\rvu_{!\rva},a)
\;.
\eeq



For any bnet,

\beq
P(\rvy=y|\rvx=x)
=
P_{G}(\rvy=y|\rvx=x)
\eeq

\beq
P(\rvy=y|\cald\rvx=x)
=
P_{\cald_{\rvx=x}G}(\rvy=y)
\eeq


\begin{claim}
For a non-linear DEN diagram,



\beq
P(y|\cald\rvx=x)=
E\left[
\delta[y, y(\rvu_{!\rvx},x)]\right]
\;.
\eeq
\end{claim}
\proof
\beqa
P(\rvy=y|\cald\rvx=x)
&=&
P_{\cald_{\rvx=x}G}(\rvy=y)
\\
&=&\sum_{u_{!\rvx}}P(u_{!\rvx})
P_{\cald_{\rvx=x}G}
(\rvy=y|u_{!\rvx})
\\
&=&\sum_{u_{!\rvx}}P(u_{!\rvx})
\delta[y, y(u_{!\rvx},x)]
\\
&=&
E_{\rvu_{!\rvx}}
[\delta[y, y(u_{!\rvx}, x)]]
\\
&=&
E[\delta[y,y(\rvu_{!\rvx}, x)]]
\eeqa
\qed

\begin{claim}
For a nonlinear DEN diagram,

\beq
E[\rvy|\cald \rvx=x]=
E[y(\rvu_{!\rvx}, x)]
\;.
\eeq
\end{claim}
\proof

\beqa
E[\rvy|\cald \rvx=x]
&=&
\sum_{y}
yP(\rvy=y|\cald\rvx=x)
\\
&=&
\sum_{y}
yE[
\delta[y, y(u_{!\rvx},x)]]
\\
&=&
E[y(\rvu_{!\rvx}, x)]
\eeqa
\qed


For any bnet
\beqa
P(y|\cald\rvx=x, z)&=&
\frac{P(y, z|\cald\rvx=x)}
{P(z|\cald\rvx=x)}
=
P_{\cald_{\rvx=x}G}(y|x, z)
\eeqa

For a nonlinear DEN diagram,
\beq
P(y, z|\cald\rvx=x)
=
\sum_{u_{!\rvx}}P(u_{!\rvx})
\delta[y, y(u_{!\rvx},x)]
\delta[z, z(u_{!\rvx},x)]
\eeq

\beq
P(z|\cald\rvx=x)=
\sum_{u_{!\rvx}}P(u_{!\rvx})
\delta[z, z(u_{!\rvx},x)]
\;.
\eeq
