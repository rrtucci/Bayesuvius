\chapter{Dimensionality Reduction}
\label{ch-dim-reduc}


This chapter is based on Ref. \cite{wiki-dim-reduc}.


Suppose you have a dataset with 500 feature columns $\rvX$ plus a treatment
column $\rvt$ and an outcome column $\rvy$.
Causal DAG discovery software such as  bnlearn (see Chapter \ref{ch-struc-learn}) 
cannot handle that much data.
In order to make any headway in this situation,
some kind of lossy data compression is required. That is precisely what {\bf dimensionality reduction} (DR) is: a type
of lossy data compression for datasets whereby the number of feature columns is reduced.
More specifically, DR of a 500 feature dataset might entail the 
following steps:
\begin{enumerate}

\item choose from the 500 features $\rvX$, a bunch of disjoint feature subsets $\rvS_i$ such that in each subset, the variables are very strongly correlated as in a clique, so they truly act as if they are inseparable, as a single node that we shall refer to as a combined node.

\item reduce the number of values (i.e., states) of the combined nodes using DR.

\item run causal discovery on the uncombined nodes, combined nodes and $(\rvt, \rvy)$
 nodes.
\end{enumerate}

The above steps for {\bf bnet  coarsening} can be
viewed as a special case of what physicists call a {\bf Renormalization Group} (RG) transformation. Ken Wilson received a Nobel Prize in 1982 for his RG theory.

In our steps for bnet coarsening,
we did not specify how to do DR. 
The most common
method for doing DR is
Principal Component Analysis (PCA)
(see Chapter \ref{ch-pca}). PCA 
only keeps the largest principal 
components so it is truly lossy. Non-negative Matrix Factorization (NN MF)
(see Chapter \ref{ch-nn-mat-fac})
and Factor Analysis (FA) (see Chapter \ref{ch-factor-ana})
are often mentioned as other
methods, besides PCA, for doing DR. The way I've
defined DR, both of these methods aren't truly DR methods because both of them are non-lossy.

It is often said that the NN MF method doesn't throw away the mean value $\av{X}$ of the hidden variable $X$,
whereas PCA does. However, even though the calculation of the principal components of X
does assume $\av{X}=0$, and therefore $\av{Y}=\av{X}W=0$, we can certainly calculate and store the truncated 
versions of $\av{X}$ or $\av{Y}$.
