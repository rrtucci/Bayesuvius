\chapter{Gradient Descent}
\label{ch-gradient-descent}


{\bf Gradient Descent} (GD) is when we have a sequence of points $w_k\in\RR^n$ and a convex loss function $\call:\RR^n\rarrow \RR$ such that

\beq
\boxed{
w_{k+1}  = w_{k} -\alp\nabla_w\call(w_{k})}
\xymatrix{
&&\ar[d]^{-\alp\nabla_w\call(w_k)}\\
\ar[rr]_{w_{k+1}}\ar[rru]^{w_k}&&
}
\label{eq-grad-descent}
\eeq
for some \qt{learning rate} $\alp>0$.
Since the function $\call(w)$ is convex, 
it has a minimum $w^*$ and $w_k\rarrow w^*$
as $k\rarrow \infty$.

In Machine Learning (ML), it
is usually the case that

\beq
\call(w) = \sum_{\s=1}^{nsam}
\HAT{\call}(\haty^\s(x^\s, w), y^\s)
\eeq
where the sum is over $nsam$ samples. Normally, $nsam$ would be 
all the samples in a dataset.
In ML, $nsam$ is often very large, 
and $\nabla_w\HAT{\call} $ cannot
be calculated analytically so it must be calculated numerically\footnote{Gradients can 
be calculated numerically 
by the method of
back-propagation, which is explained in Chapter \ref{ch-backp}.},
individually for each of the $nsam$ samples. In such cases,
a Monte Carlo method called {\bf Stochastic Gradient Descent} (SGD)
is often used. 
SGD just means choosing at random
a small subset of the $nsam$ samples (\qt{mini-batch}), and
approximating $\nabla_w\call(w)$ by
an average of the gradients for the mini-batch.

This description of GD is fairly complete, so why this chapter? In this chapter, we will 
give a dynamical bnet (see Chapter \ref{ch-dyn-bnets}) illustrating how GD is used in ML.

\begin{figure}[h!]
\centering
$$\xymatrix{
&\vec{\rvx}\ar[d]\ar[r]&\ul{\vecy}
\ar[d]&\\
w\ar[r]\ar@/_1pc/[rrr]&
\vec{\HAT{\rvy}}\ar[r]&\call\ar[r]&w'
}$$
\caption{Basic gradient descent bnet.}
\label{fig-bfit}
\end{figure}


Samples 
$(x^\s, y^\s)\in val(\rvx)\times val(\rvy)$
are given. $nsam(\vecx)=nsam(\vecy)$.

Estimator function 
$\haty(x; w)$
for $x\in val(\rvx)$ and $w\in\RR$
is given.

Let 
\beq
P_{\rvx, \rvy}(x,y)=
\frac{1}{nsam(\vecx)}
\sum_\s \indi(x=x^\s, y=y^\s)
\;.
\eeq


Let 
\beq
\call(\vecx, \vecy, w)=
\frac{1}{nsam(\vec{y})}
\sum_\s
|y^\s-\haty(x^\s; w)|^2
\;
\eeq
$\call$ is called the {\bf mean square error}.

Best fit is parameters $w^*$
such that

\beq 
w^*= \argmin_w
\call(\vecx, \vecy, w)
\;.
\eeq

The TPMs, printed in blue, for
the basic curve fitting bnet
 Fig.\ref{fig-bfit}, are
as follows.

\beq\color{blue}
P(w) \text{ = given}
\;.
\eeq
The first time
it is used, $w$ is arbitrary.
After the first time, it is determined 
by previous stage.

\beq\color{blue}
P(\vecx)=\prod_\s P_\rvx(x^\s)
\eeq

\beq\color{blue}
P(\vecy|\vecx)=\prod_\s P_{\rvy|\rvx}(y^\s\cond x^\s)
\eeq

\beq\color{blue}
P(\haty^\s|w, \vecx)=
\delta(\haty^\s, \haty(x^\s;w))
\eeq


\beq\color{blue}
P(\call|\vec{\haty}, \vecy)=
\delta(\call,\frac{1}{nsam(\vecx)}
\sum_\s |y^\s-\haty^\s|^2)
\;.
\eeq


\beq\color{blue}
P(w'|w, \call)=
\delta(w',
w-\alp\partial_w\call)
\eeq
$\alp>0$ is called {\bf the descent rate or learning rate}.
If $\Delta w=w'-w=-\alp 
\frac{\partial\call}
{\partial w}$, then
 $\Delta \call=\frac{-1}{\alp}
(\Delta w)^2<0$  so this will
minimize the error
$\call$.
This is an example of GD.