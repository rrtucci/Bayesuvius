\chapter{Stochastic Differential Equations COMING SOON}
\label{ch-stochastic-diff-eqns}


Stochastic Differential Equation (SDE)
First order differential equation with noise.
DEN (Deterministic bnet with External Noise) (see Chapter \ref{ch-linear-sys})

\section{Notation}

\hrule\noindent {\bf Random variables, mean, covariance}

\beq 
\av{\rva} = E[\rva]
\eeq

\beq
\Delta \rva = \rva - \av{\rva}
\eeq

\beq
Cov(\rva, \rvb) =\av{\rva, \rvb}=
\av{\Delta\rva \Delta\rvb}
\eeq

\beq
\Delta_{t_0}^{t_1}a = a(t_1) - a(t_0)
\eeq

\hrule \noindent
{\bf Intervals of real numbers and of integers}

$[a, b]=\{ x\in \RR: a\leq x \leq b\}$

Suppose $i, j\in \{0, 1, \ldots, N-1\}$

$[i:j] =\{i, i+1, \ldots, j-1\}$  (like Python)

$[i \upto j] = \{i, i+1, \ldots, j\}$ (To distinguish from Python,
we use dash instead of colon 
to indicate that last int is included.
)




$[n]=[0:n]=\{0, 1, 2, \ldots, n-1\}$


Consider times $t_0=0<t_1<t_2<\ldots < t_{N-1}$



$t_{[i\upto j]} = [t_i, t_{i+1}, \ldots, t_j]$


\beq
\lim_{N\rarrow \infty}t_{[i\upto j]} =
[t_i, t_j]
\eeq
\hrule\noindent {\bf Stochastic Process}

The {\bf outcome space} $\Omega$ 
denotes the set of all {\bf events} $\omega$ .
A {\bf stochastic process} $\rvx(t, \omega)\in\RR^n$ with $t\geq 0$ and $\omega\in\Omega$ is a map $\rvx:(\RR^+,\Omega) \rarrow \RR^n$. Normally, we don't write the $\omega$ dependence: we use $\rvx(t)$ instead of $\rvx(t, \omega)$.

For compactness, we will sometimes denote $x(t)$ by $x_t$ and an event $(x,t)$ by  $(\XT{x}{t})$. 

Will often use $x_i=x(t_i)$.

We will use lower case Latin indices like $i,j,k$ for time indices
and lower case Greek letters like $\alp, \beta, \mu, \nu$  for $x\in\RR^n$ components.
Hence $x_{\mu, i}=x_\mu(t_i)$

We will use the Einstein implicit index summation
convention for lower case Greek indices.
Hence

\beq
A_\mu B_\mu = \sum_{\mu \in[n]}A_\mu B_\mu
\eeq


$dx^n = dx_0 dx_1 \ldots dx_{n-1}$
\hrule\noindent
{\bf Path}

A path is defined as one of following sets:

$\rvx([t,s])=\{  \rvx(\tau): \tau\in[t,s]\}$


$\rvx(t_{[j\upto k]}) =\{  x(\tau): \tau\in\{t_j, t_{j+1}, \ldots, t_k\}\}$

$\rvx_{[j\upto k]} =\{ \rvx_j, \rvx_{j+1},
\ldots,\rvx_k\}$

Measure theorists speak of a family of sigma algebras wherein the elements of the family increase with $t$. They call this a {\bf filtration}. A path $x([t, s])$ is equivalent to a filtration,
so we won't speak of filtrations here.
\hrule
For any matrix $A$,
we will use $A^\dagger$ to denote the Hermitian conjugate $A$, $A^T$ to denote its transpose, and $A^*$ to denote its complex conjugate. $A^\dagger = A^{* T}$.

 
 \section{White Noise and Brownian Motion}
 
 \begin{figure}[h!]
 \centering
 \includegraphics[width=4in]
 {stochastic-diff-eqns/white-noise-labeled}
 \caption{One dimensional white noise $\rvW(t)$}
 \label{fig-white-noise-t}
 \end{figure}
 
 \begin{figure}[h!]
  \centering
  \includegraphics[width=4in]
  {stochastic-diff-eqns/brownian-motion-labeled}
  \caption{One dimensional Brownian motion $\rvB(t)$}
  \label{fig-brownian-motion-t}
  \end{figure}
  
 

White noise $\rvW(t)
\in\RR^n$ for $t\geq 0$

\begin{enumerate}
\item
\beq
\rvW(t) =\caln(W(t);\mu=0, Cov=Q)
\eeq

\item
$\rvW(t)$ and $\rvW(s)$ independent for
$t\neq s$

\item

\beq E[\rvW(t)]=0
\eeq

\item
\beq
C_\rvW(t, s) = \av{\rvW(t), \rvW^T(s)} = Q\delta(t-s)
\eeq



\end{enumerate}
Brownian motion (a.k.a. as a Wiener process) $\rvB\in\RR^n$
for $t\geq 0$

\begin{enumerate}

\item

\beq 
\rvB(0)=0\footnote{For $\rvB(0)=\beta_0\neq 0$, replace
$\rvB$ by $\rvB-\beta_0$}
\eeq

\item

\beq
\frac{
\Delta_{t_k}^{t_{k+1}}\rvB
}{\Delta_{t_k}^{t_{k+1}}t}
\sim \caln(\mu=0, Cov=Q)
\eeq

\beq
\rvW(t_k)\sim  \caln(\mu=0, Cov=Q)
\eeq

\beq
\frac
{d \rvB}{dt} = \rvW
\eeq


\item

\beq
E[|\Delta^{t}_{s}\rvB|^2] = n|t-s|
\eeq
where $\rvB(t)\in\RR^n$


\item
If $[r, s]\cap [r', s']=\emptyset$ then

\beq
E[(\Delta^{s}_{r}\rvB) (\Delta^{s'}_{r'}\rvB)]=0
\eeq


\beqa
E[\Delta_1^4\rvB \Delta_3^6\rvB]
&=&
E[(\Delta_1^3\rvB + \Delta_3^4\rvB)
(\Delta_3^4\rvB + \Delta_4^6\rvB)
]
\\
&=&
E[(\Delta_3^4\rvB)^2]
\\
&=& n |4-3|
\eeqa

In general

\beq
E[(\Delta^{s}_{r}\rvB) (\Delta^{s'}_{r'}\rvB)]=n {\rm len}([r, s]\cap [r', s'])
\eeq

\end{enumerate}


\section{SDE bnet}

special nodes

We discussed {\bf marginalizer node} in Chapter
\ref{ch-marginalizer}.

\begin{itemize}

\item {\bf diff and diff0 nodes} 

$$\xymatrix{
\rva\ar[dr]
&&\rvb\ar[dl]
\\
&
\rvx
}
$$

\beq  \color{blue}
P(x|a, b) =\indi (x =a-b)
\eeq

\beq  \color{blue}
x =a-b
\eeq
 diff0 node is the diff node with $x=0$
 


\item {\bf accumulator nodes}

$$
\xymatrix{
\rvx_3 \ar[d]
&\rvx_2\ar[d]
&\rvx_1\ar[d]
&\rvx_0\ar[d]
\\
\rvs_3 
&\rvs_2\ar[l]
 &\rvs_1\ar[l]
  &\rvs_0\ar[l]
}$$



\beq
\color{blue}
\begin{array}{lll}
\rvs_3 &=& \rvx_3 + \rvs_2
\\
\rvs_2 &=& \rvx_2 + \rvs_1
\\
\rvs_1 &=& \rvx_1 + \rvs_0
\\
\rvs_0&=& x_0
\end{array}
\eeq

\item {\bf incrementer nodes}

$$
\xymatrix{
\rvB_3\ar[dr] 
&& \rvB_2\ar[dl]\ar[dr]
&& \rvB_1\ar[dl]\ar[dr]
&& \rvB_0\ar[dl]
\\
&\Delta^3_2\rvB 
&&\Delta^2_1 \rvB 
&&\Delta^1_0 \rvB
&
}$$

\beq\color{blue}
\begin{array}{lll}
\Delta_2^3\rvB &=& \rvB_3 - \rvB_2
\\
\Delta_1^2\rvB &=& \rvB_2 - \rvB_1
\\
\Delta_0^1\rvB &=& \rvB_1 - \rvB_0
\end{array}
\eeq

\item {\bf de-incrementer nodes}
$$
\xymatrix{
&\Delta^3_2\rvx_2 \ar[dl] 
&&\Delta^2_1 \rvx \ar[dlll]\ar[dl]
&&\Delta \rvx^1_0 \ar[dlllll]\ar[dlll]\ar[dl]
&
\\
\rvx_3 
&& \rvx_2 
&& \rvx_1
&& \rvx_0
\ar@/^1pc/[ll]
\ar@/^1pc/[llll]
\ar@/^1pc/[llllll]
}
$$
\beq
\color{blue}
\begin{array}{lll}
\rvx_3 &=& \Delta_2^3\rvx + \Delta_1^2\rvx
+ \Delta^1_0\rvx + x_0
\\
\rvx_2 &=& \Delta_1^2\rvx
+ \Delta^1_0\rvx + x_0
\\
\rvx_1 &=&
\Delta^1_0\rvx + x_0
\\
\rvx_0&=&x_0
\end{array}
\eeq
\end{itemize}

\hrule \noindent{\bf SED bnet}

$\rvx, \rvB\in \RR$
\beq
d\rvx= f(\rvx, t)dt + L(\rvx, t)d\rvB(t)
\eeq

\begin{figure}
$$
\xymatrix{
\rvB_3\ar@[red][dr] 
&& \rvB_2\ar@[red][dl]\ar@[red][dr]
&& \rvB_1\ar@[red][dl]\ar@[red][dr]
&& \rvB_0\ar@[red][dl]
\\
&\Delta^3_2\rvB \ar[d]
&&\Delta^2_1 \rvB \ar[d]
&&\Delta^1_0 \rvB \ar[d]
&
\\
&\Delta^3_2\rvx \ar@[green][dl] 
&&\Delta^2_1 \rvx \ar@[green][dlll]\ar@[green][dl]
&&\Delta^1_0\rvx \ar@[green][dlllll]\ar@[green][dlll]\ar@[green][dl]
&
\\
\rvx_3 
&& \rvx_2 \ar[lu] 
&& \rvx_1 \ar[lu]
&& \rvx_0\ar[lu]
\ar@[green]@[green]@/^1pc/[ll]
\ar@[green]@/^1pc/[llll]
\ar@[green]@/^1pc/[llllll]
}
$$
\caption{Bnet for general SDE with $N=4$ number of times. Note that this bnet
contains within it first an incrementer bnet
(in red) for the $\rvB_i$,
and
then a de-incrementer bnet (in green)
for the $\rvx_i$.}
\label{fig-sde-3-nodes}
\end{figure}


\beq
\color{blue}
\Delta_{2}^{3}\rvB = 
\rvB_3 -\rvB_2
\eeq

\beq
\color{blue}
\Delta_{2}^{3}\rvx = 
f(x_3, t)\Delta_2^3 t +
L(x_3, t)\Delta_{2}^{3}\rvB
\eeq

\beq\color{blue}
\rvx_3 = \Delta_{2}^{3}\rvx
+\Delta_{1}^{2}\rvx
+\Delta_{0}^{1}\rvx
+x_0
\eeq

\section{Simple properties of SDE}

\subsection {Transition Matrices}

For $t,s\geq 0$ and $x, y\in \RR^n$,

$P(\XT{y}{t}|\XT{x}{s})$

$\av{y|P_{t,s}|x}$
where
\beq
\int dx\;\ket{x}\bra{x}
=1,\quad 
\av{y|x} = \delta(y-x)
\eeq

\subsection{Markov chain}

For $s<t$, 
\beq
P(x(t)| x([0, s]))=
P(x(t)| x(s))
\eeq
\subsection{Chapman-Kolgomorov Equation}

\begin{claim}
(Chapman-Kolgomorov equation)

Let $x(t_k) = x_k$

\beq
P(x_3|x_1) =\int dx^n_2\; P(x_3|x_2)P(x_2|x_1)
\eeq
\end{claim}
\proof

\beqa
P(x_3, x_2|x_1) &=&
P(x_3|x_2, x_1)P(x_2|x_1)
\\
&=&
P(x_3|x_2)P(x_2|x_1)
\eeqa
Now integrate both sides over $x_2$.
\qed


\subsection{Martingale}

For $t, t_i < T$

\beq
E[\;|\rvy(t_i)|\;] < \infty\quad \forall i\quad
(E[\;|\rvy(t)|\;] < \infty \quad \forall t\geq 0) 
\eeq 


\beq
E\left[\rvy(t)|x(t_{[i\upto j]})\right]= y(t_j)
\quad (E\left[\rvy(t)|x([0,s])\right]= y(s))
\eeq


Brownian motion is a martingale.
\beq
E[\rvB(t)| B(t_{[i\upto j]}] = B(t_j)
\eeq


It\^{o} integrals $\int L\XT{x}{t}d\rvB_t$
are martingales too, but Stratonovich integrals
$\int L\XT{x}{t}\circ d\rvB_t$
aren't. 



\section{It\^{o} Integral}

Consider 1 dimensional case $\rvx, \rvW\in \RR$.
\beq
\frac{d\rvx}{dt}= f(\rvx, t) + L(\rvx, t)\rvW(t)
\eeq

\beq
\rvx(t) - \rvx(0) =
\int_{0}^{t}dt\; f\XT{x}{t} + J
\eeq

\beq
J = \int_{0}^t dt\;
L\XT{x}{t}\rvW(t)
\eeq

$t_0=0 < t_1 <\ldots <t_{N-1}$, $t_k^*\in [t_k, t_{k+1})$



\beq 
J_N = 
\sum_k L(\rvx, t_k^*)\Delta_{t_k}^{t_{k+1}}\rvB
\eeq

\beq 
J = \lim_{N\rarrow \infty} J_N
\eeq



Suppose $L=\rvx=\rvB$

\beq
J_N=\sum_k \rvB(t_k^*) \Delta_{t_k}^{t_{k+1}}\rvB
\eeq

\begin{enumerate}

\hrule
\item $t_k^* = t_{k+1}$ 
\beqa
E[J_N] &=&
\sum_k E[\rvB(t_{k+1})\Delta_{t_k}^{t_{k+1}}\rvB] 
\\
&=&
\sum_k
E[(\Delta_{0}^{t_{k+1}}\rvB)\Delta_{t_k}^{t_{k+1}}\rvB] 
\\
&=&
\sum_k
E[(\Delta_{t_k}^{t_{k+1}}\rvB)^2]
\\
&=&t
\eeqa

\hrule
\item $t_k^* = \frac{t_k + t_{k+1}}{2}$
Stratonovich integral

\hrule
\item $t_k^* = t_k$, Ito integral\footnote{More correctly,
It\^{o}}


\beqa
E[J_N] &=&\sum_k E[\rvB(t_k)\Delta_{t_k}^{t_{k+1}}\rvB] 
\\
&=& \sum_k
E[(\Delta_{0}^{t_k}\rvB)\Delta_{t_k}^{t_{k+1}}\rvB] 
\\
&=&0
\eeqa

\beqa
J_N &=& 
\sum_k \rvB(t_k)\Delta_{t_k}^{t_{k+1}}\rvB
\\
&=&
\frac{1}{2}\sum_k\left[
-[\Delta_{t_k}^{t_{k+1}}\rvB]^2
+ \Delta_{t_k}^{t_{k+1}}(\rvB^2)\right]
\eeqa

\beqa
J &=& \lim_{N\rarrow \infty} J_N
\\
&=&
\frac{1}{2}
(-t+\rvB^2(t))
\eeqa
$E[J] = \frac{1}{2}(-t+t)=0$






\end{enumerate}

\beq
d[\rvB^2(t)]=
2\rvB(t)d\rvB(t)
+t
\eeq


\beq
[d\rvB(t)]^2 = dt
\eeq



For $\rvB\in\RR^n$,

\beq
\boxed{
d[\rvB_\alpha(t)\rvB_\beta(t)]=\delta(\alpha, \beta)
\left[
2\rvB_\alpha(t)d\rvB_\alpha(t)
+nt\right]}
\eeq

\beq
\boxed{
d[\rvB_\alpha(t)]d[\rvB_\beta(t)]
=Q_{\alpha, \beta}dt}
\eeq






\section{Fokker-Planck Equation}


Consider $n$ dimensional case $\rvx, \rvW\in \RR^n$. Let $\mu, \nu,\alpha, \beta\in [n]$.
\beq
d\rvx_\mu= f(\rvx, t)dt + L(\rvx, t)d\rvB_\mu(t)
\eeq

\beq
R_{\mu, \nu} = \frac{1}{2} L_{\mu,\alpha}Q_{\alpha,\beta}L^T_{\beta,\nu}
\eeq


\begin{claim}
\beq
d\phi=
\left[\pder{\phi}{t} +
 f_\mu\pder{\phi}{x_\mu} + 
 R_{\mu, \nu}
\frac{\partial^2\phi}{\partial x_\mu\partial x_\nu}\right]dt
+
\pder{\phi}{x_\mu}L_{\mu,\nu}d\rvB_\nu
\eeq
\end{claim}
\proof






\beq
d\phi =
\pder{\phi}{t}dt
+
\sum_\mu \pder{\phi}{x_\mu} d\rvx_\mu
+\frac{1}{2}
\sum_\mu \sum_\nu \frac{\partial^2\phi}{\partial x_\mu\partial x_\nu}d\rvx_\mu d\rvx_\nu
\eeq


\beq
\pder{\phi}{x_\mu} d\rvx_\mu
=
\pder{\phi}{x_\mu} 
\left[
f_\mu dt + L_{\mu, \nu}d\rvB_\nu
\right]
\eeq

\beqa
\frac{\partial^2\phi}{\partial x_\mu\partial x_\nu}d\rvx_\mu d\rvx_\nu
&=&\frac{\partial^2\phi}{\partial x_\mu\partial x_\nu}
\left[
f_\mu dt + L_{\mu, \alpha}d\rvB_\alpha
\right]
\left[
f_\nu dt + L_{\nu, \beta}d\rvB_\beta
\right]
\\
&=&
\frac{\partial^2\phi}{\partial x_\mu\partial x_\nu}
L_{\mu, \alpha}  L_{\nu, \beta} d\rvB_\alpha d\rvB_\beta
\\
&=&
\frac{\partial^2\phi}{\partial x_\mu\partial x_\nu}
L_{\mu, \alpha}  L_{\nu, \beta} Q_{\alpha,\beta} dt
\eeqa

\beqa
d\phi &=&
\pder{\phi}{t}dt
+
\pder{\phi}{x_\mu} [f_\mu dt + L_{\mu,\nu}d\rvB_\nu]
+ R_{\mu, \nu}
\frac{\partial^2\phi}{\partial x_\mu\partial x_\nu}dt
\\
&=&
\left[\pder{\phi}{t} +
f_\mu \pder{\phi}{x_\mu} + R_{\mu, \nu}
\frac{\partial^2\phi}{\partial x_\mu\partial x_\nu}\right]dt
+
\pder{\phi}{x_\mu}L_{\mu,\nu}d\rvB_\nu
\eeqa

\qed

$\rvx(0)=x_0$

Example,  $n=1$, $\rvx=\rvB$, 
$\phi = \rvB^K$

$L=Q=1$, $f=0$

\beq
d(\rvB^m) = \left[m \rvB^{m-1}+ m(m-1) \rvB^{m-2}\right]dt
+ m\rvB^{m-1}d\rvB
\eeq

{\bf Fokker-Planck equation} (FP equation) (a.k.a. {\bf Fokker-Planck-Kolgomorov equation} )
for probability $P(x,t)$
of single event $(x,t)$

\begin{claim}
	\beq
	\pder{P}{t}= \calf_\rvx P
	\eeq
	is solved formally by
	
	\beq
	P(x,t)=e^{(t-t_0)\calf_\rvx}P(x, t_0)
	\eeq
%	where
%	
%		\beq
%	\calf_\rvx = 
%	\lim_{s\rarrow 0+}
%	\frac{E[\phi(\XT{x}{t+s})] - \phi(\XT{x}{t})}{s}
%	\eeq
	\end{claim}

\begin{claim} 
(Forward FP equation)

Assume
\beq
dx = f\XT{x}{t}dt + L\XT{x}{t}d\rvB
\eeq
Then

\beq
	\pder{P\XT{x}{t}}{t}= \calf_\rvx P\XT{x}{t}
	\eeq
with

\beq
\calf_\rvx \bullet=
-\;\pder{}{x_\mu}
(\bullet f_\mu) + 
\frac{\partial^2}{ \partial x_\mu\partial x_\nu}(\bullet R_{\mu, \nu})
\eeq
\end{claim}
\proof

\beq
\int dx^n\; P\XT{x}{t}\left[ \pder{\phi}{x_\mu}L_{\mu,\nu}d\rvB_\nu
\right]
=0
\eeq

Integration by parts

\beq
udv = d(uv)-(du)v 
\eeq

\beq
\int_{-\infty}^{+\infty}udv=
\underbrace{uv|_{-\infty}^{+\infty}}_{0}
\quad
-
\int_{-\infty}^{+\infty}(du)v 
\eeq



\beq
\int dt dx^n\; P\XT{x}{t}\frac{d\phi}{dt}=
\int dt dx^n\; P\XT{x}{t}
\left[\pder{\phi}{t} +
 f_\mu\pder{\phi}{x_\mu} + R_{\mu, \nu}
\frac{\partial^2\phi}{\partial x_\mu\partial x_\nu}\right]
\eeq



\beq
-\int dt dx^n\; \phi\frac{dP}{dt}=
\int dt dx^n\;  \phi
\left[-\;\pder{P}{t} -
\pder{(Pf_\mu)}{x_\mu} + 
\frac{\partial^2 }{\partial x_\mu\partial x_\nu}(PR_{\mu, \nu})\right]
\eeq


\qed

Note that 
\beq
\pder{P}{t} = -\pder{J_\mu}{x_\mu}
\label{eq-cons-prob}
\eeq

\beq
J_\mu = Pf_\mu - \pder{(PR_{\mu,\nu})}{x_\nu}
\eeq

Eq.(\ref{eq-cons-prob})
is the {\bf equation for conservation of probability}\footnote{It implies conservation of probability because
$$0=\pder{}{t}\int_V dV\;P=
\int_V dV\;\nabla\cdot \vec{J}= 
\int \vec{J}\cdot d\vec{S}$$
}. $J_\mu$ is called the {\bf probability
flux}, $f_\mu$ is called the {\bf drift},
and $R_{\mu, \nu}$ is called the
{\bf diffusion matrix} (or {\bf diffusion coefficient} if it's a scalar)

Diffusion equation, $n=1$, $f=0$, $L=1$, $R=D>0$

\beq
dx = d\rvB
\eeq

\beq
\pder{P}{t} = D
\frac{\partial^2P}{\partial^2x}
\eeq

Overdamped Langevin Equation\footnote{
This equation, also known as Brownian dynamics (see
Ref.\cite{wiki-brownian-dyn}),  arises from
Newton's equation $m\ddot{x}= -\lam\dot{x}  -U'(x)$ when the acceleration $\ddot{x}$ is negligible, so the drag force and potential force cancel each other.} 

\beq
dx_\mu = -\;\frac{1}{2}\pder{U}{x_\mu}dt + d\rvB
\eeq

\begin{claim}
For the overdamped Langevin equation,
if $Q =\frac{1}{\lam}>0$, then the steady state 
solution is

\beq
P(x) = \frac{e^{-\lam U(x)}}{Z}
\eeq
where 
\beq
Z = \int dx^n\;e^{-\lam U(x)}
 \eeq

\end{claim}
\proof
$L=1$, $\pder{P}{t}=0$
\beq
0= -
\pder{(Pf_\mu)}{x_\mu} + \frac{1}{2\lam}
\frac{\partial^2 P }{\partial x_\mu^2}
\eeq

$f_\mu =-\; \frac{1}{2}\pder{U}{x_\mu}$


\beq
0= \frac{1}{2}
\pder{}{x_\mu}\left(P\pder{U}{x_\mu} + \frac{1}{\lam}
\pder{P }{x_\mu}
\right)
\eeq


\beq
0=
\pder{}{x_\mu}\left(\lam e^{-\lam U}\pder{U}{x_\mu}+
\pder{e^{-\lam U}}{x_\mu}
\right) 
\eeq

\qed

Recall

\beq
\calf_\rvx \bullet=
-\;\pder{}{x_\mu}
(\bullet f_\mu) + 
\frac{\partial^2}{ \partial x_\nu\partial x_\mu}(\bullet R_{\mu, \nu})
\eeq

Define $\calb_\rvx$ to be the same as $\calf_\rvx$
but with every derivative $A\pder{B}{x_\mu}$
replaced by $-B\pder{A}{x_\mu}$. Hence\footnote{For those
	who know Quantum Mechanics, our $\calf$ equals 
	a Hamiltonian $H$ times $i$, $\calf =Hi$.
}

\beq
\calb_\rvx \bullet=
f_\mu\pder{\bullet}{x_\mu}
 + R_{\mu, \nu}
\frac{\partial^2\bullet}{\partial x_\mu\partial x_\nu}
\eeq



\begin{claim} Forward FP equation for transition matrix $ P(\XT{x}{t}|\XT{w}{s})$
	
\beq
\pder{P(\XT{x}{t}|\XT{w}{s})}{t}=
\calf_\rvx P(\XT{x}{t}|\XT{w}{s})
\eeq
with
\beq
P(\XT{x}{s}|\XT{w}{s})=\delta(x-w)
\eeq

In case you know Dirac notation, this claim can be stated in Dirac notation as

\beq
\pder{\av{x|P_{t,s}|w}}{t}
=\int 
\av{x|\calf_{t}|x'}dx'\av{x'|P_{t,s}|w}
\eeq
with

\beq
\av{x|P_{s,s}|w} =\delta(x-w)
\quad \text{(Hence, $P_{s,s}=1$)}
\eeq
\end{claim}


	
	
\begin{claim} Backwards FP equation for transition matrix $P(\XT{y}{u}|\XT{x}{t})$
\beq
-\;\pder{P(\XT{y}{u}|\XT{x}{t})}{t}=
\calb_\rvx P(\XT{y}{u}|\XT{x}{t})
\eeq
with

\beq
P(\XT{y}{t}|\XT{x}{t}) =\delta(y-x)
\eeq

In case you know Dirac notation, this claim can be stated in Dirac notation as

\beq
-\; \pder{\av{y|P_{u,t}|x}}{t}
=\int 
\av{y|\calb_t|y'}dy'\av{y'|P_{u,t}|x}
\eeq
with
\beq
\av{y|P_{t,t}|x}=\delta(y-x)
\quad \text{(Hence, $P_{t,t}=1$)}
\eeq
	
\end{claim}

The forward FP equation is the
time reversed version of the 
backward FP equation. Thus, they describe 
the same stochastic process,
in opposite time directions.

Note that if we view
a transition matrix 
$P(\XT{y}{u}|\XT{x}{t})$  as a matrix whose rows and columns are labeled by 
all possible events $(\XT{x}{t})$, then the forward FP equation (resp., backward FP equation)
is a differential equation
constraining all the rows $(\XT{y}{u})$ for a fixed column
$(\XT{x}{t})$
(resp., all the columns for a fixed row).
They both constrain
the same matrix $P(\XT{y}{u}|\XT{x}{t})$,
but in 2 different ways.

	






\section{First and second order statistics}

\beq
m_\mu(t) = E[\rvx_\mu(t)]=
\av{\rvx_\mu(t)}
\eeq

\beq
C_{\mu, \nu}(t,s) = \av{\rvx_\mu(t), \rvx_\nu(s)}
\eeq

\beq
V_{\mu, \nu}(t) = C_{\mu, \nu}(t,t)
\eeq


\beq
dx_\mu = \left[a_\mu(t) + F_{\mu, \nu}(t)x_\nu \right] dt + 
L_{\mu, \nu}(t)d\rvB_\nu
\eeq

\beq
R_{\mu, \nu} = 
\frac{1}{2}L_{\mu, \alp}Q_{\alp, \beta}L^T_{\beta, \nu} =\frac{1}{2} (LQL^T)_{\mu, \nu}
\eeq

\begin{claim}
\label{cl-mt-Vt}
\beq
\frac{dm}{dt}=
a + Fm
\eeq

\beq
\frac{dV}{dt}=
VF^T +FV^T+ R
\eeq
\end{claim}
\proof

\beqa
\frac{dm}{dt}&=&
\av{a + F\rvx}
\\\
&=&
a + Fm
\eeqa

Let $\partial_\alp =\pder{}{x_\alp}$
\beqa
d\av{\rvx_\mu, \rvx_\nu}
&=&
dx_\alp\partial_\alp\av{\rvx_\mu, \rvx_\nu}
+
dx_\alp dx_\beta\frac{1}{2}\partial_\alp\partial_\beta\av{\rvx_\mu, \rvx_\nu}
\\
&=&
\av{\rvx_\mu, 
d\rvx_\nu}
+
\av{d\rvx_\mu, 
\rvx_\nu}
+ \av{d\rvx_\mu, d\rvx_\nu}
\\
&=&
\av{\rvx_\mu, 
F_{\nu,\alp}\rvx_\alp dt}
+
\av{F_{\mu,\alp}\rvx_\alp dt, 
\rvx_\nu}
+ 
R_{\mu, \nu}dt
\\
&=&
(VF^T+FV^T+R)dt
\eeqa
\qed

Propagator
\beq
\pder{\Psi(\tau, t)}{\tau}  = F(\tau) \Psi(\tau, t)
\eeq

satisfies

\beq
\begin{array}{l}
\Psi(a, c) = \Psi(a, b)\Psi(b, c)
\\
\Psi^{-1}(a,b) = \Psi(b,a)
\\
\Psi(a,a)=1
\end{array}
\eeq



\begin{claim}
\beq
m(t) = \Psi(t, t_0)m(t_0) +\int_{t_0}^{t}d\tau\;
\Psi(t, \tau)a(t)
\label{eq-mt-psi}
\eeq

\beq
V(t)=\av{\rvx(t), \rvx^T(t)}=\Psi(t, t_0)V(t_0)\Psi^T(t, t_0)
+
\int_{t_0}^t d\tau\; \Psi(t, \tau)R(\tau)
\Psi^T(t, \tau)
\label{eq-Vt-psi}
\eeq


\beq
\av{\rvx(t), \rvx^T(s)} = 
\left\{
\begin{array}{ll}
V(t)\Psi^T(s,t) &\text{if } t<s
\\
\Psi(t,s)V(s) & \text{if }t\geq s
\end{array}
\right.
\label{eq-sde-cov-s-t}
\eeq
\end{claim}
\proof
Eqs.(\ref{eq-mt-psi} and (\ref{eq-Vt-psi}))
can be proven simply by taking the time derivative of both sides.
This yields the differential
equations for $m(t)$ and $V(t)$ that were established in Claim \ref{cl-mt-Vt}
We won't prove Eq.\ref{eq-sde-cov-s-t} here.
For a proof,  see
Ref.\cite{sde-applied-book}.
\qed


If $P(x,t=0)$ is a Gaussian, $P(x,t)$ must be Gaussian too,
because the transformation is linear. Therefore,

\beq
P(x,t) = P(x(t))=
\caln(x(t); \mu=m(t), \Sigma^2=V(t))
\eeq

\beq
P(x(t)|x(s)) =\caln(x(t); \mu=m(t|s), \Sigma^2=V(t|s))
\eeq

where

\beq
m(t|s) =
\Psi(t,s)x(s)
+\int_{t_0}^t d\tau \Psi(t, \tau)a(\tau)
\eeq

\beq
V(t|s)=\int_s^td\tau\;
\Psi(t, \tau)
R(\tau)\Psi^T(t, \tau)
\eeq

\beq 
\Psi_k = \Psi(t_{k+1}, t_k)
\eeq

\beq
a_k = \int_{t_k}^{t_{k+1}}
d\tau\; \Psi(t_{k+1}, \tau)a(\tau)
\eeq

\beq
\Sigma_k=
\int_{t_k}^{t_{k+1}}d\tau\;
\Psi(t_{k+1}, \tau)R(\tau)\Psi^T(t_{k+1}, \tau)
\eeq


\beq
P(\eps_k) = \caln(\eps_k; \mu=0, \Sigma=\Sigma_k)
\eeq

$x(t_k)=x_k$



\begin{figure}[h!]
$$
\xymatrix{
\ul{\eps}_3 \ar[d]
& \ul{\eps}_2 \ar[d]
& \ul{\eps}_1 \ar[d]
& \ul{\eps}_0\ar[d]
\\
\rvx_3 
& \rvx _2 \ar[l]
& \rvx_1 \ar[l]
& \rvx_0\ar[l]
}
$$
\caption{xx}
\label{fig-ist-2nd-order-bnet}
\end{figure}

\beq\color{blue}
\rvx_{k+1}= \Psi_k\rvx_k + a_k +\ul{\eps}_k
\eeq




\section{Fourier Transform of stochastic process}

\newcommand{\Tintegral}[1]{
\int_{-\;\frac{T}{#1}}^{\frac{T}{#1}}}


recall

\beq
\delta(t) =\frac{1}{2\pi} \int_{-\infty}^{\infty}d\omega \;e^{i\omega t}
\eeq

$x(t)\in\RR^n$
\beq
\TIL{x}(\omega)=\calf[x(t)](\omega)=
\int_{-\infty}^{\infty}dt\; x(t) e^{-i\omega t}
\eeq
\beq
x(t)=\calf^{-1}[x(\omega)](t)=
\frac{1}{2\pi}
\int_{-\infty}^{\infty}d\omega\; \TIL{x}(\omega) e^{i\omega t}
\eeq

\beq
x_T(\omega)=\Tintegral{2}
dt\;x(t) e^{-i\omega t}
\eeq



{\bf power spectral density}
\beq
S_\rvx(\omega)= \lim_{T\rarrow 0}
\frac{1}{T} E[\TIL{x}_T(\omega)\TIL{x}^\dagger_T(\omega)]
\eeq



\beq 
S_{\rvW}(\omega) = Q
\eeq


{\bf autocorrelation function for stationary process}
\beq
AC_\rvx(\tau)=
E[x(t)x^T(t+\tau)]
\eeq


\begin{claim}

\beq 
\Tintegral{2}dt
\Tintegral{2}ds\;
g(t-s)=
\int_{-T}^{T}d\tau\;g(\tau)(T-|\tau|)
\eeq
\end{claim}
\proof

Let 

\beq 
\tau=\frac{t-s}{\sqrt{2}},\quad
\sigma=\frac{t+s}{\sqrt{2}}
\eeq

The absolute value of the Jacobian $|\frac{\partial(s, t)}{\partial(\tau, \sigma)}|$ equals 1.
Hence,

\begin{align}
\Tintegral{2}dt
\Tintegral{2}ds\;
g(t-s)
&=
\Tintegral{\sqrt{2}}d\tau
\int_{-\;\frac{T}{\sqrt{2}}-|\tau|}
^{\frac{T}{\sqrt{2}}+|\tau|}d\sigma \;g(\sqrt{2}\tau)
\quad(\text{See Fig. \ref{fig-integral-region}})
\\
&=
\Tintegral{\sqrt{2}}d\tau
\;g(\sqrt{2}\tau)
(\sqrt{2}T - 2|\tau|)
\\
&=
\int_{-T}
^{T}d\tau'
\;g(\tau')
(T - |\tau'|)\quad(\tau' = \sqrt{2}\tau)
\end{align}

\begin{figure}[h!]
\centering
\includegraphics[width=1.5in]
{stochastic-diff-eqns/integral.png}
\caption{Integral}
\label{fig-integral-region}
\end{figure}


\qed

\begin{claim}(Wiener Khinchin theorem)

\beq
S_\rvx(\omega)
=
\int_{-\infty}^{\infty}d\tau\; AC_\rvx(\tau)e^{-i\omega \tau}
\eeq
If $AC(\tau)=AC(-\tau)$,

\beq
S_\rvx(\omega)
=
2\int_{0}^{\infty}d\tau\; AC_\rvx(\tau)\cos(\omega\tau)
\eeq

\end{claim}
\proof

\beqa
\frac{1}{T}
E[|\TIL{x}_T(\omega)|^2]
&=&\frac{1}{T}
\Tintegral{2}ds
\Tintegral{2}dt\; 
E[x(t)x^T(s)]e^{-iw(t-s)}
\\
&=&\frac{1}{T}
\int_{-T}^{T}d\tau\;
AC(\tau)e^{-i\omega \tau}(T-|\tau|)
\\
&\rarrow&
\int_{-\infty}^{\infty}d\tau\; AC(\tau)e^{-i\omega \tau}
\eeqa
\qed


\beq
AC_{\rvW}(\tau) = Q\delta(\tau)
\eeq

\beq
\frac{d x}{dt}=Fx + L\rvW
\eeq

\beq
-i\omega \TIL{x} = F\TIL{x} + L\TIL{\rvW}
\eeq

\beq
\TIL{x}= -( F+i\omega)^{-1}L\TIL{\rvW}
\eeq

\beqa
S_{\rvW}(\omega) &=& \TIL{x}\TIL{x}^\dagger
\\
&=&
(F+i\omega)^{-1}\underbrace{LQL^\dagger}_(2R)(F^T-i\omega)^{-1}
\eeqa

\beq
AC_{\rvx}(\tau)= \frac{1}{2\pi}
\int_{-\infty}^{\infty}d\omega\; e^{-i\omega \tau}
(F+i\omega)^{-1}(2R)
(F^T-i\omega)^{-1}
\eeq



\section{Lamperti Transform}

\beq
dx_\mu = f_\mu(x,t)dt + L_{\mu,\nu}(x,t)d\rvB_\nu
\eeq

\beq
dy_\mu = g_\mu(y,t)dt + d\rvB_\mu
\label{eq-g-def-grisa}
\eeq


\begin{claim} For $n=1$, the function 
$g(y,t)$ in Eq.(\ref{eq-g-def-grisa}) 
is given by

\beq
g(y,t)=
\left.\left(
\pder{}{t}
\int_\xi^x \frac{du}{L(u,t)}
+
\frac{f\XT{x}{t}}{L\XT{x}{t}}
-\;
\frac{1}{2}
\pder{L\XT{x}{t}}{x}
\right)\right|_{x\rarrow h^{-1}(y,t)}
\eeq
\end{claim}
\proof

\beq
y=h=\int_\xi^x\frac{du}{L(u,t)}
\eeq

\beqa
dy &=& \pder{h}{t}dt + \frac{dx}{L}
-\;\frac{1}{2L^2}
\pder{L}{x}(dx)^2
\\
&=&
 \pder{h}{t}dt + \frac{fdt +  Ld\rvB}{L}
-\;\frac{1}{2L^2}
\pder{L}{x}\underbrace{(Ld\rvB)^2}_{L^2dt}
\\
&=&
\left(
\pder{h}{t} + \frac{f}{L} -\frac{1}{2}\pder{L}{x}
\right)dt + d\rvB
\eeqa





\section{Feynman-Kac Path Integrals}

$x(t_k)=x_k$

\beq
x_k = x_{k-1}
+f_k\Delta t +\Delta\rvB_k
\eeq

\beq
\Delta\rvB_k \sim \caln(\mu=0, \s^2=q\Delta t)
\eeq


\begin{align}
P(x_{[1\upto N]})&=
\prod_{k=1}^N P(x_k|x_{k-1})
\\
&=
\prod_{k=1}^N
\left[
\frac{1}{\sqrt{2\pi q\Delta t}}
\exp\left(
-\;
\frac{(x_k-x_{k-1})^2}{2q\Delta t}
\right)
\right]
\\
&=
\prod_{k=1}^N
\left[
\frac{1}{\sqrt{2\pi q \Delta t}}
\exp\left(
-\;
\frac{(f_k\Delta t + \Delta B_k)^2}{2q\Delta t}
\right)
\right]
\\
&=
\underbrace{\prod_{k=1}^N
\left[
\frac{1}{\sqrt{2\pi q \Delta t}}
\right]}_{\gamma^N}
\exp\left(
-\;
\frac{1}
{2q}\int_0^{t_N}dt
\left[(\ddot{B})^2 +
2f\dot{B} +f^2\right]
\right)
\end{align}



\beq
P(B_{[0 \upto N]})
=
\gamma^N\exp\left(-\;
\frac{1}{2 q}\int_0^{t_N} dt (\ddot{B})^2
\right)
\eeq

\beq
\frac{P(x_{[1\upto N]})}{P(B_{[1\upto N]})}=
\exp\left(-\;
\frac{1}{2q}\int_0^{t_N}dt
\left[
2f\dot{B} +f^2\right]
\right)
\label{eq-px-div-pb}
\eeq

\beq
\cald B = \gamma^N \prod_{k=1}^N dB_k
\eeq
$\RR^N=
\text{set of all paths (instances) $x_{[1\upto N]}$ of random variable
$\rvx_{[1\upto N]}$ }
$


\beq
P(x_{N+1}|x_0=0)  =
\int_{\RR^{N}}\cald B\;
\exp\left(
\frac{1}
{2q}\int_0^{t_N}dt
\left[(\ddot{B})^2 +
2f\dot{B} -f^2\right]
\right)
\eeq

\section{Karhunen–Loève series}

\beq
\int_0^{T}dt\;\ket{t}{\bra t} = 1,
\;
\av{t|t'}=\delta(t-t')
\eeq

\beq
C(t, t')= \av{t|C|t'}
\eeq


For $n=1,2,3,\ldots$, 
\beq
C\ket{\phi_n}=\lam_n\ket{\phi_n}
\eeq

\beq
\sum_{n=1}^\infty \ket{\phi_n}\bra{\phi_n}=1,
\;
\av{\phi_n|\phi_{n'}}=
\delta(n,n')
\eeq



\beq
C = \sum_{n=1}^\infty \ket{\phi_n}\lam_n\bra{\phi_n}
\eeq



\beq
\phi_n(t) = \av{t|\phi_n},
\;
\phi_n^*(t) = \av{\phi_n|t}
\eeq

\beq
\av{t|C|t'} = \sum_{n=1}^\infty \phi_n(t)\lam_n\phi_n^*(t')
\eeq


For Brownian motion,

\beq
\lam_n  =\left(\frac{2T}{(2n-1)\pi}
\right)^2
\eeq

\beq
\phi_n(t)=
\sqrt{\frac{2}{T}}
\sin \left(
\frac{t}{\sqrt{\lam_n}}\right)
\eeq


Karhunen-Loeve expansion

\beq
\rvB(t) =\sum_n z_n\av{t|\phi_n}
\eeq

where

\beq
z_n\sim \caln(\mu=0, \s^2 =\lam_n)
\eeq


\section{Girsamov Theorem}
\beq
dx = f(x,t)dt + d\rvB
\label{eq-girsa-f}
\eeq

\beq
dy = g(y,t)dt + d\rvbeta
\label{eq-girsa-g}
\eeq

\begin{claim}(Girsanov theorem, part 1)

If $dx=dy$,

\beq
d\rvbeta =
(f-g)dt + d\rvB
\eeq

\end{claim}
\proof
Just subtract
Eq.(\ref{eq-girsa-f}) 
from
Eq.(\ref{eq-girsa-g}) .
\qed

\begin{claim}(Girsanov theorem, part 2)

\beq
\frac{P(x_{[1\upto N]})}{P(y_{[1\upto N]})}=
\exp\left(-\;
\frac{1}{2q}\int_0^{t_N}dt\;
(f-g)^2 +
\frac{1}{2q}\int 2(f-g)d{\rvB})
\right)
\eeq
\end{claim}
\proof

From Eq.(\ref{eq-px-div-pb})
\beq
\frac{P(x_{[1\upto N]})}{P(y_{[1\upto N]})}=
\exp\left(-\;
\frac{1}{2q}\int_0^{t_N}dt
\underbrace{
\left[
2f\dot{B} + f^2
-2g\dot{\beta} -g^2
\right]}_{\cala}
\right)
\eeq

\beqa
\cala &=&2f\dot{\rvB}
 -2g[\dot{\rvB}+f-g]
+ f^2-g^2
\\
&=&
(f-g)2\dot{\rvB}
+
(f-g)(-2g) + (f-g)(f +g)
\\
&=&
(f-g)[2\dot{\rvB}
-2g + f +g]
\\
&=&
(f-g)^2 + 2\dot{\rvB}(f-g))
\eeqa
\qed

\begin{claim}(Girsanov theorem, part 3)

If 
\beq
Z=
\frac{P(x_{[1\upto N]})}
{P(y_{[1\upto N]})}
\eeq
then

\beq
E[h(x_{[1\upto N]})]=
E[Zh(y_{[1\upto N]})]
\eeq
\end{claim}
\beqa
E[h(\rvx_{[1\upto N]})]
&=&
\int \underbrace{dx_{[1\upto N]}}_
{dy_{[1\upto N]}} P(x_{[1\upto N]}) 
\underbrace{h(x_{[1\upto N]})}_
{h(y_{[1\upto N]})}
\\
&=&
\int dy_{[1\upto N]} 
Z
P(y_{[1\upto N]})
h(y_{[1\upto N]})
\\
&=&
E[Zh(y_{[1\upto N]})]
\eeqa

\section{Doob's Transform}

\beq
D(.|\XT{x}{t})
=
\int dy\; D(.|\XT{y}{t+s})P(\XT{y}{t+s} |\XT{x}{t})
\eeq

\beq
P^D(\XT{y}{t+s}|\XT{x}{t})=\frac{D(.|\XT{y}{t+s})P(\XT{y}{t+s}|\XT{x}{t})}
{D(.|\XT{x}{t})}
\eeq

\beq 
\int dy\; P^D(\XT{y}{t+s}|\XT{x}{t})=1
\eeq






Recall that

\beq
\calf_\rvx \bullet=
-\;\pder{}{x_\mu}
(\bullet f_\mu) + 
\frac{\partial^2}{ \partial x_\mu\partial x_\nu}(\bullet R_{\mu, \nu})
\eeq
and

\beq
\calb_\rvx \bullet=
f_\mu\pder{\bullet}{x_\mu}
 + R_{\mu, \nu}
\frac{\partial^2\bullet}{\partial x_\mu\partial x_\nu}
\eeq

If
\beq
\left[\pder{}{s} + \calb_\rvy\right] \phi\XT{y}{s}=0,
\eeq
then that implies we can define a process $y(s)$
such that

\beq
dy_\mu = f_\mu(\XT{y}{s})ds + L_{\mu,\nu}(\XT{y}{s})d\rvB_\nu
\eeq
with 

\beq 
P(\XT{z}{t}|\XT{y}{s}) =\phi\XT{y}{s}
\eeq


\begin{claim} (Doob's Transform)
 \label{cl-doobs-transform}
 
Let $\calf_\rvy^D$ (resp.,   
$\calb_\rvy^{D}$) be the 
	same as $\calf_\rvy$ (resp., 
	$\calb_\rvy$), but with $f_\mu(\XT{y}{t})$
	replaced by $f^D_\mu(\XT{y}{t})$
		given by
		\beq
		f^D_\mu(\XT{y}{t})=
		f_\mu(\XT{y}{t}) +
		2R_{\mu, \nu}\pder{\ln D(.|\XT{y}{t})}{y_\nu}
		\eeq
Suppose
\beq
\left[\pder{}{s} + \calb_\rvy\right] D(.|\XT{y}{s})=0,
\eeq
and
\beq
\left[\pder{}{s} + \calb_\rvy^D\right] 
P(\XT{y}{t+s}|\XT{x}{t})=0
\eeq
Then
	
\beq
\left[\pder{}{s} + \calb_\rvy\right]
P^D(\XT{y}{t+s}|\XT{x}{t})
=0
\eeq

	
\end{claim}
\proof

$P_s=P(\XT{y}{t+s}|\XT{x}{t})$, $D_s=D(.|\XT{y}{t+s})$, $D_0=D(.|y_{t})$

$\partial_\mu=\pder{}{y_\mu}$,
$\partial_\nu=\pder{}{y_\nu}$,
$\partial_s=\pder{}{s}$




\beq
(\partial_s + \calb_\rvy)P^D_s
=
\frac{1}{D_0}
\left[\partial_s(D_sP_s)
+f_\mu\partial_\mu(D_sP_s)
+R_{\mu, \nu}
\partial_\mu\partial_\nu (D_sP_s)
\right]
\eeq


\beq
\partial_\mu\partial_\nu (D_sP_s)
=
\left\{
\begin{array}{l}
\partial_\mu\partial_\nu(D_s)P_s
\\
+
\partial_\nu(D_s)\partial_\mu(P_s)
\\
+
\partial_\mu(D_s)\partial_\nu(P_s)
\\
+
D_s
\partial_\mu\partial_\nu (P_s)
\end{array}
\right.
\eeq

\beq
(\partial_s + \calb_\rvy)P^D_s
=\left\{
\begin{array}{l}
\frac{P_s}{D_s}
\bcancel{\left[\partial_s +\calb_\rvy
\right]D_s}
\\
+
\frac{1}{D_0}
\left[D_s\partial_s(P_s)
+f_\mu D_s\partial_\mu P_s
\right]
\\
+R_{\mu, \nu}
\left\{
\begin{array}{l}
\partial_\nu(D_s)\partial_\mu(P_s)
\\
+
\partial_\mu(D_s)\partial_\nu(P_s)
\\
+
D_s
\partial_\mu\partial_\nu (P_s)
\end{array}
\right.
\end{array}
\right.
\eeq


\begin{align}
(\partial_s + \calb_\rvy)P^D_s
&=\left\{
\begin{array}{l}
\frac{D_s}{D_0}
\left[\partial_s P_s
+f_\mu\partial_\mu P_s
+R_{\mu, \nu}
\partial_\mu\partial_\nu P_s
\right]
\\
+R_{\mu, \nu}
\partial_\nu(D_s)\partial_\mu(P_s)
\end{array}
\right.
\\
&=
\frac{D_s}{D_0}
\left[\partial_s P_s
+[f_\mu
+ 2R_{\mu, \nu}
\partial_\nu(\ln D_s)
]
\partial_\mu P_s
+R_{\mu, \nu}
\partial_\mu\partial_\nu P_s
\right]
\\
&=
\frac{D_s}{D_0}
\left[\partial_s P_s
+f_\mu^D
\partial_\mu P_s
+R_{\mu, \nu}
\partial_\mu\partial_\nu P_s
\right]
\\
&=
\frac{D_s}{D_0}
(\partial_s + \calb^D_\rvy)P_s
\end{align}

\qed


Let $A=(\XT{x}{t+s})$ , $B=(\XT{x}{T})$.
Bayes Rule says 

\beq
P(A|B, \XT{x}{t})=\frac{P(B|A, \XT{x}{t})P(A|\XT{x}{t})}
{P(B|\XT{x}{t})}
\eeq


\beqa
P(\XT{x}{t+s}|\XT{x}{T}, \XT{x}{t})
&=&
\frac{
P(\XT{x}{T}|\XT{x}{t+s}, \XT{x}{t})P(\XT{x}{t+s}|\XT{x}{t})
}{
P(\XT{x}{T}|\XT{x}{t})
}
\\
&=&
\frac{
P(\XT{x}{T}|\XT{x}{t+s})P(\XT{x}{t+s}|\XT{x}{t})
}{
P(\XT{x}{T}|\XT{x}{t})
}
\eeqa

If we set

\beq
D(.|\XT{x}{t+s})= P(\XT{x}{T}|\XT{x}{t+s})
\eeq
then Claim \ref{cl-doobs-transform} applies

\section{Appendix: Some explicitly solvable examples}
Let $\mu, \nu, \alpha, \beta\in [n]$
\beq
d\rvx_\mu= f(\rvx, t)dt + L(\rvx, t)d\rvB_\mu(t)
\eeq


\beq
\pder{P}{t}= 
-\;\pder{Pf_\mu}{x_\mu}
 + \frac{\partial^2}
 { \partial x_\mu\partial x_\nu}
 \left(PR_{\mu, \nu}\right)
\eeq

\begin{itemize}

\item
{\bf Brownian motion} ($f_\mu=0$, $R_{\mu,\nu}=D\delta(\mu, \nu)$)

\beq
dx_\mu =d\rvB_\mu
\eeq


\item {\bf Overdamped Langevin Equation}
($f_\mu=-\frac{1}{2}\pder{U}{x_\mu}$, $R_{\mu,\nu}=D\delta(\mu, \nu)$)

\beq
dx_\mu = -\;\frac{1}{2}\pder{U}{x_\mu}dt + d\rvB_\mu
\eeq

\item {\bf Ornstein–Uhlenbeck process (a.k.a. Langevin equation)} ($f_\mu=-\lam x_\mu$, $R_{\mu,\nu}=D\delta(\mu, \nu)$)

This is the same as the Langevin equation, if identify $\rvx$ with
the velocity of the 
particle and $\lam$ with the drag coefficient.


\beq
d\rvx = -\lam \rvx dt + d\rvB
\eeq

\item 
{\bf 1-dim ($n=1$) Black-Sholes} ($f=a x$, $R=(b\rvx)^2 q$)

\beq
d\rvx = a \rvx dt + b \rvx d\rvB
\eeq

\item {\bf 1-dim ($n=1$) General SDE}
($f=a x + c$, $R=(b\rvx + d)^2 q$)


\beq
d\rvx = [a(t)\rvx +c(t)]dt + [b(t)\rvx+ d(t)]d\rvB
\eeq


\beq
x(t) =\Psi(t, t_0)
\left(
x(t_0)
+
\int_{t_0}^{t}ds\;\Psi^{-1}(s, t_0)[c(s)-b(s)]
+
\int_{t_0}^{t}
\Psi^{-1}(s, t_0)d(s)d\rvW(s)
\right)
\eeq

\beq
\Psi(t, t_0)=
\exp\left(
\int_{t_0}^t ds\; \left[a(s)-\frac{1}{2}b^2(s)\right]
+
\int_{t_0}^t b(s) d\rvW(s)
\right)
\eeq

\end{itemize}

\section{Appendix: Ornstein-Uhlenbeck 
recurring example}

\hrule
SDE
\beq  
dx = -\lam x dt + d\rvB
\eeq
\hrule
\noindent$P\XT{x}{t}$, Mean and Variance

\beq
P(x_t)=\caln(x_t;\mu=\av{\rvx_t}, 
s^2=\av{\rvx_t, \rvx_t} )
\eeq
\beq
\av{\rvx_t}=x_0e^{-\lam t}
\eeq

\beq
\av{\rvx_t, \rvx_t} = \frac{q}{2\lam}(1-e^{-\lam t})
\eeq



\hrule
\noindent transition  matrix, $P(\XT{x}{t}|\XT{x}{s})$, conditional mean and variance

\beq
P(\XT{x}{t}|\XT{x}{s})= \caln(\XT{x}{t}; \mu= m(t|s), \s^2=V(t|s))
\eeq
\beq
m(t|s)=x_s e^{-\lam (t-s)}
\eeq
\beq
V(t|s)=\frac{q}{2\lam}(1-e^{-2\lam (t-s)})
\eeq
\hrule\noindent Power Spectrum, Autocorrelation

\beq
S(\omega) =
\frac{q}{\omega^2 + \lam^2}
\eeq

\beq AC(\tau) =
\frac{q}{2\lam}e^{-\lam|\tau|}
\eeq

\hrule\noindent steady state covariance
\beq
\av{x_\infty, x_\infty}=
\frac{1}{2\pi}
\int_{-\infty}^{\infty}d\omega\; S(\omega) = \frac{q}{2\lam}
\eeq

\hrule \noindent Doob's transform

\beq
D(\XT{x}{T}|\XT{x}{t})=\caln(x_T; \mu=a(t)x_t, \sigma^2(t))
\eeq

\beq
a(t)=e^{-\lam(T-t)}
\eeq
\beq
\s^2(t) = \frac{q}{2\lam}[1-e^{-2\lam(T-t)}]
\eeq
$a(0)=e^{-\lam T}$, $a(T)=1$. $\s^2(T)=0$,


\beqa
\pder{\ln D}{x}&=&
\left[-\;\frac{1}{2}\ln(2\pi\s^2) -\;\frac{1}{2\s^2}[x_T-ax]^2
\right]
\\
&=&
\frac{a}{\s^2}(x_T-ax)
\eeqa
Goes to zero $1 \frac{0}{0}\rarrow 0$ at $t=T$.

\beq
dx = \left[
-\lam x+ \frac{qa}{\s^2}[x_T-ax]
\right]dt + d\rvB
\eeq
