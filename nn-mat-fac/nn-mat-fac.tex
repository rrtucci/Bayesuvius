\chapter{Non-negative Matrix Factorization COMING SOON}
\label{ch-nn-mat-fac}

This chapter is based on Ref.\cite{wiki-nmf}.

Given
 matrix $Y$, factor it
 into product of two matrices

\beq
Y=WX
\;,
\eeq 

where all 3 matrices
have non-negative entries.

$Y\in \RR^{ny\times na}_{\geq 0}$: visible info matrix

$W\in \RR^{ny\times nx}_{\geq 0}$: weight info matrix

$X\in \RR^{nx\times na}_{\geq 0}$: hidden info matrix


Usually, $ny > nx<na$ so compression of
information (a.k.a. dimensional reduction,
 clustering)

\section{Bnet 
interpretation}

 Express node $\rvy$ as a chain of 
 two nodes.

\begin{figure}[h!]
\centering
$$\xymatrix{
\rvy& \rva\ar[l]&= &\rvy &\rvx\ar[l]&\rva\ar[l]
}$$
\caption{Bnet interpretation of
non-negative matrix factorization.}
\label{fig-nmf}
\end{figure}
The TPMs, printed in blue,
for bnet Fig.\ref{fig-nmf},
are as follows.

\beq\color{blue}
P(\rvy=y|a)=\frac{Y_{y,a}}{\sum_y Y_{y,a}}
\eeq

\beq\color{blue}
P(y|x)=\frac{W_{y,x}}{\sum_y W_{y,x}}
\eeq

\beq\color{blue}
P(x|a)=\frac{\sum_y W_{y,x}}{ \sum_y Y_{y,a}}X_{x,a}
\eeq

\section{Simplest recursive
 algorithm}

Initialize: Choose $nx$. Choose $W^{(0)}$ and $X^{(0)}$
that have non-negative entries. 

Update: For $n=0, 1 , \dots $,
do

\beq
X_{i,j}^{(n+1)}\leftarrow X_{i,j}^{(n)}
\frac{
[(W^{(n)})^T Y]_{i,j}
}{
[(W^{(n)})^T 
\underbrace{W^{(n)} X^{(n)}}_{\approx Y}
]_{i,j}
}
\eeq
and
\beq
W_{i,j}^{(n+1)}\leftarrow W_{i,j}^{(n)}
\frac{
[Y(X^{(n+1)})^T]_{i,j}
}
{
[
\underbrace{W^{(n)} X^{(n+1)}}_{\approx Y}
(X^{(n+1)})^T]_{i,j}
}\;.
\eeq
After each step, record error defined by

\beq
\cale^{(n)} =\parallel Y-W^{(n)}
X^{(n)}\parallel_2
\;.
\eeq
Using 2-norm, a.k.a. Frobenius matrix norm.
Continue until reach acceptable error.

Can also use Kullback-Leibler divergence for error:

\beq
\cale = 
\sum_a P(a)
 D_{KL}(P(\rvy=y|a)\parallel \sum_xP(y|x)P(x|a))
\;,
\eeq
for some arbitrary choice of prior $P(a)$. For 
example, can choose $P(a)$ uniform.