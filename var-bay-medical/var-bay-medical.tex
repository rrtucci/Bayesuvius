\chapter{Variational Bayesian Approximation
for Medical Diagnosis}
\label{ch-var-bay-medical}

This chapter is based
on Ref.\cite{jaak-jordan}.

A Variational Bayesian Approximation (VBA)
is when we approximate
a probability distribution
by another 
probability distribution that depends
on a continuous \qt{variational parameter}.
This parameter is
adjusted 
within its range of possible values,
to make the approximation
as good as possible.
There are many VBA methods.
VBA methods are inspired by 
ancient  methods
used in Calculus 
of Variations applied to Physics
and Engineering problems.

In this chapter,
we do VBA  via
Jensen's inequality and
convex/concave dual functions.

Ref.\cite{jaak-jordan}, on which
this chapter is based,
applies VBA methods to the problem of diagnostic inference using the Quick Medical Reference (QMR) bipartite Bayesian Network. 
According to Ref.\cite{jaak-jordan} 
the maximal clique size of the QMR 
bnet is
150 nodes, which rules out exact methods
of inference like the Junction Tree Algorithm
(see Chapter \ref{ch-junc-tree}).
For such high complexity cases,
one is forced to use
either a VBA or a Monte Carlo method.

\begin{figure}[h!]
$$\xymatrix{
\ul{\ell}
\ar[dr]\ar[drr]\ar[drrr]\ar[drrrr]\ar[drrrrr]
&
&\rvd_1\ar[dl]\ar[drr]
&\rvd_2\ar[dll]\ar[dl]\ar[dr]\ar[d]\ar[drr]
&\rvd_3\ar[dll]\ar[d]\ar[dr]
\\
&\rvs_1
&\rvs_2
&\rvs_3
&\rvs_4
&\rvs_5
}$$
\caption{
Typical bnet (bipartite, 2 level graph) 
for medical diagnosis
to which we will apply VBA
methods.
In this case, $nd=3$ and
$ns=5$. 
According to Ref.\cite{jaak-jordan},
for QMR, $nd\approx 600$ 
and $ns\approx 4000$.
}
\label{fig-var-bay-med-bnet}
\end{figure}

Fig.\ref{fig-var-bay-med-bnet}
gives a typical bnet
for medical diagnosis
to which we will apply VBA methods.
$\rvd_i\in \bool$
for $i=1,2, \ldots, nd$
are the possible diseases,
$\rvs_\s\in\bool$ 
for $\s=1,2, \ldots, ns$ are the possible
symptoms, and $\ul{\ell}\in\bool$
is the leakage  due to 
possible error in 
the parents of the symptoms.
Note that the 
arrows point from
diseases to symptoms
because diseases precede
in time
the symptoms.

Let
\beq
\ZZ_{[1,n]}=\{1,2, \ldots, n\}
\eeq

\beq
pa_\s = \{i\in\ZZ_{[1,nd]}: \rvd_i\in pa(\rvs_\s)\} = \text{ parents of $\rvs_\s$}
\eeq
Note that $pa_\s$
does not include $\ul{\ell}$,
which is also a  parent
of $\rvs_\s$.

\beq
ch_i= \{\s\in\ZZ_{[1,ns]}:
 \rvs_\s\in ch(\rvd_i)\}=
 \text{ children of $\rvd_i$ }
\eeq

\beq
\rvd_{A} =\{\rvd_k: k\in A\}
\eeq

\newcommand{\dall}[0]{d^{nd}}
\beq
\rvd^{nd} =\{\rvd_k: k\in \ZZ_{[1, nd]}\}
\eeq

\beq
\rvd_{!j} =\{\rvd_k: k\in \ZZ_{[1, nd]}-\{j\}\}
\eeq

The TPMs, printed in blue,
for the bnet 
Fig.\ref{fig-var-bay-med-bnet},
are as follows:

\beq \color{blue}
P(d_j) = \text{ given }
\eeq

\beq \color{blue}
P(\ell) = \text{ given }
\eeq

\beqa \color{blue}
P(\rvs_\s=0|d_{pa_\s}, \ell)
&=&\color{blue}
\underbrace{P(\rvs_\s=0|\ell)}_{e^{-\theta_{\s|0}}}
\prod_{j\in pa_\s}
\underbrace{P(\rvs_\s=0|d_j)}_
{e^{-\theta_{\s|j}d_j}}
\\
&=&\color{blue}
e^{-\theta_{\s|0} -
\sum_{j\in pa_\s}\theta_{\s|j}d_j}
\eeqa
where $\theta_{\s|0}, \theta_{\s|j}>0$.
This  
$P(\rvs_\s=0|d_{pa_\s}, \ell)$
corresponds to
the noisy-or model 
(See Chapter \ref{ch-noisy-or}.).

\beq \color{blue}
P(\rvs_\s=1|d_{pa_\s})=
1-
e^{-\theta_{\s|0} -
\sum_{j\in pa_\s}\theta_{\s|j}d_j}
\eeq

Define
\beq
x_\s=
\theta_{\s|0} +
\sum_{j\in pa_\s}\theta_{\s|j}d_j
\eeq
Suppose $A \subset 
\ZZ_{[1,ns]}$, $A^c=\ZZ_{[1,ns]}-A$.
Then

\begin{align}
P(d_j|\rvs_A=0, \rvs_{A^c}=1)&=
 \frac{P(\rvs_A=0, \rvs_{A^c}=1|d_j)P(d_j)}
 {P(\rvs_A=0, \rvs_{A^c}=1)}
 \\
 &=\caln(!d_j)
 \sum_{d_{!j}}P(\rvs_A=0, \rvs_{A^c}=1|\dall)P(\dall)
 \\
 &=\caln(!d_j)
  \sum_{d_{!j}}P(\rvs_{A^c}=1|\dall)
  P(\rvs_A=0|\dall)P(\dall)
\\
&=\caln(!d_j)\sum_{d_{!j}}
\left\{
\begin{array}{l}
\prod_{\s\in A^c}
\overbrace{(1-e^{-x_\s})
}^{\text{ call } \calp_\s}
\\
\prod_{\s\in A}
e^{-\theta_{\s|0}}
\prod_{j\in pa_\s}[e^{-\theta_{\s|j}}]^{d_j}
\\
\prod_{j\in \ZZ_{[1,nd]}} P(d_j)
\end{array}
\right.
\end{align}
Summing this expression
over $d_{!j}$ in closed form seems
impossible, 
but
we will
approximate
the expression
so that this can be done.

Define
\beq
f(x_\s)=\ln(1-e^{-x_\s})
\eeq
and

\beq
\calp_\s = 1-e^{-x_\s} = e^{f(x_\s)}
\eeq
$f(x_\s)$
is a concave function.
See Fig.\ref{fig-dual-dual-ln-1-e-x}
for a plot of it.

Next, we
shall find a lower
and upper bound 
for $\calp_\s$.

To find
the upper bound, we will use 
dual functions, which are
discussed in Section
\ref{sec-dual-fun}.

Let $\TIL{f}(p_\s)$ be the dual
 function of $f(x_\s)
 = \ln(1-e^{-x_\s})$.
 In Section
 \ref{sec-dual-fun}, we show that
 
 \beqa
 \TIL{f}(p_\s)&=&
 \min_{x_\s}(x_\s p_\s - f(x_\s))
 \\
 &=&
 -p_\s\ln  p_\s
 +(1+p_\s)\ln(1+p_\s)
 \eeqa
 and

\beq
f(x_\s)\leq x_\s p_\s - \TIL{f}(p_\s)
\;.
\eeq
Therefore



\beqa
\calp_\s
&=&
e^{f(x_\s)}
\\
&\leq &
 e^{x_\s p_\s - \TIL{f}(p_\s)}
 \\
 &=&
 \underbrace{
 e^{-\TIL{f}(p_\s)}
 e^{-\theta_{\s|0}p_\s}
 \prod_{j\in
  pa_\s}[e^{-\theta_{\s|j}p_\s}]^{d_j}
  }_{\text{ call } \calb(p_\s)
  }
\eeqa

To find
a lower bound for $P_\s$,
we will use
Jensen's inequality, which
is discussed in Section
\ref{sec-jensens}.
Let $q_{j|\s}\in [0,1]$ 
satisfy $\sum_j q_{j|\s}=1$.
Then


\beqa
\calp_\s &=&
 e^{f(x_\s)}
 \\
 &=&
  e^{f\left(\theta_{\s|0} +\sum_j
 \theta_{\s|j}d_j \right)}
 \\
  &=&
  e^{
  f\left(
  \theta_{\s|0}
  +
  \sum_j q_{j|\s}\frac{\theta_{\s|j}d_j}{q_{j|\s}}
  \right)
  }
 \\
 &\geq&
 e^{
 \sum_j q_{j|\s}
 f\left(
 \theta_{\s|0}
 +\frac{\theta_{\s|j}d_j}{q_{j|\s}}
 \right)
 }
 \\
 &=&
 e^{
 \sum_j
 q_{j|\s}\left[
 d_j f\left(
 \theta_{\s|0} + 
 \frac{\theta_{\s|j}d_j}{q_{j|\s}}
 \right)
 +(1-d_j)f(\theta_{\s|0})
 \right]
 }
 \\
 &=&
 \underbrace{
e^{
f(\theta_{\s|0})+
\sum_j
 q_{j|\s} d_j\left[
  f\left(
 \theta_{\s|0} + 
 \frac{\theta_{\s|j}d_j}{q_{j|\s}}
 \right)
 - f(\theta_{\s|0})
 \right]
 }
 }_{
 \text{ call } \cala(q_{.|\s})
 }
  \eeqa
  
 In conclusion, 
\beq
\boxed{
\cala(q_{.|\s})
\leq
\calp_\s
\leq
\calb(p_\s)
}
\eeq
with 
variational
parameters 
$q_{.|\s}$ and $p_\s$.


