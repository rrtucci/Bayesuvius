\chapter{Linear Deterministic Bnets with Exogenous 
Noise}\label{ch-linear-sys}

In this chapter, we will consider 
bnets which were referred to,
prior to the invention of bnets, as:
Sewall Wright's {\bf Path Analysis (PA)}
 and
{\bf Structural Equations Models (SEM)}.
Judea Pearl in his
books calls them
{\bf Structural Causal Models (SCM)},
because they are very 
convenient for doing causal analysis.
We will refer  to
them as PA diagrams
in honor of Sewall Wright.


A {\bf PA diagram}
is a special kind of bnet.
To 
build a PA diagram,
start with a 
linear deterministic bnet $G$.
Now make a bigger bnet $\ol{G}$
called a PA diagram
by 
adding to each node $\rva$ of $G$ a
non-deterministic  
root node $\rvu_\rva$
pointing into $\rva$ only.
The nodes $\rvu_\rva$ are called
the {\bf exogenous (external) variables}.
The exogenous
variables make their children noisy.
They are assumed 
to be unobserved
and their TPMs are priors.
Since they are 
root nodes, they are 
mutually independent.
When we
draw
a PA diagram,
we will
never draw the exogenous nodes,
leaving them implicit.

\hrule\noindent{\bf Example:}


\begin{figure}[h!]
$$\xymatrix{
&\rvx\ar[dl]_\beta\ar[dr]^\alp
\\
\rvw\ar[dr]_\epsilon&&\rvz\ar[ll]_\gamma\ar[dl]^\delta
\\
&\rvy
}$$
\caption{
Example of a
PA diagram wherein
$\rvx$ splits
into two nodes $\rvz$
and $\rvw$,
then merges into node $\rvy$.
There is also an arrow
$\rvz\rarrow \rvw$.
Exogenous
nodes are not shown.}
\label{fig-scm-diamond}
\end{figure}

The TPMs, printed in blue,
for the nodes of the PA diagram 
Fig.\ref{fig-scm-diamond},
are as follows.

\beq\color{blue}
P(y|w, z, u_\rvy)=
\indi(y=\epsilon w +\delta z
+ u_\rvy)
\eeq

\beq\color{blue}
P(w|x, z, u_\rvw)=
\indi(w=\beta x +\gamma z + u_\rvw)
\eeq

\beq\color{blue}
P(z|x, u_\rvz)=
\indi(z=\alpha x + u_\rvz)
\eeq

\beq\color{blue}
P(x|u_\rvx)=
\indi(x=u_\rvx)
\eeq

Hence,
\beqa
y&=&
\epsilon w +\delta z
+ u_\rvy
\\
&=&
\epsilon (\beta x +\gamma z + u_\rvw)
 +\delta z
+ u_\rvy
\\
&=&
(\epsilon\gamma + \delta)z
+ \epsilon\beta x
+\epsilon u_\rvw+ u_\rvy
\\
&=&
(\epsilon\gamma + \delta)z
+ \epsilon\beta u_\rvx
+\epsilon u_\rvw+ u_\rvy\;.
\eeqa
Therefore

\beq
\left(\pder{y}{z}\right)_{u.-u_\rvz}=
\epsilon\gamma + \delta
\;,
\eeq
where the
partial
derivative holds fixed
all
exogenous
variables except
$u_\rvz$.
Note that
this partial
derivative is a 
sum of terms,
and that each of those terms
represents a different
directed path
from $\rvz$ to $\rvy(\rvz)$.
This
is a general
property
of PA
diagrams.
\hrule

\section*{Fully Connected PA diagrams}
The bnets
considered in this section
are all fully connected.
Fully connected
bnets are
defined in Chapter \ref{ch-bnet-def}.
This section
uses the notation
$\av{\rvx, \rvy}$
for 
covariance
of any two random variables $\rvx, \rvy$.
This $\av{\rvx, \rvy}$ notation
is defined in the 
Notational
Conventions Chapter \ref{ch-not-cons}.


Consider a
PA diagram
with 
deterministic nodes
$\rvx.=(\rvx_k)_{k=0, 1, \ldots nx-1}$
and 
corresponding exogenous nodes 
$\rvu.=(\rvu_k)_{k=0, 1, \ldots nx-1}$.
Assume $\av{\rvu_i, \rvu_j}=0$
if $i\neq j$. The strength
of each 
connection 
$\rvx_i\rarrow \rvx_j$
of the PA diagram 
is measured by a
{\bf structural
coefficient} $\alp_{j|i}\geq 0$.

\hrule\noindent
{\bf Fully connected PA diagram with $nx=2$}

\begin{figure}[h!]
$$
\xymatrix{
\rvx_0
\ar[d]^{\alp{1|0}}
\\
\rvx_1
}$$
\caption{
Fully connected PA
diagram with two $\rvx_j$
nodes
(exogenous nodes $\rvu_j$
not shown).}
\label{fig-fully-conn-2}
\end{figure}

Consider the 
PA diagram of Fig.\ref{fig-fully-conn-2}.
This diagram represents the 
following {\bf structural equations}:
\begin{subequations}
\label{eq-fully-conn-2}
\beqa
\rvx_0&=&\rvu_0
\\
\rvx_1&=&\alp_{1|0}\rvx_0  + \rvu_1
\;.
\eeqa
\end{subequations}
Eqs.\ref{eq-fully-conn-2}
constitute a system of 2 
linear equations in 2 unknowns
(the $\rvx$'s) so we can solve
for the $\rvx$'s in terms 
of the $\alpha$'s and $\rvu$'s.

Note also that
\beq
\av{\rvx_1, \rvx_0}=
\alp_{1|0}\av{\rvx_0, \rvx_0}
\;.
\eeq
Thus, $\alp_{1|0}$
can be estimated  
from the covariances $\av{\rvx_i, \rvx_j}$.

\hrule\noindent
{\bf Fully connected
PA diagram with $nx=3$}

\begin{figure}[h!]
$$
\xymatrix{
\rvx_0\ar[d]_{\alp_{1|0}}
\ar[dr]^{\alp_{2|0}}
\\
\rvx_1\ar[r]^{\alp_{2|1}}
&\rvx_2
}$$
\caption{
Fully connected PA diagram with 
three $\rvx_j$ nodes
(exogenous nodes $\rvu_j$
not shown).}
\label{fig-fully-conn-3}
\end{figure}

Consider the PA diagram
of Fig.\ref{fig-fully-conn-3}.
This diagram represents the 
following {\bf structural equations}:
\begin{subequations}
\label{eq-fully-conn-3}
\beqa
\rvx_0 &=& \rvu_0
\\
\rvx_1&=&\alp_{1|0}\rvx_0 + \rvu_1
\\
\rvx_2&=&\alp_{2|1}\rvx_1 +
\alp_{2|0}\rvx_0 +\rvu_2
\;.
\eeqa
\end{subequations}
Eqs.\ref{eq-fully-conn-3}
constitute a system of
3 linear  equations in 3 unknowns
(the $\rvx$'s) so we can solve
for the $\rvx$'s in terms 
of the $\alpha$'s and $\rvu$'s.

Note also that
\begin{subequations}
\label{eq-fully-conn-3-covs}
\beqa
\av{\rvx_1, \rvx_0}&=&
\alp_{1|0}\av{\rvx_0, \rvx_0}
\\
\av{\rvx_2, \rvx_0}&=&
\alp_{2|1}\av{\rvx_1, \rvx_0}
+
\alp_{2|0}\av{\rvx_0, \rvx_0}
\\
\av{\rvx_2, \rvx_1}&=&
\alp_{2|1}\av{\rvx_1, \rvx_1}
+
\alp_{2|0}\av{\rvx_0, \rvx_1}
\eeqa
\end{subequations}
Eqs.\ref{eq-fully-conn-3-covs}
constitute a system of
3 linear  equations in 3 unknowns
(the $\alpha$'s) so we can solve
solve for the $\alpha$'s in terms
of covariances $\av{\rvx_i, \rvx_j}$.
This gives an estimate
for the $\alp$'s.


\hrule\noindent{\bf Fully
connected 
PA diagram with arbitrary $nx$.} 

Let $\rvx.=(x_i)_{i=0, 1,
 \ldots, nx-1}$
and $\rvx_{<i}=
(x_k)_{k=0, 1, \ldots, i-1}$.
Consider
a fully connected
PA diagram
with  deterministic nodes labeled
$\rvx_i$.
The $\rvx_i$ labels 
are assumed
to be in {\bf topological order}
(i.e., the parents of
node $\rvx_i$ are $\rvx_{<i}$).
Let the TPMs,
printed in blue, for the nodes $\rvx.$
of the 
PA diagram, be

\beq\color{blue}
P(x_i|x_{<i}, u_i)=
\indi(
x_i=\sum_{k<i}\alp_{i|k}x_k
 + u_i)
\;,
\eeq
for some parameters $\alp_{i|k}\geq 0$.
The exogenous 
nodes $\rvu.$ are not draw; 
they are implicit. They are assumed
to be independent so

\beq
P(u.)=\prod_i P(u_i)
\eeq
and

\beq
\av{\rvu_i, \rvu_j}=0
\text{  if $i\neq j$}
\;.
\eeq
Note that

\beqa
P(x.)&=&\sum_{u.}P(u.)
\prod_i P(x_i|x_{<i}, u_i)
\\
&=&
E_{\rvu.}[\prod_i P(x_i|x_{<i}, u_i)]
\;.
\eeqa


In terms of random variables,
this system
is described by the following 
{\bf structural equations}:

\beq
\rvx_i=\sum_{k<i}\alp_{i|k}\rvx_k
 + \rvu_i
\;.
\eeq
The structural equations can be
wriiten in matrix form
as follows.
Define a lower triangular
matrix $A$
with the connection 
strengths $\alp_{i|k}\geq 0$
as entries.
For example, for $nx=4$,

\beq
A=
\left[
\begin{array}{cccc}
0&0&0&0
\\
\alp_{1|0}&0&0&0
\\
\alp_{2|0}&\alp_{2|1}&0&0
\\
\alp_{3|0}&\alp_{3|1}&\alp_{3|2}&0
\end{array}\right]
\;.
\eeq
If we now represent the multinodes
$\rvx.$ and $\rvu.$ as column vectors
$\rvx$ and $\rvu$, we get

\beq
\rvx = A \rvx +\rvu
\label{eq-mat-fully-conn}
\;.
\eeq
Note that

\beq
\rvx=(1-A)^{-1}\rvu
\;.
\eeq
Therefore,


\beq
\rvx_i=f_i(\rvu_{\leq i})
\;.
\eeq
Therefore,
if $i>j$,

\beq
\av{\rvu_i, \rvx_j}
=\av{\rvu_i, f_j(\rvu_{\leq j})}=0
\;.
\eeq
Thus,

\beqa
\av{\rvx_i, \rvx_j}&=&
\sum_{k<i}\alp_{i|k}\av{\rvx_k, \rvx_j}
+
\av{\rvu_i, \rvx_j}
\\
&=&
\sum_{k<i}\alp_{i|k}\av{\rvx_k, \rvx_j}
\label{eq-alp-covs-gen}
\eeqa
Eqs.\ref{eq-alp-covs-gen}
constitute a system of
$(nx^2-nx)/2$ linear  equations in 
$(nx^2-nx)/2$ unknowns
(the $\alpha$'s) so we can solve
solve for the $\alpha$'s in terms
of covariances $\av{\rvx_i, \rvx_j}$.
This gives an estimate
for the $\alp$'s.
