\chapter{Goodness of Causal Fit}
\label{ch-good-causal-fit}
This chapter
is an almost verbatim 
copy of a paper of mine, Ref.\cite{tucci-gcf}.

\section{Abstract}
We propose a 
Goodness of Causal Fit (GCF) measure
which depends 
on Pearl ``do" interventions.
This is different
from a measure of Goodness of Statistical Fit (GF),
which does not use interventions.
Given a DAG set  ${\cal G}$,
to find a good $G\in {\cal G}$,
we propose plotting
$GCF(G)$ versus $GF(G)$
for all $G\in {\cal G}$,
and finding a 
graph $G\in {\cal G}$  with 
a large amount 
of both types of goodness.

\section{Introduction}



Frequently,
when students
first encounter
Bayesian Networks (bnets)
and Causal Inference (CI),
they experience serious doubts
about the usefulness of this
theory, because they believe
finding the underlying causal model
(i.e., DAG)
for most realistic
 physical situations is
too difficult or impossible.
I think
that part of the problem
is that these students
are assuming, perhaps
unconsciously,
that there exists
a unique DAG
that fits Nature perfectly,
and a mind-boggling number
 of possibilities
to sift through to find that DAG.
Rather than looking
for a unique DAG,
I think a better strategy
is to write down
a set $\calg$ 
of likely DAGs,
and to calculate for 
each DAG in $\calg$,
 a measure called 
Goodness of Causal Fit (GCF).
Then use the DAGs with
the highest GCF scores.


The goal of this paper
is to propose a GCF measure.
Such a measure is of course
not unique,
and someone may propose 
in the future a measure that is better
than ours.

When designing a GCF measure,
it is important to keep
in mind the First Dictum\footnote{
This is just my whimsical name for it.} of CI: The data
 is causal model-less.
 In the First Dictum,
when we say ``data", we are referring to what is commonly
 called a dataset. A dataset is a table of data, where all
 the entries of each column have the same units, and 
measure a single feature, and each row refers to one
 particular sample or individual. Datasets are particularly 
useful for estimating probability distributions and for 
training neural nets. In the First Dictum, when we say
``causal model", we are
referring to a DAG (directed acyclic graph) or a bnet
 (Bayesian Network= DAG + probability table 
for each node of DAG).

You can try to derive a causal model from a dataset,
but you'll soon find out that you can only go so far.
The process of finding a {\it partial} causal model from a dataset
is called structure learning (SL).  SL can be done quite
 nicely with Marco Scutari's open source program 
{\tt bnlearn} (Ref\cite{bnlearn}).
The problem is that SL often cannot 
narrow down the causal model to a single one.
It finds an undirected graph (UG),
and it can determine the direction of some of the arrows in the UG, 
but it is often incapable, for well understood 
fundamental ---not just technical--- reasons,
 of finding the direction of {\it all}  the arrows of the UG. 
So it often fails to fully specify a DAG model.

Let's call the ordered pair
(dataset, causal model) a
{\bf dataset++}.
 Then what the First Dictum is saying is that a dataset 
is causal model-free or causal
model-less (although sometimes one can
find a partial causal model hidden in there).
A dataset is not a dataset++.

Graphs
which contain both directed 
and undirected edges
are called 
{\bf partially directed (PD) graphs}.
{\tt bnlearn} takes
a dataset as input
and returns a PD graph
$G_{pd}$.
Given a PD graph $G_{pd}$,
let $\calg(G_{pd})$
be the DAG set $\calg$
which 
is generated
by giving directions to all 
undirected edges of $G_{pd}$
in all possible ways.
We will refer to 
 $\calg(G_{pd})$ as 
the {\bf DAG set generated by $G_{pd}$}.
and to any $\calg'\subset 
\calg(G_{pd})$ as a {\bf DAG set partially 
generated by $G_{pd}$.}
Once we define below our
 GCF measure,
we will apply it to
sets of the type $\calg(G_{pd})$
as an example.


It's clear that any measure
of GCF will have to
involve interventions
such as the ``do" intervention
invented by Pearl et al 
for CI.
Without interventions like ``do",
it is impossible
to distinguish causally the
DAGs in a set $\calg(G_{pd})$.

Henceforth, random variables
will be indicated by underlining.

\section{Goodness of Statistical Fit}
Before trying to
define a GCF measure,
it
is instructive to review 
the closely related, well established, measures
of Goodness of Statistical Fit (GF).



Consider 
two
probability distributions
$PO(x)$ and $PE(x)$,
where $x\in S_\rvx$.
By a GF measure, we mean 
a measure of the 
difference between 
$PO$ and $PE$.
Usually $PO$ is the
observed probability distribution and 
$PE$ is the expected, theoretical one.


Three popular
measures of
the difference between $PO$ and $PE$
are:
\begin{enumerate}
\item
The
{\bf Kullback-Liebler divergence}:
\beq
D_{KL}(PO\parallel PE) =
\sum_{x\in S_\rvx}
PO(x)\ln \frac{PO(x)}{PE(x)}
\;.
\eeq
\item
The
{\bf Pearson divergence}
(a.k.a. {\bf Pearson Chi-squared test statistic}):
\beq
D_{\chi^2}(PO\parallel PE)=
\sum_{x\in S_\rvx}
\frac{[PO(x)-PE(x)]^2}{PE(x)}
=
\sum_{x\in S_\rvx}
\frac{PO^2(x)}{PE(x)}-1
\;.
\eeq

It's easy to show 
using $\ln(1+\delta)=\delta +\calo(\delta^2)$
that
if $\left|\frac{PO(x)}{PE(x)}-1\right|<<1$
for all $x\in S_\rvx$, then

\beq
D_{KL}(PO\parallel PE)\approx 
D_{\chi^2}(PO\parallel PE)
\eeq

\item
The {\bf Euclidean distance squared}:

\beq
D_E(PO,PE)=
\sum_{x\in S_\rvx}
[PO(x)-PE(x)]^2
\eeq
\end{enumerate}
Note that of these 3 measures,
only $D_E(PO, PE)$ is symmetric 
in $PO$ and $PE$.


Given any bnet $G$
with full probability
distribution
\footnote{We define
$x.$
to be a vector
with components $x_i$}
  $P_G(x.)$
and a
probability distribution\footnote{
Empirical distributions will 
be denoted by $P$ with a tilde over it.}
$\TIL{P}(x.)$
derived empirically from a dataset,
let

\beqa
D(G)
&=&
\sum_{x.}
\TIL{P}(x.)\ln
\frac{\TIL{P}(x.)}
{P_{G}(x.)}
\\
&=&
D_{KL}(
\TIL{P}(\rvx.)\parallel P_{G}(\rvx.))
\eeqa
We define {\bf Goodness of Statistical Fit (GF)}
of the bnet $G$ by

\beq
GF(G)=\ln \frac{1}
{D(G)}
\eeq

\section{GCF example 1}

\begin{figure}[h!]
$$
\begin{array}{ccc}
\xymatrix{
&\rvb\ar@[red]@{-}[ld]\ar[dr]
\\
\rva\ar[rr]&&\rvz
}
&
\xymatrix{
&\rvb\ar[dl]\ar[dr]
\\
\rva\ar[rr]&&\rvz
}
&
\xymatrix{
&\rvb\ar[dr]
\\
\rva\ar[ur]\ar[rr]&&\rvz
}
\\
G_{pd}&G_1&G_2
\end{array}
$$
\caption{$\calg(G_{pd})=\{G_1, G_2\}$.
The partially directed graph $G_{pd}$
generates the DAGs $G_1$ and $G_2$
by giving directions to
all undirected edges of $G_{pd}$
in
all possible ways.
(In this case, there is only one
undirected edge in $G_{pd}$.) }
\label{fig-ob-eq-1}
\end{figure}

For the first example of
our measure of GCF,
we consider 
$\calg(G_{pd})=\{G_1, G_2\}$
given by Fig.\ref{fig-ob-eq-1}.
We will assume the following:

\begin{itemize}
\item
First, we assume that we have collected
a dataset from which we have
extracted a full empirical
distribution
$\TIL{P}(z, a,b)$.
From $\TIL{P}(z, a,b)$,
we assume that the following
have been calculated.
$\TIL{P}(z,b|a)$
$\TIL{P}(z, a|b)$
$\TIL{P}(a)$, $\TIL{P}(b)$.
\item
Second, we assume that a
dataset has been collected
 for which $\rva$ was held
fixed to each of
the possible values
$a\in S_\rva$ of $\rva$.
Furthermore, we assume
that the distribution
$\TIL{P}(z, b|do(\rva)=a)$
has been extracted from that dataset.
\item
Third, we assume that a
dataset has been collected
 for which $\rvb$ was held
fixed to each of
the possible values
$b\in S_\rvb$ of $\rvb$.
Furthermore, we assume that
the distribution
$\TIL{P}(z, a|do(\rvb)=b)$
has been extracted
from that dataset.
\end{itemize}
We will refer to
$\TIL{P}(z, b|do(\rva)=a)$
and 
$\TIL{P}(z, a|do(\rvb)=b)$
as {\bf empirical do-probability distributions}.


Now define
\beqa
D_a&=&
\sum_{z,b}\TIL{P}(z,b|a)
\ln
\frac{\TIL{P}(z,b|a)}
{\TIL{P}(z,b|do(\rva)=a)}
\\
&=&D_{KL}(\TIL{P}(\rvz, \rvb|a)
\parallel \TIL{P}(\rvz, \rvb|do(\rva)=a))
\\
D_\rva &=& \sum_a \TIL{P}(a) D_a = E_a[D_a]
\eeqa
and

\beqa
D_b
&=&
D_{KL}(\TIL{P}(\rvz, \rva|b)
\parallel \TIL{P}(\rvz, \rva|do(\rvb)=b))
\\
D_\rvb 
&=&
\sum_a \TIL{P}(b) D_b = E_b[D_b]
\;.
\eeqa
We will
refer to $D_\rva$ and $D_\rvb$
as {\bf do-divergences}.

Note that
\beq
D_a(G_2)=0
\text{ for all $a$ so }
\underbrace{D_\rva(G_2)}_0
\leq D_\rvb(G_2)
\eeq
and

\beq
D_b(G_1)=0
\text{ for all $b$ so }
D_\rva(G_1)\geq 
\underbrace{D_\rvb(G_1)}_0
\;.
\eeq

If $D_\rva\leq D_\rvb$,
then $\rva\rarrow \rvb$,
and if $D_\rva\geq D_\rvb$ then
$\rva\larrow \rvb$.
Thus, the arrow and the 
inequality sign point
 in opposite directions.
Alternatively, just remember that
the arrow points to the larger of 
the two $D$'s.

If $D_\rva\leq  D_\rvb$, then define
$GCF(G_1)=-1$ and $GCF(G_2)=1$.

If $D_\rvb\leq  D_\rva$, then define
$GCF(G_1)=1$, $GCF(G_2)=-1$.



\section{GCF example 2}

\begin{figure}[h!]
$$
\xymatrix{
&\rvx_1
\ar@[red]@{-}[dl]\ar@[red]@{-}[dr]
\\
\rvx_2\ar[dr]
&&\rvx_3\ar[dl]
\\
&\rvx_4\ar[d]
\\
&\rvx_5
\\
&G_{pd}
}
\;\;\;\;\;
\xymatrix{
&\rvx_1
\ar[dl]
\ar[dr]
\\
\rvx_2\ar[dr]
&&\rvx_3\ar[dl]
\\
&\rvx_4\ar[d]
\\
&\rvx_5
\\
&G_1
}
\;\;\;\;\;
\xymatrix{
&\rvx_1
\ar[dr]
\\
\rvx_2\ar[dr]\ar[ur]
&&\rvx_3\ar[dl]
\\
&\rvx_4\ar[d]
\\
&\rvx_5
\\
&G_2
}
\;\;\;\;\;
\xymatrix{
&\rvx_1
\ar[dl]
\\
\rvx_2\ar[dr]
&&\rvx_3\ar[dl]\ar[ul]
\\
&\rvx_4\ar[d]
\\
&\rvx_5
\\
&G_3
}
$$
\caption{$\calg=\{G_1, G_2, G_3\}$.
$\calg$ is a set of observationally
equivalent (OE) graphs. 
These are graphs that have the
same value for GF, and
are therefore indistinguishable
from GF alone. For more info about 
OE graphs, see Chapter
\ref{ch-obs-equi}.
Note that $\calg(G_{pd})$
includes one more DAG,
the one in which node $\rvx_1$
is a collider.}
\label{fig-ob-eq-2}
\end{figure}

For the second example of
our measure of GCF,
consider 
$\calg=\{G_1, G_2, G_3\}$
given by Fig.\ref{fig-ob-eq-2}.

Which of the do-divergences
$D_{\rvx_2}$, $ D_{\rvx_1}$ and $D_{\rvx_3}$,
is smallest, middle and
highest, depends on the empirical
do-probability distributions.
For definiteness,
suppose the
sizes of these do-divergences
are related as follows:



\beq
D_{\rvx_2}\leq  D_{\rvx_1} \leq  D_{\rvx_3}
\;.
\eeq


For any two do-divergences
$D_\rva$ and $D_\rvb$,
define the distance
\beq
d_{\rvb,\rva}=
|D_\rvb-D_\rva|
\eeq

If we abbreviate $D_{\rvx_j}$ by $D_j$, 
we can define the GCF for each of
the graphs in $\calg$ by:

\begin{subequations}
\label{eq-gcf-example2}
\beq
GCF(G_1)=
\frac{
-d_{2,1} + d_{1,3}
}
{d_{2,1} + d_{1,3}}
\eeq

\beq
GCF(G_2)=
\frac{
d_{2,1} + d_{1,3}
}
{d_{2,1} + d_{1,3}}=1
\eeq

\beq
GCF(G_3)=
\frac{
-d_{2,1} - d_{1,3}
}
{d_{2,1} + d_{1,3}}=-1
\eeq
\end{subequations}

\section{GCF in general}
Eqs.(\ref{eq-gcf-example2})
are a special case of
the following formulae.

For any two do-divergences
$D_\rva$ and $D_\rvb$,
define the 
{\bf do-divergence distance} by
\beq
d_{\rvb,\rva}=
|D_\rvb-D_\rva|
\;.
\eeq
For any $G_i\in\calg$,
define 
the {\bf edge-sign function} 
 by
\beq
\s_{G_i}(\rva \text{---}\rvb)=
\left\{
\begin{array}{ll}
+1&\text{ if edge
$\rva\text{---}\rvb$ in $G_i$
points
towards larger of $D_\rva$ and $D_\rvb$.}
\\
-1&\text{ otherwise}
\end{array}
\right.
\eeq
Finally, suppose that
$\calg$
is either partially
or fully
generated by
a PD graph $G_{pd}$
with undirected edges
$\{\rva_k\text{---}
\rvb_k\}_{k=0,1, \ldots, nk-1}$.
Then 
define the GCF of 
graph
$G_i\in \calg$ by

\beq
GCF(G_i)= 
\frac{
\sum_{k=0}^{nk-1}
\s_{G_i}(\rva_k\text{---}\rvb_k)
d_{\rva_k, \rvb_k}
}
{
\sum_{k=0}^{nk-1}
d_{\rva_k, \rvb_k}
}
\;.
\label{eq-rel-gfc}
\eeq
Note that
$-1\leq GCF(G_i)  \leq 1$.

If the DAG set $\calg$ 
contains only one DAG $G$,
define $GCF(G)=1$, because all
arrows in $G$ are in the correct
direction, as far as we know.

Call the  
{\bf skeleton} of a DAG, the undirected
graph that one obtains
by turning
all its edges from directed 
to undirected ones.

So far, we 
have applied our measure of
GCF to a DAG set $\calg$
which is
either fully or
partially generated
by a PD graph $G_{pd}$,
or is a singleton set.
But what if we want a GCF
that can score every DAG
in a  DAG set
$\calg$ that contains
DAGs with different skeletons?
In that case, 
let $\{\rva_k\text{---}
\rvb_k\}_{k=0,1, \ldots, nk-1}$
be {\it all} the edges
of graph $G_i\in \calg$,
and use the relative GCF
given by Eq.(\ref{eq-rel-gfc}), 
or use  an absolute 
GCF defined by

\beq
GCF_a(G_i)= 
\sum_{k=0}^{nk-1}
\s_{G_i}(\rva_k\text{---}\rvb_k)
d_{\rva_k, \rvb_k}
\;.
\eeq

So let $\calg$
be an arbitrary DAG set.
Our GCF (or $GCF_a$) measure
is not enough to
decide the best 
possible $G$ in $\calg$,
because there might 
be several graphs with 
$GCF\approx 1$.
For this reason,
we recommend
plotting $GCF(G)$ (or $GCF_a(G)$)
versus $GF(G)$
for all $G\in\calg$.
Then  choose a $G$ with a
large amount
of both types of goodness.
It might even be
advantageous to average over
a small subset of DAGs in  $\calg$
that have large amounts of both
types of goodness.
This would be similar to  averaging
over an ensemble of decision trees to get 
a random forest.

A plot of 
$GCF(G)$
versus $GF(G)$
agrees with the spirit of
the First 
Dictum and datasets++,
because also in those,
we acknowledge a separation between the
dataset (GF)  and causal model (GCF) degrees of freedom.

