\chapter{Modified Treatment Policy (COMING SOON)}
\label{ch-modi-treat}

This  chapter
assumes that the
reader has
already read
Chapter \ref{ch-pot-out} on Potential Outcomes (PO) theory.


\url{https://sci-hub.se/10.1002/sim.5907}

\url{https://www.khstats.com/blog/lmtp/lmtp/}

\url{https://en.wikipedia.org/wiki/Generalized_linear_model}


\cite{hernan-book}

{\bf Modified Treatment Policy (MTP)}

\section{One time MTP}
Consider a typical PO bnet $G$
and its corresponding imagined bnet $G_{im2}$ (see Fig.\ref{fig-modi-po}).
A MTP adds a Bayesian prior to the
 node $\ul{\tilx}$ in $G_{im2}$.
The prior depends on the {\bf treatment
variable} (a.k.a. {\bf exposure variable}) $\rvx$ and its parents $pa(\rvx)=\rvc$.
This adds arrows $\rvc\rarrow\ul{\tilx}$
and $\rvx\rarrow \ul{\tilx}$ (see Fig.\ref{fig-unmodi-modi}).
An ATE is calculated by removing all arrows entering $\ul{\tilx}$
so as to mimic a RCT. Hence, a MTP cannot be used to calculate
an ATE or mimic an RCT.


\begin{figure}[h!]
$$
\begin{array}{ccc}
\xymatrix{
\rvc
\ar[dd]\ar[ddrr]
\\
\\
\rvx \ar[rr]
&
&\rvy
}
&
\xymatrix{
\rvc
\ar[dd]\ar[dr] \ar[drr]
\\
& \rvy(0)\ar[dr]
&\rvy(1)\ar[d]
\\
\rvx\ar[rr]
&
&\rvy
}
&
\xymatrix{
\rvc
\ar[dd]\ar[dr] \ar[drr]
\\
& \rvy(0)\ar[dr]
&\rvy(1)\ar[d]
\\
\rvx
&\ul{\tilx}=\tilx\ar[r]
&\rvy
}
\\
G
&G_{im1} =\cali_{\rvx\rarrow\rvy} G
& G_{im2}=\cali_{\ul{\tilx}\rarrow\rvy}\cali_{\rvx\rarrow\rvy}(\tilx)G
\end{array}
$$
\caption{$G$ is one of the
simplest possible bnets considered in  PO theory.
In the usual PO theory,
one only considers the bnet $G_{im1}$.
In MTP theory, we
consider the bnet $G_{im2}$.
For $G_{im2}$, $\tilx\in S_{\ul{\tilx}}=S_\rvx=\bool$.
}
\label{fig-modi-po}
\end{figure}


\begin{figure}[h!]
$$
\begin{array}{cc}
\xymatrix{
pa(\rvx) =\rvc
\ar[dd]
&
&pa'(\rvy)\ar[d]
\\
&&[\rvy(\tilx)]_{\tilx\in S_{\ul{\tilx}}}\ar[d]
\\
\rvx
&\ul{\tilx}=\tilx\ar[r]
&\rvy
}
&
\xymatrix{
pa(\rvx)=\rvc
\ar[dd]\ar[ddr]
&
&pa'(\rvy)\ar[d]
\\
&&[\rvy(\tilx)]_{\tilx\in S_{\ul{\tilx}}}\ar[d]
\\
\rvx\ar[r]
&\ul{\tilx}\ar[r]
&\rvy
}
\\
G_{im2}
 &G_{im2,mod}
\end{array}
$$
\caption{In this figure, $G_{im2}$
is the generalization of the $G_{im2}$
in Fig.\ref{fig-modi-po}.
Note that $pa'(\rvy)$ are the parents in $G$ of
$\rvy$, excluding $\rvx$.
Note that $S_{\ul{\tilx}}\subset S_\rvx$. }
\label{fig-unmodi-modi}
\end{figure}

The TPM, printed in  blue, for node $\ul{\tilx}$
of the imagined bnet $G_{im2}$ in Fig.\ref{fig-unmodi-modi}, is as follows
\beq\color{blue}
P(\ul{\tilx}=\tilx) =\delta(\tilx, x)
\eeq

The TPM, printed in  blue, for node $\ul{\tilx}$
of the modified imagined bnet $G_{im2,mod}$ in Fig.\ref{fig-unmodi-modi},
 is as follows
\beq\color{blue}
P(\ul{\tilx}=\tilx | \rvx=x, c)= \text{ prior}
\eeq

Hence, node $\ul{\tilx}$
has a non-informative (frequentist) prior in $G_{im2}$,
and it has an informative (Bayesian) prior in $G_{im2,mod}$.

Assumptions about $G_{im2, mod}$

\begin{enumerate}


\item {\bf Consistency (a.k.a. SUTVA)}

The TPM, printed in  blue, for node $\rvy$
of the modified imagined bnet $G_{im2,mod}$ in Fig.\ref{fig-unmodi-modi},
 is as follows

\beq \color{blue}
P(y\cond \tilx, \{y(\tilx')\}_{\tilx'\in S_{\ul{\tilx}}})=
\delta(y , \sum_{\tilx'\in S_{\ul{\tilx}}}
 y(\tilx')\indi(\tilx'=\tilx))
\eeq

\beqa
P(\rvy=y|\tilx, pa'(\rvy)=\xi)&=&
P(\rvy(\tilx)=y|\tilx, pa'(\rvy)=\xi)
\eeqa


\item {\bf Exchangeability (a.k.a.
conditional independence assumption (CIA))}

\beqa
P(\rvy(\tilx')=y|\tilx, pa'(\rvy)=\xi)
&=&
P(\rvy(\tilx')=y|pa'(\rvy)=\xi)
\eeqa

\item {\bf Identifiability}

We must have
\beq
S_{\ul{\tilx}} \subset S_\rvx
\eeq
in $G_{im2, mod}$
or else queries of the type
 $P(\rvy(\tilx)=y|pa'(\rvy)=\xi)$ are not identifiable.
This is clear because if $\tilx\not\in S_\rvx$,
then we have no information of the type
$P(\rvy=y|\tilx, pah(\rvy)=c)=P(\rvy(\tilx)=y|pa'(\rvy)=\xi)$.

\item{\bf Positivity}

Define the following 2 propensities
for $x\in S_\rvx$ and $\tilx \in S_{\ul{\tilx}}$:

\beq
g_{x|c}=P(\rvx=x |c)
\eeq

\beq
\tilg_{\tilx|c}=P(\ul{\tilx}=\tilx |c)
\eeq

Positivity for  $G_{im2,mod}$
is the requirement that

\beq
0<g_{\tilx|c}<1 \text{ for all } \tilx \in S_{\ul{\tilx}}
\eeq
If $g_{\tilx|c}$
is deterministic, then this requirement
is not satisfied, and we cannot do IPW (i.e.,
inverse propensity weighing).
If $g_{\tilx|c}$
is defined to equal $g_{f(\tilx)|c}$ for some
function $f:S_{\ul{\tilx}}\rarrow S_\rvx$
and $S_{\ul{\tilx}}\subset S_\rvx$, then
positivity is assured.

Note that using a MDP can allow IPW to
be performed  in cases
when the propensity $g_{x|c}$
is not defined for all $x\in S_\rvx$, but is defined
for all $x\in S_{\ul{\tilx}}$, where
$S_{\ul{\tilx}}$
is some proper subset of $S_\rvx$.

\end{enumerate}

The probability
distribution
$P(\tilx|x, c)$ (i.e., the  $\ul{\tilx}$ prior for $G_{im2,mod}$)
is called the {\bf modified treatment plan (MTP)}.
A MTP can be deterministic or probabilistic.
An MTP that depends (resp., does not depend)
 on $x$ is said to
be dynamic (resp., static), because $\tilx$ depends (resp., does not
depend) on the previous value
of $x$.

\begin{enumerate}
\item Deterministic MTP

Suppose $S_{\ul{\tilx}}\subset S_\rvx$
and we are given a function
$\tilx_c(\cdot):S_\rvx\rarrow S_{\ul{\tilx}}$.
Then let the TPM, printed
in blue, for node $\ul{\tilx}$, be as follows:

\beq\color{blue}
P(\ul{\tilx}=\tilx | \rvx=x, c)= \delta(\tilx, \tilx_c(x))
\eeq

Examples of $\tilx_c(x)$
\begin{itemize}

\item threshold for in $x$ for small $x$

Let $S_\rvx=[a,b]$ where $a<b$.
For some $\theta, u(c)\in [a,b]$,

\beq
\tilx_c(x) = \left\{\begin{array}{ll}
\theta &\text{ if }x\leq u(c)
\\
x &\text{ if }x> u(c)
\end{array}
\right.
\eeq

\item upshift in $x$ for small $x$

Let $S_\rvx=[a,b]$ where $a<b$.
For some $\Delta x >0$,
and $u(c)\in[a,b]$,

\beq
\tilx_c(x) = \left\{\begin{array}{ll}
(x +  \Delta x) &\text{ if }x< u(c)\text{ and }
(x +  \Delta x)\in[a,b]
\\
x &\text{ if }x> u(c)
\\
b & \text{ if }(x +  \Delta x)>b
\end{array}
\right.
\eeq



\end{itemize}


\item Stochastic MTP

For some
convenient, user specified,
probability
distribution
$P(\tilx| x', x, c)$,
where $\tilx\in S_{\ul{\tilx}}$,
$x,x'\in S_\rvx$,
 and $c\in S_\rvc$,
let the TPM, printed
in blue, for node $\ul{\tilx}$, be as follows:


\beq\color{blue}
P(\tilx|x,c)=
\sum_{x'\in S_\rvx} P(\tilx| x', x, c)
\underbrace{P(\rvx=x'|c)}_{\text{unmodified propensity}}
\eeq

Examples
\begin{itemize}
\item
Suppose we are given a function $x_c:S_{\ul{\tilx}}\rarrow S_\rvx$.
($x_c(\tilx)$ could be the inverse, if
 it exists, of the function $\tilx_c(x)$
defined in the deterministic TMP case.)

\beq
P(\tilx|x,c) = \frac{P(\rvx=x_c(\tilx)|c)}{\sum_\tilx numerator}
\eeq

\item
Suppose
$S_\rvx=S_{\ul{\tilx}}=\bool$, $\theta\in\RR$,
and let


\beq
P(\tilx|x, c)
=
[\pi(x,c)]^{\tilx} [1-\pi(x,c)]^{1-\tilx}
\eeq
where

\beq
\pi(x,c)=
\frac{\theta^x P(\rvx=x|c)}{\sum_x numerator}
\eeq

\end{itemize}


\end{enumerate}

\subsection{PIE}

\beqa
\caly_{|x, c} &=& \sum_y y P(\rvy=y\cond x, c)
\\
&=&
\sum_y y P(\rvy(x)=y\cond x, c)
\eeqa

\beq
\tilde{\caly}_{|x, c} =
\sum_y y P(\rvy(\tilx)=y\cond x, c)
\underbrace{P(\tilx|x,c)}_{
\begin{array}{c}=\delta(\tilx, \tilx_c(x))
\\
\text{if MTP is}
\\
\text{deterministic}
\end{array}}
\eeq

\beq
\caly = \sum_{x,c} \caly_{|x, c}P(x|c)P(c)
\eeq

\beq
\tilde{\caly} = \sum_{x,c} \tilde{\caly}_{|x, c}P(x|c)P(c)
\eeq


PIE Estimand

\beq
PIE= \caly- \tilde{\caly}
\eeq

\subsection{PIE Estimators}
\section{Multi-time MTP}

Multi-time (a.k.a. longitudinal) MTP (LMTP)
g-formula for LMTP\footnote{Other formulae
that
define recursively the full
probability distribution of a bnet,
are also called g-formulae. Chapter
\ref{ch-g-formula} defines
a different g-formula.}



Consider  $t=1,2, \ldots, nt$,

For $nt=1$,

\beq
\begin{array}{cc}
\xymatrix{
\rvc_1\ar[d]\ar[dr]
\\
\rvx_1\ar[r]
&\rvy
}
 &
\xymatrix{
\rvc_1\ar[d]\ar@/^1pc/[dd]\ar[ddr]
\\
\rvx_1\ar[d]
\\
\ul{\tilx}_1\ar[r]
&\rvy
}
\end{array}
\eeq

\beq
P(y,x_1, c_1)=
P(y|x_1, c_1)
P(x_1|c_1)
P(c_1)
\eeq

\beq
P(y,\tilx_1,x_1, c_1)=
P(y|\tilx_1, c_1)
P(x_1|c_1)
P(\tilx_1|x_1, c_1)P(c_1)
\eeq
For $nt=2$,

\beq
\begin{array}{ll}
\xymatrix{
\rvc_1\ar[d]\ar[r]\ar[dr]
&\rvc_2\ar[d]\ar[rd]
\\
\rvx_1\ar[r] \ar[ru]
&\rvx_2\ar[r]
&\rvy
}
&
\xymatrix{
\rvc_1\ar[d]\ar[r]\ar@/^1pc/[dd]\ar[ddr]\ar[dr]
&\rvc_2\ar[d]\ar@/^1pc/[dd]\ar[ddr] \ar[rdd]
\\
\rvx_1\ar[d]
&\rvx_2\ar[d]
\\
\ul{\tilx}_1\ar[ru] \ar[ruu] \ar[r]
&\ul{\tilx}_2\ar[r]
&\rvy
}
\end{array}
\eeq

\beq
P(y,x_{\leq 2},c_{\leq 2})=
\left\{
\begin{array}{l}
P(y|x_2, c_2)
\\
*P(x_1|c_1) P(c_1)
\\
*P(x_2|x_1, c_{2,1})P(c_2|x_1, c_1)
\end{array}
\right.
\eeq

\beq
P(y,\tilx_{\leq 2}, x_{\leq 2},c_{\leq 2})=
\left\{
\begin{array}{l}
P(y|\tilx_2, c_2)
\\
*P(x_1|c_1)
P(\tilx_1|x_1, c_1) P(c_1)
\\
*P(x_2|\tilx_1, c_{2,1})
P(\tilx_2|x_2, c_{2,1})
P(c_2|\tilx_1, c_1)
\end{array}
\right.
\eeq


For $nt=3$,

\beq
\begin{array}{cc}
\xymatrix{
\rvc_1\ar[d]\ar[r]\ar[dr]
&\rvc_2\ar[d]\ar[r]\ar[dr]
&\rvc_3\ar[d] \ar[rd]
\\
\rvx_1\ar[r]\ar[ru]
&\rvx_2\ar[r]\ar[ru]
&\rvx_3\ar[r]
&\rvy
}
&
\xymatrix{
\rvc_1\ar[d]\ar[r]\ar[ddr]\ar[dr]\ar@/^1pc/[dd]
&\rvc_2\ar[d]\ar[r]\ar[ddr] \ar[dr]\ar@/^1pc/[dd]
&\rvc_3\ar[d]\ar[ddr] \ar@/^1pc/[dd] \ar[rdd]
\\
\rvx_1\ar[d]\ar[rd]
&\rvx_2\ar[d]\ar[rd]
&\rvx_3\ar[d]
\\
\ul{\tilx}_1\ar[ru] \ar[ruu] \ar[r]
&\ul{\tilx}_2\ar[ru]\ar[ruu]  \ar[r]
&\ul{\tilx}_3\ar[r]
&\rvy
}
\end{array}
\label{eq-modi-bnet-nt-3}
\eeq

\beq
P(y, x_{\leq 3}, c_{\leq 3})
=
\left\{
\begin{array}{l}
P(y| x_3, c_3)
\\
*P(x_1|c_1) P(c_1)
\\
*P(x_2|x_1, c_{2,1})P(c_2| x_1, c_1)
\\
*P(x_3|x_{2}, c_{3,2}) P(c_3|x_2, c_2)
\end{array}
\right.
\eeq


\beq
P(y,\tilx_{\leq 3}, x_{\leq 3}, c_{\leq 3})
=
\left\{
\begin{array}{l}
P(y|\tilx_3, c_3)
\\
*P(x_1|c_1)
P(\tilx_1|x_1, c_1)P(c_1)
\\
*P(x_2|\tilx_1, c_{2,1})
P(\tilx_2|\tilx_1, x_{2,1}, c_{2,1})  P(c_2|\tilx_1, c_1)
\\
*P(x_3|\tilx_{2}, c_{3,2})
P(\tilx_3|\tilx_{2}, x_{3,2}, c_{3,2}) P(c_3|\tilx_2, c_2)
\end{array}
\right.
\eeq

In general,

\beq
P(y,x_{\leq nt}, c_{\leq nt})=
P(y|x_{nt}, c_{nt})
\prod_{t=1}^{nt}
\{
P(x_t|x_{t-1}, c_{t, t-1} )
P(c_t|\tilx_{t-1}, c_{t-1})
\}
\eeq

\beq
P(y,\tilx_{\leq nt},x_{\leq nt}, c_{\leq nt})=
\left\{
\begin{array}{l}
P(y|\tilx_{nt},x_{nt}, c_{nt})
\\
*\prod_{t=1}^{nt}
P(x_t|\tilx_{t-1}, c_{t, t-1} )
P(\tilx_t|\tilx_{t-1}, x_{t,t-1}, c_{t, t-1})
P(c_t|\tilx_{t-1},  c_{t-1})
\end{array}
\right.
\eeq



Note that Ref.\cite{hernan-book} by Hern\'{a}n and Robins  (HR) uses the
more common notation $L_{t} = \rvc_{t}$,
$A_{t} = \rvx_{t}$
for $t=1, 2, \ldots, nt$. Hence, the $nt=2$ bnet
Eq.(\ref{eq-modi-bnet-nt-3}), written in the HR notation, is
as follows
(before and after adding the $\tilde{A}$ nodes):
\beq
\begin{array}{cc}
\xymatrix{
L_1\ar[d]\ar[r]\ar[dr]
&L_2\ar[d]\ar[r]\ar[dr]
&L_3\ar[d]\ar[rd]
\\
A_1\ar[r]\ar[ru]
&A_2\ar[r]\ar[ru]
&A_3\ar[r]
&\rvy
}
&
\xymatrix{
L_1\ar[d]\ar[r]\ar[ddr]\ar[dr]\ar@/^1pc/[dd]
&L_2\ar[d]\ar[r]\ar[ddr] \ar[dr]\ar@/^1pc/[dd]
&L_3\ar[d]\ar[ddr] \ar@/^1pc/[dd]
\\
A_1\ar[d]\ar[rd]
&A_2\ar[d]\ar[rd]
&A_3\ar[d]
\\
\tilde{A}_1\ar[ru]\ar[r]
&\tilde{A}_2\ar[ru]\ar[r]
&\tilde{A}_3\ar[r]
&\rvy
}
\end{array}
\eeq
HR doesn't actually display the $\tilde{A}$
nodes in his DAGs. Furthermore, in HR,
 nodes $A_t$ and $L_t$
have parent nodes
that are time steps $0,1,2,3,\ldots $ in the past.
I, on the other hand, will
not draw or consider parents that are 2 or more
time steps in the past,
because I will assume the effect of those parents is
negligible to first approximation.

\section{Entropy-likelihood connection}
\label{sec-ent-like-connect}

\beq
\call_{y|\theta} = \ln P(y|\theta)
\eeq

\beq
\av{f(y)}= \sum_y P(y|\theta) f(y)= E_{\rvy|\theta}[f(y)]
\eeq

\beq
H(\rvy|\theta)= -\av{\call_{y|\theta}}
\eeq

\begin{claim}

\beq
\av{\partial_\theta  \call_{y|\theta}}=0
\eeq

\beq
\av{\partial^2_\theta  \call_{y|\theta}}=
-\av{ (\partial_\theta \call_{y|\theta})^2 }
\eeq

\end{claim}
\proof

\beqa
\av{\partial_\theta  \call_{y|\theta}}
&=&
\sum_y P(y|\theta)\partial_\theta\ln  P(y|\theta)
\\
&=&
\sum_y \partial_\theta P(y|\theta)
\\
&=& 0
\eeqa


\beqa
\av{\partial^2_\theta  \call_{y|\theta}}
&=&
\sum_y  P(y|\theta)\partial_\theta \left[ \frac{1}{P(y|\theta)}
 \partial_\theta P(y|\theta) \right]
\\
&=&
- \sum_y  P(y|\theta)
\frac{1}{P(y|\theta)^2}[ \partial_\theta P(y|\theta)]^2
+   \sum_y   \partial_\theta^2   P(y|\theta)
\\
&=&
-\sum_y  P(y|\theta)[ \partial_\theta \ln P(y|\theta)]^2
\\
&=&
-\av{ (\partial_\theta \call_{y|\theta})^2 }
\eeqa
\qed

\beq
\Delta \theta =  \theta' - \theta
\eeq

\beq
-\Delta H(\rvy|\theta)=
\Delta\av{ \call_{y|\theta}}
=
\av{ \call_{y|\theta'}}-
\av{ \call_{y|\theta}}
\eeq

\beq
\av{ \call_{y|\theta'}}=
\av{ \call_{y|\theta}}
+
\Delta \theta
\underbrace{\av{ \partial_\theta\call_{y|\theta}}}_{=0}
+
\frac{(\Delta\theta)^2}{2}
\underbrace{\av{ \partial^2_\theta\call_{y|\theta}}}_
{-\av{ (\partial_\theta\call_{y|\theta})^2} }
+ \calo((\Delta\theta)^3)
\eeq

\beq
-\Delta H(\rvy|\theta)=\Delta\av{ \call_{y|\theta}}
=
-\frac{(\Delta\theta)^2}{2}
\av{ (\partial_\theta\call_{y|\theta})^2}
+ \calo((\Delta\theta)^3)
\eeq

If average over $P(y|\theta)$,
then $\theta$ maximizes log likelihood.


\section{Exponential Family of Distributions}
\label{sec-exp-fam}

Exponential Family (EF) of distributions

\beq
P(y|\theta, \phi) =
\exp\left(\frac{\theta y - b(\theta)}{a(\phi)}+c(y,\phi)\right)
\eeq


\begin{claim}
\beq
\mu = E[\rvy] = b'(\theta)
\eeq

\beq
\s^2= \av{\rvy, \rvy} = a(\phi)b''(\theta)
\eeq
\end{claim}
\proof

\beqa
0&=& \partial_\theta \int_{-\infty}^\infty
dy\; P(y|\theta, \phi)
\\
&=&
\int_{-\infty}^\infty
dy\;\frac{1}{a}
[y-b'(\theta)]P(y|\theta, \phi)
\\
&=&
\frac{1}{a}[E[\rvy]-b'(\theta)]
\eeqa

\beqa
0&=& \partial^2_\theta \int_{-\infty}^\infty
dy\; P(y|\theta, \phi)
\\
&=&
\int_{-\infty}^\infty
dy\;\left\{\frac{-b''(\theta)}{a} +
\frac{1}{a^2}
[y-b'(\theta)]^2\right\}
P(y|\theta, \phi)
\eeqa
Hence,

\beq
\av{[y-b'(\theta)]^2}= a b''(\theta)
\eeq

\qed

\beqa
\caln(y; \mu, \s^2)
&=&
\frac{1}{\sqrt{2\pi \s^2}}
\exp\left(
-\frac{(y-\mu)^2}{2\s^2}
\right)
\\
&=&
\exp\left(
\frac{-y^2 + 2\mu y -\mu^2}{2\s^2}
-\ln\sqrt{2\pi \s^2}
\right)
\\
&=&
\exp\left(
\frac{-\frac{1}{2}y^2 + \mu y -\frac{1}{2}\mu^2}{\s^2}
-\ln\sqrt{2\pi \s^2}
\right)
\eeqa
So, for the
Normal Distribution, $\theta=\mu$ , $a=\s^2$, $b=\mu^2/2$,
$b'=\mu$, $b''=1$.

\beq
P(\vecy|\vtheta, \phi) =
\prod_\s
\exp\left(\frac{\theta_\s y_\s - b(\theta_\s)}{a(\phi)}+c(y_\s ,\phi)\right)
\eeq

\beq
\mu_\s = E[\rvy_\s]= \partial_{\theta_\s} b
\eeq

\beq
\av{\rvy_\s, \rvy_{\s'}} =
\delta(\s, \s')
 a\partial_{\theta_\s}\partial_{\theta_{\s'}}b
\eeq


\section{Generalized Linear Model (GLM)}

The General Linear Model (GLM)
is a way of modelling a random variable $\rvy_\s$.
A GML
has 3 parts:

\begin{enumerate}
\item Exponential Family

Model $\rvy_\s$ by probability distribution
of exponential family (EF).
EF is described in Section [\nameref{sec-exp-fam}]]

\item Linear predictor

In EF, replace $\theta_\s$
the {\bf linear predictor} $\xbeta= \sum_i X_{\s, i}\beta_i$.
$\xbeta$ is commonly denoted by $\eta_\s$,
but I will avoid that convention because
I think results are much clearer
if one uses the more explicit $\xbeta$ instead.

\item Link Function

\beq
\mu_\s = E[\rvy_\s] = \partial_{\xbeta} b
\eeq

\beq
\xbeta = g(\mu_\s) = \hat{\theta}(\mu_\s)
\eeq

\beq
\mu_\s = g^{-1}(\xbeta)=\hat{\mu}(\xbeta)
\eeq


$\hat{\theta}()=g()$ is called
the {\bf link function}.
\end{enumerate}


For Linear Regression, $\mu_\s = \xbeta$,
so the link function and its
inverse are the identity map.

\beqa
Ber(y_\s;p_\s) &=& p_\s^{y_\s} (1-p_\s)^{1-y_\s}
\\
&=&
\exp(y_\s\ln p_\s + (1-y_\s)\ln(1-p_\s))
\\
&=&
\exp\left( y_\s
\underbrace{\ln\left(\frac{p_\s}{1-p_\s}\right)}_{\xbeta}
+\underbrace{\ln (1-p_\s)}_{-b}\right)
\eeqa


$\mu_\s = p_\s$,
and $\xbeta= \lodds(\mu_\s)$ so $\mu_\s= \smoid(\xbeta)$.

$g() =\lodds()$ and $g^{-1}() =\smoid()$

\beq
a=1
\eeq

\beqa
b
&=&
 -\ln (1-\mu_\s)
\\
&=&
-\ln\left(
1- \frac{1}{1+e^{-\xbeta}}
\right)
\\
&=&
-\ln\left(
\frac{e^{-\xbeta}}{1+e^{-\xbeta}}
\right)
\\
&=&
\ln(1+e^{\xbeta})
\eeqa

\beqa
\partial_{\xbeta} b &=&
\frac{e^{\xbeta}}{1+e^{\xbeta}}
\\
&=& \smoid(\xbeta)
\\
&=&\mu_\s
\eeqa


% Please add the following required packages to your document preamble:
% \usepackage[table,xcdraw]{xcolor}
% If you use beamer only pass "xcolor=table" option, i.e. \documentclass[xcolor=table]{beamer}
\begin{table}[h!]
\centering
\begin{tabular}{|
>{\columncolor[HTML]{FFFFC7}}l |l|l|}
\hline
\cellcolor[HTML]{F6F694}prob. distribution & \cellcolor[HTML]{F6F694}link function
$\xbeta=g(\mu_\s)$ & \cellcolor[HTML]{F6F694}$\mu_\s=g^{-1}(\xbeta)$ \\ \hline
\begin{tabular}[c]{@{}l@{}}Normal\\ $\rvy_\s\in(-\infty, +\infty)$\end{tabular} & \begin{tabular}[c]{@{}l@{}}$\xbeta= \mu_\s$\\ ($g$= identity map)\end{tabular} & $\mu_\s = \xbeta$ \\ \hline
\begin{tabular}[c]{@{}l@{}}Exponential\\ $\rvy_\s\in  (0, +\infty)$\end{tabular} & $\xbeta= -\frac{1}{\mu_\s}$ & $\mu _\s= - \frac{1}{\xbeta}$ \\ \hline
\begin{tabular}[c]{@{}l@{}}Poisson\\ $\rvy_\s\in \{0,1,2, \ldots\}$\end{tabular} & $\xbeta = \ln \mu_\s$ & $\mu_\s = \exp(\xbeta)$ \\ \hline
\begin{tabular}[c]{@{}l@{}}Bernoulli\\ $\rvy_\s\in\bool$\end{tabular} & $\xbeta = \lodds(\mu_\s)$ & $\mu_\s = \smoid(\xbeta)$ \\ \hline
\end{tabular}
\caption{Various probability distributions of the EF that have simple link functions that are easy to invert.}
\label{tab-ef-link-fun}
\end{table}


\beq
y = X\beta + \eps
\eeq

\beq
\av{\eps, \eps^T}= \s^2 I_N
\eeq

Maximum Likelihood Estimate  (MLE)

\beq
\hat{\beta}=
(X^T X)^{-1} X^T y
\eeq

\beqa
\av{\hat{\beta},\hat{\beta}^T}
&=&
\av{X^T X)^{-1} X^T y, y^T X (X^T X)^{-1}}
\\
&=&
\s^2  (X^T X)^{-1}
\eeqa

Next
we will give for the GLM,
an implicit expression
(i.e., not a closed
form) for $\av{\hat{\beta}}$
and an asymptotic
expression for
$\av{\hat{\beta}, \hat{\beta}^T}$.

Will use the results of Section [\nameref{sec-ent-like-connect}]
in our proofs, where we define
the likelihood for the
particular case we are considering now by

\beq
\call =  \sum_\s \call_\s
\eeq
where

\beqa
\call_\s &=&  \call_{y_\s|\theta_\s }
\\
&=&
\frac{y_\s\theta_\s - b(\theta_\s)}{a(\phi)} + c(y_\s, \phi)
\eeqa


\begin{claim}
(Implicit expression
for $\av{\hat{\beta}}$
for GLM)

\beq
 \sum_\s \frac{[y_\s - b'(\xbeta)]}{\av{y_\s, y_\s}}
 \pder{\hat{\mu}_\s}{\xbeta}
X_{\s, \s'} =0
\eeq
for all $\s'$.
\end{claim}
\proof


\beq
\pder{\call_{\s}}{\beta_{\s'}}
=
\pder{\call_{\s}}{\theta_\s}\pder{\theta_\s}{\mu_\s}
\pder{\hat{\mu}_\s}{\xbeta}\pder{\xbeta}{\beta_{\s'}}
\eeq

\beq
\pder{\call_{\s}}{\theta_\s}  =
\frac{y_\s - b'(\theta_\s)}{a(\phi)}
\eeq

\beq
\pder{\mu_\s} {\theta_\s}
=
b''(\theta_\s) = \frac{\av{y_\s, y_\s}}{a(\phi)}
\eeq

\beq
\pder{\hat{\mu}_\s}{\xbeta} =
(g^{-1})'(\xbeta)
\eeq

\beq
\pder{\xbeta}{\beta_{\s'}}= X_{\s, \s'}
\eeq

\beq
\pder{\call_{\s}}{\beta_{\s'}}
=
\frac{[y_\s - b'(\theta_\s)]}{\av{y_\s, y_\s}}
 \pder{\hat{\mu}_\s}{\xbeta}
X_{\s, \s'}
\eeq

\beq
0= \av{\pder{\call}{\beta_{\s'}}}  =
\sum_\s  \pder{\call_{\s}}{\beta_{\s'}}
=
 \sum_\s \frac{[y_\s - b'(\theta_\s)]}{\av{y_\s, y_\s}}
 \pder{\hat{\mu}_\s}{\xbeta}
X_{\s, \s'}
\eeq
for all $\s'$.
 \qed


(asymptotic covariance)
\begin{claim}
(Asymptotic expression
for $\av{\hat{\beta},\hat{\beta}^T}$
for GLM)


\beq
\av{\hat{\beta},\hat{\beta}^T}
\rarrow (X^T W X)^{-1}
\eeq
where

\beq
W_{\s, \s'} = \left[\frac{(\partial_{\xbeta} \hat{\mu} )^2 }
{\av{y_\s, y_\s} } \delta(\s, \s')\right]_{\beta=\hat{\beta}}
\eeq
\end{claim}
\proof

\begin{align}
\av{\pder{\call_\s}{\beta_ { \s'}}
\pder{\call_\s}{\beta_ { \s''}} }
&=
\av{
\frac{[y_\s - b'(\xbeta)]}{\av{y_\s, y_\s}}
 \pder{\hat{\mu}_\s}{\xbeta}
X_{\s, \s'}
\frac{[y_\s - b'(\xbeta)]}{\av{y_\s, y_\s}}
 \pder{\hat{\mu}_\s}{\xbeta}
X_{\s, \s''}
}
\\
&=
 \left[\pder{\hat{\mu}_\s}{\xbeta}\right]^2
\frac{X_{\s, \s'} X_{\s, \s''}}{\av{y_\s, y_\s}}
\end{align}

\beq
\sum_{\s}
\av{\pder{\call_{\s}}{\beta_ { \s'}}
\pder{\call_{\s}}{\beta_ { \s''}} }
=
(X^T W X)_{\s'', \s'}
\eeq

\beq
\av{\frac{\partial^2\call_\s}{\partial\beta_ { \s'}\partial\beta_ { \s''}} }
=
-\av{\pder{\call_\s}{\beta_ { \s'}}
\pder{\call_\s}{\beta_ { \s''}} }
\eeq
Summing both sides over $\s$

\beq
\av{\frac{\partial^2\call}{\partial\beta_ { \s'}\partial\beta_ { \s''}} }
=
-(X^T W X)_{\s'', \s'}
\eeq



\beq
\cali = X^T W X
\eeq
$\cali$ is called the {\bf information matrix}

\beq
\av{\hat{\beta}, \hat{\beta}^T}\rarrow \cali^{-1}
\eeq

\qed
