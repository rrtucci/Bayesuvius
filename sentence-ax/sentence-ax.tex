\chapter{Sentence Splitting with SentenceAx}
\label{ch-sentence-ax}

%        Assume:
%        batch_size= 24, s_{ba}
%        hidden_size= 768,  d
%        NUM_ILABELS= 6, n_{il}
%        ILABELLING_DIM= 30
%        \Lam=2 iterative layers
%		D=5 number of depths.
%
%        Below we show the shape of the input and output tensors for each layer.
%
%        LINES for depth=0
%        LINES for depth=1
%        LINES for depth=2
%        LINES for depth=3
%        LINES for depth=4
%
%        where LINES=
%        encoding_layer: [24, 84, 6]->[24, 105, 768]
%        *****iterative layer 0: [24, 105, 768]->[24, 105, 768]
%        dropout: [24, 105, 768]->[24, 105, 768]
%        bunch of torch operations: [24, 105, 768]->[24, 84, 768]
%        merge layer: [24, 84, 768]->[24, 84, 300]
%        ilabelling_layer: [24, 84, 300]->[24, 84, 6]
%        encoding_layer: [24, 84, 6]->[24, 105, 768]
%        *****iterative layer 1:  [24, 105, 768]->[24, 105, 768]
%        dropout: [24, 105, 768]->[24, 105, 768]
%        bunch of torch operations: [24, 105, 768]->[24, 84, 768]
%        merge layer: [24, 84, 768]->[24, 84, 300]
%        ilabelling_layer: [24, 84, 300]->[24, 84, 6]


The SentenceAx (Sax) software (at github repo Ref.\cite{sentence-ax-github}) is a complete re-write of the Openie6 (O6) software
(at github repo Ref.\cite{openie6-github}).
Sax is $99\%$ identical algorithmically to O6 but it's packaged in what we hope is a friendlier form.
 The O6 software is described by its creators
 in the paper Ref.\cite{openie6-paper},
 which we will henceforth refer to as 
 the O6 paper.

 Sax is a fine tuning of the BERT model.
 What this means in the
 language of Bayesian
 Networks is that we use
 BERT as a prior
 probability.
 The BERT model is the encoder part of the
 vanilla Transformer network which
 we discuss in Chapter \ref{ch-transformer}.

 This chapter describes the technical
 aspects of Sax. Although this chapter
 can be read without reading the O6 paper, we highly recommend to
 our readers that they read the O6 paper also.
 Some parts of this chapter are taken almost verbatim
 from the O6 paper. Other parts try to fill-in gaps in the
 explanations provided by the O6 paper or to improve those explanations. Yet others parts describe small changes that we made to the O6 software, in an effort to improve its clarity.




 In this chapter, we
 will use the Numpy-like tensor notation
 discussed in Section
 \ref{sec-numpy-tensors}. In particular, note that $[n] = [0:n] = \{0, 1,\ldots, n-1\}$ and that $T^{[n], [m]}$ is an $n\times m$ matrix.

\section{Preliminary Conventions}

\subsection{Tensor Notation}
Our tensor notation is discussed in Section
\ref{sec-numpy-tensors}.
Here is a quick review
of some of the more essential
facts in that section on tensors.
Below will often accompany
  an equation in tensor
  component notation
  with, in parenthesis, the equivalent matrix equation.



\begin{itemize}

\item{\bf reshaping}

\beq
T^{\nu, \delta}\rarrow T^{\Delta}
\;\;
\left(
T^{[n_\rvh], [d]} \rarrow T^{[D]}
\right)
\eeq

\beq
T^{\Delta}\rarrow T^{\nu, \delta}
\;\;
\left(
T^{[D]}\rarrow T^{[n_\rvh], [d]}
\right)
\eeq

\item {\bf concatenation}
\beq
T^{[n]}= (T^0, T^1,\ldots, T^{n-1})=
(T^\nu)_{\nu\in[n]}
\eeq

\item {\bf Hadamard product (element-wise, entry-wise multiplication)}
\beq
T^{[n]}* S^{[n]}= (T^\nu S^\nu)_{\nu\in[n]}
\eeq


\item {\bf Matrix multiplication}

$T^{[n]}= T^{[n], [1]}$ is a column vector.

\beq
(T^{[n]})^T S^{[n]}=\text{scalar}
\eeq

\beq
T^{[a],[b]}S^{[b], [c]}
=\left[\sum_{\beta\in[b]} T^{\alp, \beta}
S^{\beta, \gamma}\right]
_{\alp_\in [a], \gamma \in [c]}
\eeq
Most treatments of tranets, including the
the O6 paper and PyTorch,  order the
operations chronologically from
left to right. So if $A$ occurs before $B$,
they write $AB$.
This is contrary
to what is done in Linear Algebra, where one
orders the operations chronologically from right to left, and one writes $BA$.
We will adhere to the Linear Algebra
convention, since it is so prevalent
and is the overwhelming precedent.
\end{itemize}


\subsection{PyTorch conventions}



\begin{itemize}
\item {\bf Linear}

Some pseudo-code
\begin{mdframed}[hidealllines=true,backgroundcolor=blue!10]
{\tt
lin = nn.Linear(b, a)

$y^{[n], [b]}$ = lin($x^{[n],[a]}$)
}
\end{mdframed}
In the L2R (left to right) convention followed by PyTorch
\beq
x^{\nu, [a]}\rarrow y^{\nu, [b]}=x^{\nu, [a]}W^{[a], [b]}
\eeq
for all  $\nu\in[n]$. Alternatively, in
the R2L convention followed in Linear Algebra,
\beq
x^{[a], \nu}\rarrow y^{[b], \nu}=W^{[b], [a]}
x^{ [a], \nu}
\eeq
 
Note that in PyTorch, the rightmost index of the input is the 
one that is summed over.

The weights matrix $W^{[b], [a]}$ is learned by training.

\item {\bf Dropout}

Some pseudo-code
\begin{mdframed}[hidealllines=true,backgroundcolor=blue!10]
{\tt
dropo = nn.Dropout($p_{drop}$)

$y^{[n], [b]}$ = dropo($x^{[n],[a]}$)
}
\end{mdframed}


\beq
x^{\nu, [a]}\rarrow y^{\nu, [b]}=x^{\nu, [a]}\calw^{[a], [b]}
\;\;\text{(in L2R)}
\eeq
\beq
x^{\nu, [a]}\rarrow y^{\nu, [b]}=
\calw^{[b], [a]}x^{\nu, [a]}
\;\;
\text{(in R2L)}
\eeq
for all  $\nu\in[n]$.

{\tt Dropout} learns a weight matrix $W$ just like
{\tt Linear}. But at the end of the
training,
{\tt  Dropout} flips a coin
for each row of $W^{[b], [a]}$ (resp., column of $W^{[a], [b]}$), with $P(Heads)=p_{drop}, P(Tails)=1-p_{drop}=p_{keep}$. If the coin lands on T, it keeps that $W^{[b], [a]}$ (resp., column of $W^{[a], [b]}$), whereas if lands on H,
it sets that row (res., column) to zero. Then the
matrix  is
divided by $p_{keep}$.
The final matrix after all these operations  is denoted by $\calw$.



\item {\bf Embedding}

Some pseudo-code
\begin{mdframed}[hidealllines=true,backgroundcolor=blue!10]
{\tt

emb = nn.Embedding(num\_embeddings=$L$, embedding\_dim=$d$)

$Y^{[n_1], [n_2], [d]}$ =
emb($\lam^{[n_1], [n_2]}$)
}
\end{mdframed}

In Sax, we use $L=100$ and $d=768$ (for BERT base).
The $d$ is the ``hidden dimension" of BERT.
The $L$ could be as large as the vocab size
of BERT, but since we consider only
sentences with 100 words at most,
we may set $L=100$. $L$
doesn't appear in the final answer
because we sum over $\lam\in[L]$.

Next, we explain in more detail the
meaning of the tensors $\lam$ and $Y$.



Let

$L=$ number of embeddings

$d=$ embedding dimension


$\lam\in[L]$, $\alp\in[\ell]$,
$\nu_1\in [n_1]$, $\nu_2\in[n_2]$

$\ell = \nu_1 \nu_2$.

Consider a matrices $X, E, Y$ such that

\beq
Y^{\delta, \alp} = \sum_{\lam}E^{\delta, \lam}
X^{\lam, \alp}
\;\;\;
\left(
Y^{[d], [\ell]} = E^{[d], [L]} X^{[L], [\ell]}
\right)
\eeq
Assume, furthermore, that
matrix $X$ has 1-hot columns

\beq
X^{\lam, \alp}
=
\delta(\lam, \lam(\alp))
\eeq
where $\lam(): [\ell]\rarrow [L]$.

Hence,

\beq
Y^{\delta, \alp} = E^{\delta, \lam(\alp)}
\eeq


\beq
\Lam^{\alp} =\lam(\alp)
\eeq

\beq
\Lam^\alp\rarrow Y^{\delta, \alp}=E^{\delta, \lam(\alp)}
\;\;
(
\Lam^{[\ell]}
\rarrow Y^{[d], [\ell]})
\eeq

Replace $\alp$ by
$(\nu_1, \nu_2)$.

\beq
\Lam^{\nu_1, \nu_2}\rarrow Y^{\delta, \nu_1, \nu_2}=E^{\delta, \lam(\nu_1, \nu_2)}
\;\;
(
\Lam^{[n_1], [n_2]}
\rarrow Y^{[d], [n_1], [n_2]})
\eeq
Actually, {\tt emb()} orders the tensor
indices of the output so that the $\delta$ index is on the right side rather than the left side of the input indices. Thus,

\beq
Y^{[n_1], [n_2], [d]}={\tt emb}(
\Lam^{[n_1], [n_2]})
\eeq



\item{\bf Cross Entropy Loss}

Some pseudo-code
\begin{mdframed}[hidealllines=true,backgroundcolor=blue!10]
{\tt

loss = nn.CrossEntropyLoss()

output = loss(input=$x^{[n_\rvc], [n_\rvs]}$, target=$t^{[n_\rvs]}$)
}
\end{mdframed}


{\bf Cross Entropy}
in Information Theory

\beqa
H(P_{tar}^\s, P_{in}^\s)
&=&
-\sum_{\gamma\in[n_\rvc]}P_{tar}(\gamma|\s) \ln P_{in}(\gamma|\s)
\\
&=&-\sum_{\gamma\in[n_\rvc]}P_{tar}(\gamma|\s) \ln
\left[\frac{P_{in}(\gamma|\s)}{P_{tar}(\gamma|\s)}
P_{tar}(\gamma|\s)\right]
\\
&=&
H(P_{tar}^\s) + D_{KL}(P_{in}^\s\parallel P_{tar}^\s)
\eeqa
{\bf Cross Entropy Loss} in PyTorch

Let

$n_\rvs=$ total number of samples being considered,
usually batch size.
$\s\in [n_\rvs]$

$n_\rvc=$ number of classes in classification. $\gamma\in[n_\rvc]$


$x^{[n_\rvc], [n_\rvs]}=$ input  samples

$t^{[n_\rvs]}=$ target samples

\beqa
P_{in}(\gamma|\s)&=&
\frac{\exp(x^{\gamma, \s})}
{\sum_{\gamma'\in[n_\rvc]}\exp(x^{\gamma', \s})}
\\
&=&
{\rm softmax}(x^{[n_\rvc], \s})(\gamma|\s)
\eeqa

Suppose $W^\gamma:values(\rvt)\rarrow[0,1]$
for all $\gamma\in[n_\rvc]$.

\beq
P_{tar}(\gamma|\s)=
\frac{
W^\gamma (t^{\s})\indi(t^{\s}\neq -100)}
{\sum_{\gamma\in[n_\rvc]}numerator}
\eeq

-100 can be replaced by any other integer
in $values(\rvt)$ for which we want the loss to be zero (for example, an integer used for padding)




\beq
\call_{CE}^\s(t^{\s}, x^{[n_\rvc], \s})=
H(P_{tar}(\cdot|\s), P_{in}(\cdot|\s))
\eeq


\beq
\call_{CE} = \frac{1}{n_\rvs} \sum_{\s\in[n_\rvs]}\call_{CE}^\s
\eeq

For example, if $W^\gamma=1$, and $n_\rvc=2$,

\beqa
\call_{CE} &=& \frac{1}{n_\rvs}\sum_{\s}
\left[
P_{tar}(0|\s)\ln P_{in}(0|\s)
+
P_{tar}(0|\s)\ln (1-P_{in}(0|\s))
\right]
\eeqa

\item {\bf unsqueeze-repeat-gather}

Some pseudo-code
\begin{mdframed}[hidealllines=true,backgroundcolor=blue!10]
\begin{verbatim}
lll_loc = ll_loc0.unsqueeze(2).\
    repeat(1, 1, lll_state.shape[2])
lll_out = torch.gather(
    input=lll_state, dim=1, index=lll_loc)
\end{verbatim}
\end{mdframed}
Sax uses trio of operations unsqueeze-repeat-gather
in the manner of the above pseudo-code. We have already 
discussed in Section \ref{sec-numpy-tensors}
how each of these 3 operations
acts individually . Here we will discuss how they
act jointly, when used as in the above
pseudo-code.

Let

{\tt lll\_state=} $S^{[s_{ba}],[a], [d]}$

{\tt ll\_loc0=} $L_0^{[s_{ba}], [a]}$

{\tt lll\_loc=} $L^{[s_{ba}], [b], [d]}$

{\tt lll\_out=} $O^{[s_{ba}], [b], [d]}$

$\s\in s_{ba}, \alp\in[a], \beta\in[b], \delta\in[d]$

{\tt unsqueeze(2)} takes 

\beq
L_0^{[s_{ba}], [a]}\rarrow L_0^{[s_{ba}], [a], 0}
\eeq

{\tt lll\_state.shape[2]} equals $d$, and {\tt repeat(1, 1, d)}
takes

\beq
L_0^{[s_{ba}], [a], 0}\rarrow
L^{[s_{ba}], [a], [d]}= 
(\underbrace{L_0^{[s_{ba}], [a], 0}, 
\ldots,
L_0^{[s_{ba}], [a], 0}}_
{\text{$d$ times}}) 
\eeq

Let 

\beq
\lam(\s, \alp)
=
L^{\s, \alp, \delta}=
L_0^{\s, \alp}
\eeq 
Then the {\tt gather()} 
with {\tt dim=1} outputs

\beq 
O^{\s, \alp, \delta}=
S^{\s, \lam(\s, \alp), \delta}
\eeq





\end{itemize}

\section{Bayesian Network for this model}

Let

$\ell_{pad}=86$, padding length, for this batch

$\ell_{enc}=121$, encoded length, for this batch, $\ell_{enc}\geq \ell_{pad}$

$n_{dep}=5$, number of copies of solid box connected in series, number of depths

 $n_{att}=2$, number of copies of
dashed box connected in series, number of iterative (attention) layers.


$d=768$, hidden dimension per head

$n_\rvh$, number of heads (BERT base)

$D=d n_\rvh$, hidden dimension
for all heads


$s_{ba}=24$, batch size

$n_{il}=6$, number of ilabels

$d_{me}=300$, merge dimension

Fig.\ref{fig-texnn-for-sentence-ax-bnet}
shows the bnet  for Sax.\footnote{The
bnet of Fig.\ref{fig-texnn-for-sentence-ax-bnet}
was drawn with the help of the texnn software (Ref.\cite{texnn})}. The structural equations, printed in 
blue, for that bnet, are as follows.

\begin{figure}[h!]\centering
$$\xymatrix@R=2.5pc@C=3.5pc{
&&
\\
&&
\\
&*+[F*:Dandelion]{\underline{M}^{[86], [300]}}\ar[uu]\ar[r]^{W_{il}}&*+[F*:SkyBlue]{\underline{L}^{[86], [6]}}\ar[uu]
\\
*+[F*:pink]{\underline{S}^{[86], [768]}}\ar[ur]^{W_{me}}&*+[F*:pink]{\underline{E}^{[86], [768]}}\ar[l]^{1}&*+[F*:yellow]{\underline{a}^{[86]}}\ar[l]
\\
*+[F*:Orchid]{\underline{d}^{[121], [768]}}\ar[r]&*+[F*:pink]{\underline{G}^{[86], [768]}}\ar@[red]@/_5pc/[uu]|-{\color{red} W_{me}}\ar[ul]^{1}&
\\
&*+[F*:Orchid]{\underline{I}^{[121], [768]}}\ar[ul]&
\\
&*+[F*:Orchid]{\underline{B}^{[121], [768]}}\ar[u]&*+[F*:SkyBlue]{\underline{X}^{[86], [6]}}\ar[uuu]
\save
\POS"3,1"."6,1"."3,3"."6,3"!C*+<4.8em>\frm{-,}
\POS"6,2"."6,2"."6,2"."6,2"!C*+<3.8em>\frm{--}
\POS"4,1"."4,1"."4,3"."4,3"!C*+<1.0em>\frm{.}
\restore
}$$
\caption{Sax bnet. 2 copies of dashed box are connected in series. 5 copies (5 depths) of plain box are connected in series.  However, in the first of those 5 plain box copies, the dotted box  is omitted and node $\ul{G}$ feeds directly into node  $\ul{M}$ (indicated by red arrow). We display the tensor shape superscripts in the PyTorch L2R order. All tensor shape superscripts have been simplified by omitting a $[s_{ba}]$ from their left side, where $s_{ba}=24$ is the batch size. }
\label{fig-texnn-for-sentence-ax-bnet}
\end{figure}

\begin{tabular}{ll}
$\underline{a}^{[86]}$ :&{\tt ll\_greedy\_ilabel}\\
$\underline{B}^{[121], [768]}$ :&{\tt lll\_hidstate}\\
$\underline{d}^{[121], [768]}$ :&{\tt lll\_hidstate}\\
$\underline{E}^{[86], [768]}$ :&{\tt lll\_pred\_code}\\
$\underline{G}^{[86], [768]}$ :&{\tt lll\_word\_hidstate}\\
$\underline{I}^{[121], [768]}$ :&{\tt lll\_hidstate}\\
$\underline{L}^{[86], [6]}$ :&{\tt lll\_word\_score}\\
$\underline{M}^{[86], [300]}$ :&{\tt lll\_word\_hidstate}\\
$\underline{S}^{[86], [768]}$ :&{\tt lll\_word\_hidstate}\\
$\underline{X}^{[86], [6]}$ :&{\tt lll\_word\_score}
\end{tabular}



\begin{subequations}

\begin{equation}\color{blue}
\begin{aligned}
a^{[86]} &= \text{argmax}(X^{[86], [6]};dim=-1)
\label{eq-a-fun-sentence-ax-bnet}
\\ &:{\tt ll\_greedy\_ilabel}
\end{aligned}
\end{equation}

\begin{equation}\color{blue}
\begin{aligned}
B^{[121], [768]} &= \text{BERT}()
\label{eq-B-fun-sentence-ax-bnet}
\\ &:{\tt lll\_hidstate}
\end{aligned}
\end{equation}

\begin{equation}\color{blue}
\begin{aligned}
d^{[121], [768]} &= \text{dropout}(I^{[121], [768]})
\label{eq-d-fun-sentence-ax-bnet}
\\ &:{\tt lll\_hidstate}
\end{aligned}
\end{equation}

\begin{equation}\color{blue}
\begin{aligned}
E^{[86], [768]} &= \text{embedding}(a^{[86]})
\label{eq-E-fun-sentence-ax-bnet}
\\ &:{\tt lll\_pred\_code}
\end{aligned}
\end{equation}

\begin{equation}\color{blue}
\begin{aligned}
G^{[86], [768]} &= \text{gather}(d^{[121], [768]};dim=-2)
\label{eq-G-fun-sentence-ax-bnet}
\\ &:{\tt lll\_word\_hidstate}
\end{aligned}
\end{equation}

\begin{equation}\color{blue}
\begin{aligned}
I^{[121], [768]} &= \left[B^{[121], [768]}\indi(depth=0) M^{[86], [300]}\indi(depth> 0)\right]
\label{eq-I-fun-sentence-ax-bnet}
\\ &:{\tt lll\_hidstate}
\end{aligned}
\end{equation}

\begin{equation}\color{blue}
\begin{aligned}
L^{[86], [6]} &= M^{[86], [300]}W_{il}^{[300],[6]}
\label{eq-L-fun-sentence-ax-bnet}
\\ &:{\tt lll\_word\_score}
\end{aligned}
\end{equation}

\begin{equation}\color{blue}
\begin{aligned}
M^{[86], [300]} &= \left[G^{[86], [768]}\indi(depth=0) + S^{[86], [768]}\indi(depth> 0) \right] W_{me}^{[768], [300]}
\label{eq-M-fun-sentence-ax-bnet}
\\ &:{\tt lll\_word\_hidstate}
\end{aligned}
\end{equation}

\begin{equation}\color{blue}
\begin{aligned}
S^{[86], [768]} &= E^{[86], [768]} + G^{[86], [768]}
\label{eq-S-fun-sentence-ax-bnet}
\\ &:{\tt lll\_word\_hidstate}
\end{aligned}
\end{equation}

\begin{equation}\color{blue}
\begin{aligned}
X^{[86], [6]} &= L^{[86], [6]}\indi(depth> 0)
\label{eq-X-fun-sentence-ax-bnet}
\\ &:{\tt lll\_word\_score}
\end{aligned}
\end{equation}

\end{subequations}



\section{Loss $\call$ for this model}

The Loss $\call$ is the sum of the
Cross Entropy Loss $\call_{CE}$ and 4 penalty losses $\call_i$ for $i\in PL$ where
$PL=\{ POSC, HVC, HVE, EC\}$.


\beq
\call = \call_{CE} +
\sum_{i\in PL} \lam_{i}\call_i
\eeq

The $\lam_i$ are hyper-parameters.

Next we will define the CE loss and the 4 penalty losses.




Penalty Losses

Below, we will use the standard
notation for the positive-part function (a.k.a.
the reLU function)

\beqa
(x)_+ &=&
\left\{
\begin{array}{ll}
x& \text{ if } x\geq0
\\
0& \text{ if } x< 0
\end{array}
\right.
\\
&=& {\rm max}(0, x)
\eeqa
Since loss is supposed to be bounded below
 (usually loss is greater or equal to zero),
 the positive-part function arises frequently  when defining a loss.



Let

$\ell=$ number of words, length of sentence. $\alp \in [\ell]$

$M=$ number of depths. $\mu\in[M]$

$w^\alp=$ word at position $\alp$

$T_{pos}=\{N, V, JJ, RB\}$, POS tags, N=Noun, V=Verb, JJ=Adjective, RB=Adverb

$T_{ex}=\{ N, R, O, S\}$\footnote{
The O6 and Sax software use
a different set for $T_{ex}$. In Sax, we use for
$T_{ex}$ the list {\tt BASE\_EXTAGS} (defined globally
in the file {\tt Sax\_globals}.)
In {\tt BASE\_EXTAGS}, N becomes NONE (or 0)
and R becomes REL (or 3).
Also, 2 tranets are trained by O6 and Sax,
one for extraction (task=ex), and one for splitting (task=cc).
For splitting, $T_{ex}$ is replaced by $T_{cc}$. In Sax, we use for
$T_{cc}$ the list {\tt BASE\_CCTAGS}  (defined globally
in the file {\tt Sax\_globals}.)
In {\tt BASE\_CCTAGS}, N becomes NONE (or 0)
and R becomes CC (or 3).}. extraction tags (extags),  S=Subject, R=Relation, O=Object, N=None


$T_{ex}\backslash N = T_{ex}-\{N\}$


$POS^\alp\in T_{pos}$, Part Of Sentence of $w^\alp$.



Importance indicator function.
\beq
IMP^\alp = \indi(POS^\alp \in T_{pos})
\eeq

Head verb indicator function. A {\bf head verb} is any verb that isn't a {\bf light verb}
(do, be, is, has, etc.).

\beq
HV^\alp = \indi(w^\alp \text{ is a head verb})
\eeq

We will
derive from the data an empirical probability $P(\rvt^{\mu, \alp} =t )$ for a table element
$\rvt^{\mu, \alp}\in T_{ex}$, for all $\mu\in [M]$ and $\alp\in[\ell]$.

the O6 paper describes how the 4 penalty losses
deal with the following sentence, which we will refer to
henceforth as the red-sent:

\begin{quote}\color{red}
Obama gained popularity after
Oprah endorsed him for the presidency.
\end{quote}

For the red-sent, the head verbs are {\color{red} gained, endorsed}

Two valid extractions from red-sent are:
{\color{red}(Obama; gained; popularity)}
and {\color{red}(Oprah; endorsed him for;
 the presidency)}.


\begin{enumerate}

\item {\bf Part of Speech Coverage (POSC)}

All words important words should be part of at least one extraction.

In red-sent: the words {\color{red}Obama, gained, popularity,
Oprah, endorsed, presidency} must be covered in
the set of extractions.

\beq
\call_{POSC}=\sum_{\alp\in [\ell]}IMP^{\alp}P_{POSC}(\alp)
\eeq

\beq
P_{POSC}(\alp)=
1-{\rm max}_{\mu\in [M]}
{\rm max}_{t\in T_{ex}\backslash N}P(\rvt^{\mu, \alp}=t)
\eeq

\item {\bf Head Verb Coverage (HVC)}

Each head verb
should be present in the relation (R) span of some
(but not too many) extractions.\footnote{A span is a list of contiguous words.}

In red-sent: {\color{red} (Obama;
gained; popularity), (Obama; gained; presidency)} is not a comprehensive set of extractions.

\beq
\call_{HVC}=
\sum_{\alp\in [\ell]}
HV^\alp P_{HVC}(\alp)
\eeq

\beq
P_{HVC}(\alp)=
\left|
1-\sum_{\mu\in [M]}P(\rvt^{\mu, \alp}=R)
\right|
\eeq

\item {\bf Head Verb Exclusivity (HVE)}

The relation (R) span
of one extraction can contain at most one head
verb.

In red-sent: {\color{red}gained popularity after Oprah endorsed} is not a good relation as it contains two
head verbs

\beq
\call_{HVE}=
\sum_{\mu\in [M]}
\left(
\sum_{\alp\in [\ell]}
HV^\alp P(\rvt^{\mu, \alp}=R)
-1
\right)_+
\eeq

\item {\bf Extraction Count (EC)}

The total number of extractions with head verbs in the relation (R) span
must be no fewer than the number of head verbs
in the sentence.

\beq
\call_{EC}=
\left(
\sum_{\alp\in[\ell]} HV^\alp
-
\sum_{\mu\in [M]}EC^\mu
\right)_+
\eeq

\beq
EC^\mu=
{\rm max}_{\alp\in [\ell]}
HV^\alp P(\rvt^{\mu, \alp}=R)
\eeq


\end{enumerate}
