\chapter{Sentence Splitting with SentenceAx}
\label{ch-sentence-ax}

The Openie6 (O6) software
(at github repo Ref.\cite{openie6-github})
splits complex or compound 
sentences into
simple ones.\footnote{Simple sentences are essentially
the same  as the triples (subject, relationship, object) which when visualized as a directed or undirected graph
is called a  ``knowledge graph".} This is a necessary step in doing 
DAG extraction from text (DEFT) (See Chapter \ref{ch-deft}).

The O6 software is described by its creators
 in the paper Ref.\cite{openie6-paper},
 which we will henceforth refer to as 
 the O6 paper.

The SentenceAx (Sax) software (at github repo Ref.\cite{sentence-ax-github}) is a complete re-write of 
the  O6 software.
Sax is $95\%$ identical algorithmically to O6, but it has been rewritten in what we hope is a friendlier form.
 

 Sax is a fine tuning of the BERT model.
 What this means in the
 language of Bayesian
 Networks is that we use
 BERT as a prior
 probability.
 The BERT model is the encoder part of the
 vanilla Transformer network which
 we discuss in Chapter \ref{ch-transformer}.

 This chapter describes the technical
 aspects of Sax. Although this chapter
 can be read without reading the O6 paper, we highly recommend to
 our readers that they read the O6 paper also.
 Some parts of this chapter are taken almost verbatim
 from the O6 paper. Other parts try to fill-in gaps in the
 explanations provided by the O6 paper or to improve those explanations. Yet others parts describe small changes that we made to the O6 software, in an effort to improve its clarity.




 In this chapter, we
 will use the Numpy-like tensor notation
 discussed in Section
 \ref{sec-numpy-tensors}. In particular, note that $[n] = [0:n] = \{0, 1,\ldots, n-1\}$ and that $T^{[n], [m]}$ is an $n\times m$ matrix.

\section{Preliminary Conventions}

\subsection{Tensor Notation}
Our tensor notation is discussed in Section
\ref{sec-numpy-tensors}.
Here is a quick review
of some of the more salient
facts in that section on tensors.
Below, we will often accompany
  an equation in tensor
  component notation
  with the equivalent matrix equation,
  in parenthesis.
  
We use  
Greek letters for 
tensor indices.

Let $\alp\in[a]$, $\beta\in[b]$, $\gamma\in[c]$,
$\delta \in[d]$,
$\nu\in[n]$, $\Delta\in[D]$.


\begin{itemize}

\item{\bf reshaping}

\beq
T^{\nu, \delta}\rarrow T^{\Delta}
\;\;
\left(
T^{[n_\rvh], [d]} \rarrow T^{[D]}
\right)
\eeq

\beq
T^{\Delta}\rarrow T^{\nu, \delta}
\;\;
\left(
T^{[D]}\rarrow T^{[n_\rvh], [d]}
\right)
\eeq

\item {\bf concatenation}
\beq
T^{[n]}= (T^0, T^1,\ldots, T^{n-1})=
(T^\nu)_{\nu\in[n]}
\eeq

\item {\bf Hadamard product (element-wise, entry-wise multiplication)}
\beq
T^{[n]}* S^{[n]}= (T^\nu S^\nu)_{\nu\in[n]}
\eeq


\item {\bf Matrix multiplication}

$T^{[n]}= T^{[n], [1]}$ is a column vector.

\beq
(T^{[n]})^T S^{[n]}=\text{scalar}
\eeq

\beq
T^{[a],[b]}S^{[b], [c]}
=\left[\sum_{\beta\in[b]} T^{\alp, \beta}
S^{\beta, \gamma}\right]
_{\alp_\in [a], \gamma \in [c]}
\eeq

\end{itemize}

Most treatments of Transformer Networks (tranets), including the
the O6 paper and PyTorch,  order the
operations chronologically from
left to right (L2R). So if $A$ occurs before $B$,
they write $AB$.
This is contrary
to what is done in Linear Algebra, where one
orders the operations chronologically from right to left (R2L), and one writes $BA$.
In Chapter \ref{ch-transformer}
on tranets,
we followed the Linear Algebra
convention.
In this chapter,
we will
follow the PyTorch
convention,
because Sax is written with 
PyTorch
so it uses the PyTorch convention.


\subsection{PyTorch conventions}



\begin{itemize}
\item {\bf Linear}

Some pseudo-code
\begin{mdframed}[hidealllines=true,backgroundcolor=blue!10]
{\tt
lin = nn.Linear(b, a)

$y^{[n], [b]}$ = lin($x^{[n],[a]}$)
}
\end{mdframed}
In the L2R (left to right) convention followed by PyTorch
\beq
x^{\nu, [a]}\rarrow y^{\nu, [b]}=x^{\nu, [a]}W^{[a], [b]}
\eeq
for all  $\nu\in[n]$. Alternatively, in
the R2L convention followed in Linear Algebra,
\beq
x^{[a], \nu}\rarrow y^{[b], \nu}=W^{[b], [a]}
x^{ [a], \nu}
\eeq
 
Note that in PyTorch, the rightmost index of the input is the 
one that is summed over.

The weights matrix $W^{[b], [a]}$ is learned by training.

\item {\bf Dropout}

Some pseudo-code
\begin{mdframed}[hidealllines=true,backgroundcolor=blue!10]
{\tt
dropo = nn.Dropout($p_{drop}$)

$y^{[n], [a]}$ = dropo($x^{[n],[a]}$)
}
\end{mdframed}


\beq
x^{\nu, [a]}\rarrow y^{\nu, [a]}=x^{\nu, [a]}\calw_R^{[a], [a]}
\;\;\text{(in L2R)}
\eeq
\beq
x^{\nu, [a]}\rarrow y^{\nu, [a]}=
\calw_L^{[a], [a]}x^{\nu, [a]}
\;\;
\text{(in R2L)}
\eeq
for all  $\nu\in[n]$.

{\tt Dropout} learns a weight matrix $W$ just like
{\tt Linear}. But at the end of the
training,
{\tt  Dropout} flips a coin
for each row of $W_L^{[a], [a]}$ (resp., column of $W_R^{[a], [a]}$), with $$P(Heads)=p_{drop}, \text{ and } P(Tails)=1-p_{drop}=p_{keep}.$$If the coin lands on T, it keeps that row of $W_L^{[a], [a]}$ (resp., column of $W_R^{[a], [a]}$), whereas if lands on H,
it sets that row (resp., column) to zero. Then the
matrix  is
divided by $p_{keep}$.
The final matrix after all these operations  is denoted by $\calw_L$ (resp., $\calw_R$).



\item {\bf Embedding}

Some pseudo-code
\begin{mdframed}[hidealllines=true,backgroundcolor=blue!10]
{\tt

emb = nn.Embedding(num\_embeddings=$L$, embedding\_dim=$d$)

$Y^{[n_1], [n_2], [d]}$ =
emb($\lam^{[n_1], [n_2]}$)
}
\end{mdframed}

In Sax, we use $L=100$ and $d=768$ (for BERT base).
The $d$ is the ``hidden dimension" of BERT.
The $L$ could be as large as the vocab size
of BERT, but since we consider only
sentences with 100 words at most,
we may set $L=100$. $L$
doesn't appear in the final answer
because we sum over $\lam\in[L]$.

Next, we explain in more detail the
meaning of the tensors $\lam$ and $Y$.



Let

$L=$ number of embeddings

$d=$ embedding dimension


$\lam\in[L]$, $\alp\in[\ell]$,
$\nu_1\in [n_1]$, $\nu_2\in[n_2]$

$\ell = \nu_1 \nu_2$.

Consider  matrices $Y, E, X$ such that

\beq
Y^{\delta, \alp} = \sum_{\lam}E^{\delta, \lam}
X^{\lam, \alp}
\;\;\;
\left(
Y^{[d], [\ell]} = E^{[d], [L]} X^{[L], [\ell]}
\right)
\eeq
Assume that
matrix $X$ has 1-hot columns

\beq
X^{\lam, \alp}
=
\delta(\lam, \lam(\alp))
\eeq
where $\lam(): [\ell]\rarrow [L]$.

Hence,

\beq
Y^{\delta, \alp} = E^{\delta, \lam(\alp)}
\eeq

If we define
\beq
\Lam^{\alp} =\lam(\alp)
\eeq
then {\tt emb()}
maps

\beq
\Lam^\alp\rarrow Y^{\delta, \alp}=E^{\delta, \lam(\alp)}
\;\;
(
\Lam^{[\ell]}
\rarrow Y^{[d], [\ell]})
\eeq

 Now replace $\alp$ by
$(\nu_1, \nu_2)$.
{\tt emb()}
also maps

\beq
\Lam^{\nu_1, \nu_2}\rarrow Y^{\delta, \nu_1, \nu_2}=E^{\delta, \lam(\nu_1, \nu_2)}
\;\;
(
\Lam^{[n_1], [n_2]}
\rarrow Y^{[d], [n_1], [n_2]})
\eeq
Actually, {\tt emb()} orders the tensor
indices of the output so that the $\delta$ index is on the right side rather than the left side of the input indices. Thus,

\beq
Y^{[n_1], [n_2], [d]}={\tt emb}(
\Lam^{[n_1], [n_2]})
\eeq



\item{\bf Cross Entropy Loss}

Some pseudo-code
\begin{mdframed}[hidealllines=true,backgroundcolor=blue!10]
{\tt

loss = nn.CrossEntropyLoss()

output = loss(input=$x^{[n_\rvc], [n_\rvs]}$, target=$t^{[n_\rvs]}$)
}
\end{mdframed}


{\bf Cross Entropy}
in Information Theory:

\beqa
H(P_{tar}^\s, P_{in}^\s)
&=&
-\sum_{\gamma\in[n_\rvc]}P_{tar}(\gamma|\s) \ln P_{in}(\gamma|\s)
\\
&=&-\sum_{\gamma\in[n_\rvc]}P_{tar}(\gamma|\s) \ln
\left[\frac{P_{in}(\gamma|\s)}{P_{tar}(\gamma|\s)}
P_{tar}(\gamma|\s)\right]
\\
&=&
H(P_{tar}^\s) + D_{KL}(P_{in}^\s\parallel P_{tar}^\s)
\eeqa
{\bf Cross Entropy Loss} in PyTorch:

Let

$n_\rvs=$ total number of samples being considered (usually batch size).
$\s\in [n_\rvs]$

$n_\rvc=$ number of classes in classification. $\gamma\in[n_\rvc]$


$x^{[n_\rvc], [n_\rvs]}=$ input  samples

$t^{[n_\rvs]}=$ target samples

Define 
\beqa
P_{in}(\gamma|\s)&=&
\frac{\exp(x^{\gamma, \s})}
{\sum_{\gamma'\in[n_\rvc]}\exp(x^{\gamma', \s})}
\\
&=&
{\rm softmax}(x^{[n_\rvc], \s})(\gamma|\s)
\eeqa

Suppose $W^\gamma:values(\rvt)\rarrow[0,1]$
for all $\gamma\in[n_\rvc]$.

Define

\beq
P_{tar}(\gamma|\s)=
\frac{
W^\gamma (t^{\s})\indi(t^{\s}\neq -100)}
{\sum_{\gamma\in[n_\rvc]}numerator}
\eeq

The -100 
on the right side of the last
equation can be replaced by any other integer
in $values(\rvt)$ for which we want the loss to be zero (for example, it could be an integer used for padding)

Now define
the cross entropy loss $\call_{CE}$ by


\beq
\call_{CE}^\s(t^{\s}, x^{[n_\rvc], \s})=
H(P_{tar}(\cdot|\s), P_{in}(\cdot|\s))
\eeq


\beq
\call_{CE} = \frac{1}{n_\rvs} \sum_{\s\in[n_\rvs]}\call_{CE}^\s
\eeq

For example, if $W^\gamma=1$, and $n_\rvc=2$,

\beqa
\call_{CE} &=& \frac{1}{n_\rvs}\sum_{\s\in[n_\rvs]}
\left[
P_{tar}(0|\s)\ln P_{in}(0|\s)
+
P_{tar}(1|\s)\ln P_{in}(1|\s)
\right]
\eeqa

\item {\bf unsqueeze-repeat-gather}

Some pseudo-code
\begin{mdframed}[hidealllines=true,backgroundcolor=blue!10]
\begin{verbatim}
lll_loc = ll_loc0.unsqueeze(2).\
    repeat(1, 1, lll_state.shape[2])
lll_out = torch.gather(
    input=lll_state, dim=1, index=lll_loc)
\end{verbatim}
\end{mdframed}
Sax uses the trio of operations unsqueeze-repeat-gather
in the manner of the above pseudo-code. We have already 
discussed in Section \ref{sec-numpy-tensors}
how each of these 3 operations
acts individually. Here we will discuss how they
act jointly, when used as in the above
pseudo-code.

Let

{\tt lll\_state=} $S^{[s_{ba}],[a], [d]}$

{\tt ll\_loc0=} $L_0^{[s_{ba}], [a]}$

{\tt lll\_loc=} $L^{[s_{ba}], [b], [d]}$

{\tt lll\_out=} $O^{[s_{ba}], [b], [d]}$

$\s\in s_{ba}, \alp\in[a], \beta\in[b], \delta\in[d]$

{\tt unsqueeze(2)} takes 

\beq
L_0^{[s_{ba}], [a]}\rarrow L_0^{[s_{ba}], [a], 0}
\eeq

{\tt lll\_state.shape[2]} equals $d$, and {\tt repeat(1, 1, d)}
takes

\beq
L_0^{[s_{ba}], [a], 0}\rarrow
L^{[s_{ba}], [a], [d]}= 
(\underbrace{L_0^{[s_{ba}], [a], 0}, 
\ldots,
L_0^{[s_{ba}], [a], 0}}_
{\text{$d$ times}}) 
\eeq

Now define 

\beq
\lam(\s, \alp)
=
L^{\s, \alp, \delta}=
L_0^{\s, \alp}
\eeq 
Then the {\tt gather()} 
with {\tt dim=1} outputs.

\beq 
O^{\s, \alp, \delta}=
S^{\s, \lam(\s, \alp), \delta}
\eeq





\end{itemize}

\section{Bayesian Network for this model}

Let

$\ell_{pad}=86$, padding length, for this batch

$\ell_{enc}=121$, encoded length, for this batch, $\ell_{enc}\geq \ell_{pad}$

$n_{dep}=5$, number of copies of solid box connected in series, number of depths

 $n_{att}=2$, number of copies of
dashed box connected in series, number of iterative (attention) layers.


$d=768$, hidden dimension per head

$n_\rvh$, number of heads (BERT base)

$D=d n_\rvh$, hidden dimension
for all heads


$s_{ba}=24$, batch size

$n_{il}=6$, number of ilabels

$d_{me}=300$, merge dimension

Fig.\ref{fig-texnn-for-sentence-ax-bnet}
shows the bnet  for Sax.\footnote{The
bnet of Fig.\ref{fig-texnn-for-sentence-ax-bnet}
was drawn with the help of the texnn software (Ref.\cite{texnn})}. The structural equations, printed in 
blue, for that bnet, are as follows.

\begin{figure}[h!]\centering
$$\xymatrix@R=2.5pc@C=3.5pc{
&&
\\
&&
\\
&*+[F*:Dandelion]{\underline{M}^{[86], [300]}}\ar[uu]\ar[r]^{W_{il}}&*+[F*:SkyBlue]{\underline{L}^{[86], [6]}}\ar[uu]
\\
*+[F*:pink]{\underline{S}^{[86], [768]}}\ar[ur]^{W_{me}}&*+[F*:pink]{\underline{E}^{[86], [768]}}\ar[l]^{1}&*+[F*:yellow]{\underline{a}^{[86]}}\ar[l]
\\
*+[F*:Orchid]{\underline{d}^{[121], [768]}}\ar[r]&*+[F*:pink]{\underline{G}^{[86], [768]}}\ar@[red]@/_5pc/[uu]|-{\color{red} W_{me}}\ar[ul]^{1}&
\\
&*+[F*:Orchid]{\underline{I}^{[121], [768]}}\ar[ul]&*+[F*:SkyBlue]{\underline{X}^{[86], [6]}}\ar[uu]
\\
&*+[F*:Orchid]{\underline{B}^{[121], [768]}}\ar[u]&
\save
\POS"3,1"."6,1"."3,3"."6,3"!C*+<4.8em>\frm{-,}
\POS"6,2"."6,2"."6,2"."6,2"!C*+<3.8em>\frm{--}
\POS"4,1"."4,1"."4,3"."4,3"!C*+<1.0em>\frm{.}
\restore
}$$
\caption{Sax bnet. 2 copies of dashed box are connected in series. 5 copies (5 depths) of plain box are connected in series.  However, in the first of those 5 plain box copies, the dotted box  is omitted and node $\ul{G}$ feeds directly into node  $\ul{M}$ (indicated by red arrow). We display the tensor shape superscripts in the PyTorch L2R order. All tensor shape superscripts have been simplified by omitting a $[s_{ba}]$ from their left side, where $s_{ba}=24$ is the batch size. }
\label{fig-texnn-for-sentence-ax-bnet}
\end{figure}

\begin{tabular}{ll}
$\underline{a}^{[86]}$ :&{\tt ll\_greedy\_ilabel}\\
$\underline{B}^{[121], [768]}$ :&{\tt lll\_hidstate}\\
$\underline{d}^{[121], [768]}$ :&{\tt lll\_hidstate}\\
$\underline{E}^{[86], [768]}$ :&{\tt lll\_pred\_code}\\
$\underline{G}^{[86], [768]}$ :&{\tt lll\_word\_hidstate}\\
$\underline{I}^{[121], [768]}$ :&{\tt lll\_hidstate}\\
$\underline{L}^{[86], [6]}$ :&{\tt lll\_word\_score}\\
$\underline{M}^{[86], [300]}$ :&{\tt lll\_word\_hidstate}\\
$\underline{S}^{[86], [768]}$ :&{\tt lll\_word\_hidstate}\\
$\underline{X}^{[86], [6]}$ :&{\tt lll\_word\_score}
\end{tabular}



\begin{subequations}

\begin{equation}\color{blue}
\begin{aligned}
a^{[86]} &= \text{argmax}(X^{[86], [6]};dim=-1)
\label{eq-a-fun-sentence-ax-bnet}
\\ &:{\tt ll\_greedy\_ilabel}
\end{aligned}
\end{equation}

\begin{equation}\color{blue}
\begin{aligned}
B^{[121], [768]} &= \text{BERT}()
\label{eq-B-fun-sentence-ax-bnet}
\\ &:{\tt lll\_hidstate}
\end{aligned}
\end{equation}

\begin{equation}\color{blue}
\begin{aligned}
d^{[121], [768]} &= \text{dropout}(I^{[121], [768]})
\label{eq-d-fun-sentence-ax-bnet}
\\ &:{\tt lll\_hidstate}
\end{aligned}
\end{equation}

\begin{equation}\color{blue}
\begin{aligned}
E^{[86], [768]} &= \text{embedding}(a^{[86]})
\label{eq-E-fun-sentence-ax-bnet}
\\ &:{\tt lll\_pred\_code}
\end{aligned}
\end{equation}

\begin{equation}\color{blue}
\begin{aligned}
G^{[86], [768]} &= \text{gather}(d^{[121], [768]};dim=-2)
\label{eq-G-fun-sentence-ax-bnet}
\\ &:{\tt lll\_word\_hidstate}
\end{aligned}
\end{equation}

\begin{equation}\color{blue}
\begin{aligned}
I^{[121], [768]} &= \left[B^{[121], [768]}\indi(depth=0)+ M^{[86], [300]}\indi(depth> 0)\right]
\label{eq-I-fun-sentence-ax-bnet}
\\ &:{\tt lll\_hidstate}
\end{aligned}
\end{equation}

\begin{equation}\color{blue}
\begin{aligned}
L^{[86], [6]} &= M^{[86], [300]}W_{il}^{[300],[6]}
\label{eq-L-fun-sentence-ax-bnet}
\\ &:{\tt lll\_word\_score}
\end{aligned}
\end{equation}

\begin{equation}\color{blue}
\begin{aligned}
M^{[86], [300]} &= \left[G^{[86], [768]}\indi(depth=0) + S^{[86], [768]}\indi(depth> 0) \right] W_{me}^{[768], [300]}
\label{eq-M-fun-sentence-ax-bnet}
\\ &:{\tt lll\_word\_hidstate}
\end{aligned}
\end{equation}

\begin{equation}\color{blue}
\begin{aligned}
S^{[86], [768]} &= E^{[86], [768]} + G^{[86], [768]}
\label{eq-S-fun-sentence-ax-bnet}
\\ &:{\tt lll\_word\_hidstate}
\end{aligned}
\end{equation}

\begin{equation}\color{blue}
\begin{aligned}
X^{[86], [6]} &= L^{[86], [6]}\indi(depth> 0)
\label{eq-X-fun-sentence-ax-bnet}
\\ &:{\tt lll\_word\_score}
\end{aligned}
\end{equation}

\end{subequations}


\section{Loss $\call$ for this model}

The Loss $\call$ is the sum of the
Cross Entropy Loss $\call_{CE}$ and 4 penalty losses $\call_i$ for $i\in PL$ where
$PL=\{ POSC, HVC, HVE, EC\}$.


\beq
\call = \call_{CE} +
\sum_{i\in PL} \lam_{i}\call_i
\eeq
where the $\lam_i$ are hyper-parameters
to be determined  heuristically.

In an earlier
section, we discussed 
the Cross Entropy Loss at 
length. 
In this section, we will discuss the 4 penalty losses.


Below, we will use the standard
notation for the {\bf positive-part function} (a.k.a.
the {\bf reLU} function)

\beqa
(x)_+ &=&
\left\{
\begin{array}{ll}
x& \text{ if } x\geq0
\\
0& \text{ if } x< 0
\end{array}
\right.
\\
&=& {\rm max}(0, x)
\eeqa
Since loss is supposed to be bounded below
 (usually it is defined to be greater or equal to zero),
 the positive-part function 
 comes in handy when defining a loss.



Let

$\ell=$ number of words, length of sentence. $\alp \in [\ell]$

$M=$ number of depths. $\mu\in[M]$

$w^\alp=$ word at position $\alp$

$T_{pos}=\{N, V, JJ, RB\}$, POS tags,
POS=Part Of Speech,  N=Noun, V=Verb, JJ=Adjective, RB=Adverb

$T_{ex}=\{ S, R, O, N\}$\footnote{
The Sax software uses
a different set for $T_{ex}$ than $T_{ex}=\{ S, R, O, N\}$. In Sax, we use for
$T_{ex}$ the list {\tt BASE\_EXTAGS} (defined globally
in the file {\tt sax\_globals}.)
In {\tt BASE\_EXTAGS}, N becomes NONE (or 0)
and R becomes REL (or 3).
Also note that 2 tranets are trained by  Sax,
one for extraction (task=ex), and one for splitting (task=cc).
For task=cc, $T_{ex}$ is replaced by $T_{cc}$. In Sax, we use for
$T_{cc}$ the list {\tt BASE\_CCTAGS}  (defined globally
in the file {\tt sax\_globals}.)
In {\tt BASE\_CCTAGS}, N becomes NONE (or 0)
and R becomes CC (or 3).}. extraction tags (extags),  S=Subject, R=Relation, O=Object, N=None


$T_{ex}\backslash N = T_{ex}-\{N\}$


$POS^\alp\in T_{pos}$, Part Of Sentence of $w^\alp$.



{\bf Importance indicator function}.
\beq
IMP^\alp = \indi(POS^\alp \in T_{pos})
\eeq

{\bf Head verb indicator function.} A {\bf head verb} is any verb that isn't a {\bf light verb}
(do, be, is, has, etc.).

\beq
HV^\alp = \indi(w^\alp \text{ is a head verb})
\eeq

Let $P(\rvt^{\mu, \alp} =t )$ 
denote  an empirical probability  for a table element
$\rvt^{\mu, \alp}\in T_{ex}$, for all $\mu\in [M]$ and $\alp\in[\ell]$.

The O6 paper uses the following sentence
to exemplify the 4 types
of penalty losses. 

\begin{quote}\color{red}
Obama gained popularity after
Oprah endorsed him for the presidency.
\end{quote}
Henceforth, we will refer to
this sentence as the red-sent.

For the red-sent, the head verbs are {\color{red} gained, endorsed}

Two valid extractions from red-sent are:
{\color{red}(Obama; gained; popularity)}
and {\color{red}(Oprah; endorsed him for;
 the presidency)}.


\begin{enumerate}

\item {\bf Part of Speech Coverage (POSC)}

Penalize if 
some important words do not belong to at least one extraction.

In red-sent: all the words {\color{red}Obama, gained, popularity,
Oprah, endorsed, presidency} must be covered in
the set of extractions.

\beq
\call_{POSC}=\sum_{\alp\in [\ell]}IMP^{\alp}P_{POSC}(\alp)
\eeq

\beq
P_{POSC}(\alp)=
1-{\rm max}_{\mu\in [M]}
{\rm max}_{t\in T_{ex}\backslash N}P(\rvt^{\mu, \alp}=t)
\eeq

\item {\bf Head Verb Coverage (HVC)}

Penalize if a head verb
is not present in the relation (R) of any extraction.

In red-sent: {\color{red} (Obama;
gained; popularity), (Obama; gained; presidency)} is not a comprehensive set of extractions.

\beq
\call_{HVC}=
\sum_{\alp\in [\ell]}
HV^\alp P_{HVC}(\alp)
\eeq

\beq
P_{HVC}(\alp)=
\left|
1-\sum_{\mu\in [M]}P(\rvt^{\mu, \alp}=R)
\right|
\eeq

\item {\bf Head Verb Exclusivity (HVE)}

Penalize extractions that 
contain more than one head verb in their relation (R).

In red-sent: {\color{red}gained popularity after Oprah endorsed} is not a good relation as it contains two
head verbs

\beq
\call_{HVE}=
\sum_{\mu\in [M]}
\left(
\sum_{\alp\in [\ell]}
HV^\alp P(\rvt^{\mu, \alp}=R)
-1
\right)_+
\eeq

\item {\bf Extraction Count (EC)}

Penalize if the total number of extractions with head verbs in the relation (R) 
is too small; i.e., it is smaller
 than the number of head verbs
in the sentence.

\beq
\call_{EC}=
\left(
\sum_{\alp\in[\ell]} HV^\alp
-
\sum_{\mu\in [M]}EC^\mu
\right)_+
\eeq

\beq
EC^\mu=
{\rm max}_{\alp\in [\ell]}
HV^\alp P(\rvt^{\mu, \alp}=R)
\eeq


\end{enumerate}
