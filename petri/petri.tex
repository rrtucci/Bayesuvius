\chapter{Petri Nets}
\label{ch-petri}

This chapter
is based on Refs.\cite{wiki-pnet} and \cite{tucci-bayes-petri}

A {\bf Petri net} (pnet)
is basically a diagram of an 
idealized machine that 
features actions (called transitions)
and
buffers (called places)
that contain resources (called tokens).
This diagram evolves in time like a motion picture.  In that motion picture, transitions are fired at various times, sometimes concurrently (i.e., in parallel) and the effect of that is shown  by the motion of the tokens. The evolution of many
 machines can be abstracted
to a pnet.

A pnet  portrays the evolution and allocation of token resources of
an idealized machine, whereas 
a bnet 
portrays the causal connections of events with each other. 
Two big differences between the two diagrams  are:

\begin{itemize}
\item No evolution occurs in a bnet; it's as if the
bnet's TPMs (transition probabilitiy matrices) had been calculated empirically, and those empirical distributions
had reached a \ul{steady state} long ago.
On the other hand, evolution does occur in a pnet, so
we can say that a pnet  occupies a \ul{ transient state}.
\item pnet diagrams portray a machine. They \ul{do not necessarily portray events} as nodes (although they can, and events do occur in their motion picture). bnets, on the
other hand,  \ul{do portray  events} as nodes and
their causal connections.

\end{itemize}

So if pnets and bnets are so different,
why do I discuss pnets in this book
about bnets? Well,
it turns out that one can build a pnet
on top of a bnet, using the bnet nodes
as the transition nodes of the pnet. The 
resulting diagram,
called  a {\bf Bayes-Petri net},
gives both transient and steady state 
information about causality.

For $x\in \RR$, define
the {\bf positive part} of $x$ by

\beq
\pospart{x} = x\indi(x\geq 0)
\eeq
For $x^n\in\RR^n$, define

\beq
x^n\geq 0 \;\;\text{ iff }\;\;
x^n_i\geq 0 \;\;\forall i
\eeq

\section{Original Petri Net}
\subsection{Simple Example of Petri Net}

Fig.\ref{fig-original-pnet-eg-circle}
shows a snapshot of a pnet at a specific time,
and
Fig.\ref{fig-original-pnet-eg-square}
shows the same pnet snapshot, rendered in a different style. The first picture
represents the number of tokens by an integer within a circle.
The second picture represents the number of
tokens by  the number of bullet points within
a rectangle.

\begin{figure}[h!]
$$
\xymatrix{
&&\Circle{0}\ar[rd]
\\
\Circle{3}\ar[r]_<<<{\rvp_1}
&\rvx_1\ar[ur]_>{\rvp_2}
\ar[dr]
&&\rvx_2\ar[r]_>>{\rvp_4}
\ar@/_2pc/[lll]
&\Circle{1}
\\
&&\Circle{2}\ar[ur]_<{\rvp_3}
}$$
\caption{Simple example of a pnet.
Place nodes are labelled $\{\rvp_i\}_{i=1}^{4}$ and 
transition nodes $\{\rvx_i\}_{i=1}^2$.  The number of tokens in each place $\rvp_i$
is indicated by a number (or a whitespace for 0 tokens) within a circle.}
\label{fig-original-pnet-eg-circle}
\end{figure}


\begin{figure}[h!]
$$
\xymatrix{
&&\Rect{\phantom{0}}\ar[rd]
\\
\Rect{\bullet\bullet\bullet}\ar[r]_<<<{\rvp_1}
&\rvx_1\ar[ur]_>{\rvp_2}
\ar[dr]
&&\rvx_2\ar[r]_>>{\rvp_4}
\ar@/_2pc/[lll]
&\Rect{\bullet}
\\
&&\Rect{\bullet\bullet}\ar[ur]_<<{\rvp_3}
}
$$
\caption{Same pnet as that in Fig.\ref{fig-original-pnet-eg-circle},
except that here
the number of tokens is represented by the number of bullet points within a rectangle.}
\label{fig-original-pnet-eg-square}
\end{figure}

Let
\beq
W^{\rvp\rarrow}=
\begin{array}{c|cc}
&\rvx_1&\rvx_2
\\ \hline
\rvp_1&1&0
\\
\rvp_2&0&1
\\
\rvp_3&0&1
\\
\rvp_4&0&0
\end{array}
,\;
W^{\rvp\larrow}=
\begin{array}{c|cc}
&\rvx_1&\rvx_2
\\ \hline
\rvp_1&0&1
\\
\rvp_2&1&0
\\
\rvp_3&1&0
\\
\rvp_4&0&1
\end{array}
,\;
W=
W^{\rvp\larrow}
-W^{\rvp\rarrow}=
\begin{array}{c|cc}
&\rvx_1&\rvx_2
\\ \hline
\rvp_1&-1&1
\\
\rvp_2&1&-1
\\
\rvp_3&1&-1
\\
\rvp_4&0&1
\end{array}
\eeq
Matrix $W$ is called the incidence matrix. Note that it
 has a 1 (resp., $-1$) at position $(i,j)$
if there is an arrow $\rvp_i\larrow \rvx_j$
(resp., $\rvp_i\rarrow \rvx_j$).

If

\beq
p=\left[
\begin{array}{c}
p_1
\\
p_2
\\
p_3
\\
p_4
\end{array}
\right]
,\;
x=
\left[
\begin{array}{c}
x_1
\\
x_2
\end{array}
\right]
\eeq
then:

\beq
p =
{p(0) + Wx}
=
\left[
\begin{array}{c}
{p_1(0)-x_1+x_2}
\\
{p_2(0) + x_1 -x_2}
\\
{p_3(0)+ x_1-x_2}
\\
{p_4(0) + x_2}
\end{array}
\right]
\eeq

3 possible firing rules,
in order of increasing severity, are:

\begin{enumerate}
\item If we require that all places end with zero or a
positive number of tokens, then the firing rule is
\beq
p =
\pospart{p(0) + Wx}
=
\left[
\begin{array}{c}
\pospart{p_1(0)-x_1+x_2}
\\
\pospart{p_2(0) + x_1 -x_2}
\\
\pospart{p_3(0)+ x_1-x_2}
\\
\pospart{p_4(0) + x_2}
\end{array}
\right]
\eeq
If we fire transition  $\rvx_2$ once (i.e., if we set $x_1=0, x_2=1$),
we get 

\beq
p=
\left[
\begin{array}{c}
\pospart{p_1(0)+1}
\\
\pospart{p_2(0) -1}
\\
\pospart{p_3(0)-1}
\\
\pospart{p_4(0) +1}
\end{array}
\right]
=
\left[
\begin{array}{c}
\pospart{3+1}
\\
\pospart{0 -1}
\\
\pospart{2-1}
\\
\pospart{1 +1}
\end{array}
\right]
=
\left[
\begin{array}{c}
4
\\
0
\\
1
\\
2
\end{array}
\right]
\eeq
\item
If we require that
only incoming places
with enough content are used, then the firing rule is


\beq
p = \pospart{p(0) - W^{\rvp\rarrow}x}
+ W^{\rvp\larrow}x
\eeq
\item
If we require that all incoming places are used (\qt{fully
enabled} condition), then the
firing rule is
\beq
p = \left\{
\begin{array}{ll}
p(0) - Wx & \text{if }
[p(0) - W^{\rvp\rarrow} x]
\geq 0
\\
p(0) & \text{otherwise}
\end{array}
\right.
\eeq
\end{enumerate}
The fully enabled 
condition is usually assumed
because it
makes the most physical sense
out of the 3. The first one can \qt{borrow on credit} (i.e., deliver tokens
that it does not have yet) and the second
one can \qt{create new
tokens
out of nothing}.

\subsection{Precise Definition of Petri Net}
Let

$\ZZ_{>0}= \{1, 2, 3, \dots\}$ natural numbers.

$\ZZ_{\geq 0}= \{0, 1, 2, 3, \dots\}$ natural numbers and zero.

\hrule
$\rvp_i =$ ith {\bf place  (a.k.a. buffer, token holder) node}. This node holds either
an integer $\geq 0$ or that number of tokens (tokens are represenxed by bullet points).

$\calp = \{\rvp_i\}_{i=1}^{np}$, set of all place nodes.


Let $\ket{\rvp_i}\in \ZZ^{np\times 1}$ be the one-hot 
column vector with
$1$ at position $i$. $\{\rvp_i\}_{i=1}^{np}$ will be our {markings or places basis}

\hrule 

$\rvx_i =$ ith {\bf transition node}



$\calx = \{\rvx_i\}_{i=1}^{nx}$, set of all transition nodes.

Let $\ket{\rvx_i}\in \ZZ^{nx\times 1}$ be the one-hot 
column vector with
$1$ at position $i$. $\{\rvx_i\}_{i=1}^{nx}$ will be our {transitions basis}.

\hrule

$\rvx\rarrow \rvy$ denotes an {\bf arrow (a.k.a. directed arc)}. Note that there are two types of arrows:
those that go from a transition to a place node and those that go from a place to
a transition node. Let $\rvx\rarrow\rvy=\rvy\larrow \rvx = (\rvx,\rvy)$

$\cala =$  the set of all arrows.
 \beq\cala \subset \ol {\cala} = (\calp \times \calx) \cup (\calx \times \calp) = \{\calp\rarrow\calx \text{ or } \calx \rarrow \calp:
\rvp\in\calp, \rvx \in \calx\}\eeq


A {\bf flow} is a directed path of arrows from $\cala$.

\hrule
$\mu : \calp \rarrow \ZZ_{\geq 0}=$ {\bf marking (i.e., content,
number of tokens) of  each place}. Sometimes, it is convenient to consider a fractional number of tokens in a place, so
$\mu : \calp \rarrow \RR_{\geq 0}$.



$\kappa : \cala\rarrow \ZZ_{>0}=$ {\bf capacity of each arrow}. If unstated for some $a\in \cala$, assume $\kappa(a) = 1$. It's also possible to extend the domain of 
$\kappa()$ to $\ol{\cala}$ and define $\kappa(a) = 0$
for all $a\in \ol{\cala}-\cala$. Sometimes, it is convenient to consider a fractional arrow capacity, so
$\kappa : \cala \rarrow \RR_{\geq 0}$.

\hrule
$W^{\rvp\rarrow} \in \bool^{np\times nx}$. 
$W^{\rvp\rarrow}_{i,j} = 1$ if there is an arrow $\rvp_i\rarrow\rvx_j$
(exiting $\rvp_i$);
else $W^{\rvp\rarrow}_{i,j} = 0$. More generally, in the case a capacity function $\kappa()$ is given, let

\beq
W^{\rvp\rarrow}_{i,j} = \kappa(\rvp_i\rarrow \rvx_j)
\eeq
In Dirac notation, 

\beq
W^{\rvp\rarrow} = \sum_{i,j}\kappa(\rvp_i\rarrow \rvx_j)
\ket{\rvp_i}\bra{\rvx_j}
\eeq


$W^{\rvp\larrow} \in \bool^{np\times nx}$. 
$W^{\rvp\larrow}_{i,j} = 1$ if there is an arrow $\rvp_i\larrow\rvx_j$
(entering $\rvp_i$);
else $W^{\rvp\larrow}_{i,j} = 0$. More generally, in the case a capacity function $\kappa()$ is given, let

\beq
W^{\rvp\larrow}_{i,j} = \kappa(\rvp_i\larrow \rvx_j)
\eeq
In Dirac notation, 

\beq
W^{\rvp\larrow} = \sum_{i,j}\kappa(\rvp_i\larrow \rvx_j)
\ket{\rvp_i}\bra{\rvx_j}
\eeq

$W =W^{\rvp\larrow}-W^{\rvp\rarrow}\in \{-1, 0, 1\}^{np\times nx}$ This is called the {\bf incidence matrix}.
More generally, if a capacity function is given, 

\beq
W_{i,j} =
 W^{\rvp\larrow}_{i,j}-W^{\rvp\rarrow}_{i,j}=
\kappa(\rvp_i\larrow \rvx_j)
-
\kappa(\rvp_i\rarrow \rvx_j)
\eeq
In Dirac notation, 

\beq
W =  \sum_{i,j}\left[
\kappa(\rvp_i\larrow \rvx_j)
-
\kappa(\rvp_i\rarrow \rvx_j)
\right]
\ket{\rvp_i}\bra{\rvx_j}
\eeq

\hrule
$\calp(\rvx\larrow) = \{\rvp : \rvp \rarrow \rvx\in\cala\}=$ all {\bf $\rvx$ input places (a.k.a. $\rvx$ pre-set)}

$\calp(\rvx\rarrow) = \{\rvp : \rvx \rarrow \rvp \in \cala\}=$ all {\bf $\rvx$ output places (a.k.a. $\rvx$ post-set)}

$\calx(\rvp\larrow) = \{\rvx : \rvx \rarrow \rvp \in \cala \}=$ {\bf all $\rvp$ input transitions (a.k.a. $\rvp$ preset)}

$\calx(\rvp\rarrow) = \{\rvx : \rvp \rarrow\rvx \in\cala\}=$ {\bf all $\rvp$ output transitions (a.k.a. $\rvp$ post-set)}

\hrule
$\ket{b(0)}_\calp\in \ZZ_{\geq 0}^{np\times 1} =$ {\bf initial marking}

$\Phi = \av{\calp, \calx, W,\ket{b(0)}_\calp}$ is a {\bf pnet}.
\subsection{Firing of a Petri Net}
Consider a pnet $\Phi = \av{\calp, \calx, W,\ket{b(0)}_\calp}$.

By a {\bf firing step} of a transition $\rvx\in \calx$, we mean the action of removing $\kappa(\rvp\rarrow \rvx)$
tokens from all $\rvp\in\calp(\rvx\larrow)$, and adding $\kappa(\rvx\rarrow\rvp')$ tokens to all $\rvp'\in\calp(\rvx\rarrow)$.

A transition $\rvx\in \calx$ is {\bf enabled} 
(i.e., it may fire)
for $\rvp\in \calp(\rvx\larrow)$, if there are enough tokens in 
$\rvp$ for a firing to be possible; i.e., if $\mu(\rvp) \geq \kappa(\rvp\rarrow \rvx)$. A transition $\rvx\in\calx$ is {\bf fully enabled} if it is enabled for all $\rvp\in \calp(\rvx\larrow)$.

$t \in \ZZ_{>0}$ is the {\bf firing time}.

$b_j(t) =$ {\bf marking of place $\rvp_j$} at firing time $t$.

\beq
\ket{b(t)}_\calp =\sum_j b_j(t) \ket{\rvp_j}
\eeq

$a_i(t) =$ how many times  transition $\rvx_i$
occurs at time $t$
(i.e., the {\bf strength of transition $\rvx_i$}
at firing time $t$).

\beq
\ket{a(t)}_\calx =\sum_i a_i(t) \ket{\rvx_i}
\eeq

Note that we use subscripts $\calp$ and $\calx$ to
indicate which basis, either the $\{\ket{\rvp_i}\}_{i=1}^{np}$
or  $\{\ket{\rvx_i}\}_{i=1}^{nx}$
we are referring to.

{\bf (Original) Firing rule
for transition vector $\ket{a(t)}_\calx$}.

\beq
b_j(t+1) = b_j(t)
+\sum_i W_{j,i}(t) a_i(t)
\eeq
or, equivalently,

 
\beq
\ket{b(t+1)}_\calp =
\ket{b(t)}_\calp +
W\ket{a(t)}_\calx
\label{eq-firing-stepper}
\eeq
where $W(t)=0$ if 
the transition
is not fully enabled.
 
 In general, can replace $W\ket{a(t)}_\calx$ by
 any vector 
 $\ket{A(t)}_\calp$ that can depend on
 the place markings $\mu()$
 for all places
 and
 arrow capacities
 $\kappa()$
 for
 all arrows.

$\ket{a(0)}, \ket{a(1)}, \ket{a(2)},
\ldots$ is the {\bf sequence of firing
steps}.
Often we fire a single transition node
in each step,
instead of firing a linear combination 
of them. In such a case, the
sequence of firing steps reduces to
$a_{i(0)}, a_{i(1)}, a_{i(2)},
\ldots$


A marking $\ket{b^*}_\calp$ is {\bf reachable} from a marking  $\ket{b}_\calp$ in 
$n = 1, 2, \ldots$  steps if
$\ket{b}_\calp$ can be transformed to $\ket{b^*}_\calp$ by executing $n$ firings. We write 
$\xymatrix{\ket{b}_\calp\ar[r]_\Phi& \ket{b^*}_\calp}$ and say a
marking $\ket{b^*}_\calp$ is reachable from a marking
$\ket{b}_\calp$ if $\ket{b^*}_\calp$ is reachable from 
$\ket{b}_\calp$ in a finite
number of steps.
The {\bf reachability set} of a Petri Net $\Phi$ with 
initial marking $\ket{b(0)}_\calp$ is defined as

\beq
R(\Phi) = \{\ket{b^*}_\calp: 
\xymatrix{\ket{b(0)}_\calp\ar[r]_\Phi& \ket{b^*}_\calp} \}
\eeq

\section{Variants}

\subsection{Finite State Machine}
A {\bf Finite State Machine}
(FSM)
can be viewed as 
a type of pnet that has
\begin{itemize}
\item exactly one token moving from a place node to another place node with each step,
\item transitions which only have a single incoming and outgoing arrow
\end{itemize}
We discuss 
FSMs in detail in Chapter \ref{ch-finite-state}.


\subsection{Continuous Petri Net}
So far we have considered
discrete time $t\in \ZZ_{\geq 0}$.
If we let the interval $\Delta t=t_{i+1}-t_i$ between
succesive events tend to zero
and if we replace $W$ by 
$W/\Delta t$ in Eq.(\ref{eq-firing-stepper}), we get
a {\bf continuous Petri net} obeying:

\beq
\frac{db_j(t)}{dt}=
\sum_i W_{j,i}a_i(t)
\label{eq-SODE}
\eeq


\beq
\frac{d\ket{b(t)}_\calp}{dt}=
W\ket{a(t)}_\calx
\label{eq-diff-eq-W}
\eeq


\begin{figure}[h!]
$$
\xymatrix@C=5pc{
{a_1}
&&{a_4}
\ar[dl]|{W_{j,4}}
\\
{a_2}
&
*+++[o][F-]{\frac{db_j}{dt}}
\ar[lu]|{W_{1, j}}
\ar[ld]|{W_{2,j}}
\ar[l]|{W_{3, j}}
& {a_5}
\ar[l]|{W_{j,5}}
\\
{a_3}
&&
{a_6}
\ar[lu]|{W_{j,6}}
}$$
\caption{Representation of Eq.(\ref{eq-SODE}). The place nodes (but not the transition nodes)
of a continuous pnet carry a time derivative.}
\label{fig-continuous-pn}
\end{figure}


We extend $W$ to

\beq
\ol{W}=
\begin{array}{c|cc}
&\rvp_j&\rvx_i
\\ \hline
\rvp_j&0&W_{j,i}
\\
\rvx_i &-W_{j,i} &0
\end{array}
\eeq
The extended matrix $\ol{W}$ is 
an anti-symmetric square matrix.

Eq.(\ref{eq-SODE})
can be represented by 
 the bnet (see Fig.\ref{fig-continuous-pn}).
Thus, a continuous pnet can be 
viewed as $np$ linear bnets,
one for each 
time derivative $db_i(t)/dt$
for $i=1,2, \ldots, np$. Then
the leg nodes of those $np$
different bnets are joined 
if they carry the same transition $a_j(t)$.
One writes an equation 
for each place node (time derivative),
but not for each transition node.
\hrule
{\bf Aside, for those who know Quantum Mechanics:}

Note that if we set $H = i\ol{W}$,
since $\ol{W}$ is anti-symmetric,
it follows that  $H$ is Hermitian ($H=H^\dagger$). Consider Schoedinger's equation 
(on a finite, $np+nx$ dimensional space)
with the Hamiltonian $H$

\beq
\frac{d\ket{\psi(t)}}{dt} =-i H \ket{\psi(t)}
\eeq 
If we set

\beq
\ket{\psi(t)} =\left[
\begin{array}{c}
\ket{b(t)}_\calp\\
\ket{a(t)}_\calx
\end{array}
\right]
\eeq
then the first $np$ equations are identical to Eq.(\ref{eq-diff-eq-W})





\subsection{Colored Petri Net}
In an original pnet, the tokens
are all of the same kind,
so the marking (i.e., content)
of every place node is given by a scalar
from either $\ZZ_{\geq 0}$
or $\RR_{\geq 0}$.
In an original pnet,
the messages that flow through the arrows
are scalars too.
But if one considers
tokens of various kinds,
then one needs a vector or list  to
store their numbers in each place node.
This leads
to the definition of a {\bf colored pnet},
as a pnet wherein
place node markings
and arrow messages become lists, or matrices, or  tensors,
or even
the objects of 
a class.\footnote{The objects
or instances 
of a class are defined in OOP (Object Oriented Programming).}


\subsection{Stochastic Petri Net}
For {\bf stochastic pnets},
the components $a_i(t)$ of the transition strength vector $\ket{a(t)}_\calx$  are random variables rather than deterministic functions.
What this means is that if we measure the value of $a_i$ at time $t$,
we don't get a definite value, but rather a random one that obeys a probability distribution. For example, one might have


\beq
P[\rva_i(t)=a_i(t)] = \lam_i e^{-\lam_i a_i(t)}
\;\; \text{($a_i(t), \lam_i >0$, exponential distribution)}
\eeq
for $i=1,2, \ldots , nx$. 

Sometimes, one considers a {\bf hybrid stochastic pnet} in which some transitions are 
deterministic and fire immediately (these are called {\bf intermediate transitions}), whereas
other transitions fire at random times (these are called {\bf timed transitions}).

Note that $\ket{\rva(t)}_\calx$ is a random 
variable and $\ket{b(t)}_\calp$ is expressed in terms of  $\ket{\rva(t)}_\calx$, so $\ket{b(t)}_\calp$ becomes
 a random variable too.
 
When  Fig.\ref{fig-continuous-pn}
refers to a deterministic
continuous bnet, all nodes
are deterministic.
On the other hand, when it refers
to a stochastic continuous bnet,
all nodes are random variables
with TPMs. 

A stochastic pnet can be either discrete or continuous in time. In the
continuous in time case, $\rva_i(t)$
is a  {\bf stochastic process}; i.e., a 
random variable that 
depends on continuous time. (See Chapter \ref{ch-stochastic-diff-eqns}).
Let SOSE=system of ordinary differential equations. 
Whereas a deterministic continuous pnet
represents a \ul{SODE
for deterministic variables},
 a stochastic continuous pnet
represents a \ul{SODE
for stochastic} \ul{processes}.


\subsection{Bayes-Petri Net}

Given a bnet, one can construct a natural pnet, call it the bnet's {\bf Bayes Petri Net} (Bayes pnet),  that uses the nodes of the bnet as the transition nodes of the Bayes pnet. We will refer to this construction
as  {\bf petrifying a bnet}.

\begin{figure}[h!]
$$
\begin{array}{c}
\xymatrix@C=5pc{
(a)
&
\rvx
\ar[r]
&\rvy
}
\;\;\;\;\;\;
\implies
\;\;\;\;\;\;
\xymatrix@C=5pc{
\rvx
\petriar{r}{P_{\rvy|\rvx}}{P_{\rvx|\rvy}}
&\rvy
}
\\
\xymatrix@C=5pc{
(b)
&
\rvx
\ar[r]
&\rvy
}
\;\;\;\;\;\;
\implies
\;\;\;\;\;\;
\xymatrix@C=5pc{
\rvx
\petriar{r}{2}{3}
&\rvy
}
\end{array}
$$
\caption{Petrifying an arrow by 
adding to it
arcs that transmit
either matrix 
messages
(in $(a))$
or scalar messages (in $(b))$.}
\label{fig-petrify-ar}
\end{figure}
 
See Fig.\ref{fig-petrify-ar} $(a)$. Inspired by Pearl's message passing 
theory (see Chapter \ref{ch-mpass}), we petrify a single arrow bnet $\rvx\rarrow \rvy$
by adding to it two pnet arcs:
\begin{itemize}
\item one pnet arc pointed \qt{downstream} with respect
to the bnet arrow $\rvx\rarrow\rvy$. This
arc carries the transition
probability matrix (TPM) $P_{\rvy|\rvx}$

\item and another pnet arc  pointed \qt{upstream} with respect
to the bnet arrow $\rvx\rarrow\rvy$. This
arc carries probability matrix $P_{\rvx|\rvy}$; i.e.,  the Bayes rule inverse of the bnet's TPM $P_{\rvy|\rvx}$.
\end{itemize}


\begin{figure}[h!]
$$
\begin{array}{ccc}
\xymatrix@R=4pc@C=4pc{
&\rva\ar[dl]\ar[dr]
\\
\rvb\ar[rr]\ar[dr]
&&\rvc\ar[dl]
\\
&\rvd
}
&\xymatrix@R=4pc{
\\
&\implies&}
&
\xymatrix@R=4pc@C=4pc{
&\rva\petriar{dl}{1}{}
\petriar{dr}{}{5}
\\
\rvb\petriar{rr}{}{2}
\petriar{dr}{5}{}
&&\rvc\petriar{dl}{3}{}
\\
&\rvd
}
\end{array}
$$
\caption{Example of petrifying a bnet  with
five arrows, not just one arrow as in Fig.\ref{fig-petrify-ar}}$(b)$
\label{fig-petrify-five-ars}
\end{figure}



Studying {\bf matrix} message passing (colored pnet) as in 
Fig.\ref{fig-petrify-ar} $(a)$
can be considered the ultimate goal
of the theory of Bayes pnets, but as a first
step in that direction,
we will discuss {\bf scalar} message passing
(uncolored pnet)
as in Fig.\ref{fig-petrify-ar} $(b)$.
Just like the proponents of the original pnets
considered the simpler case of uncolored pnets first,
and later graduated to the colored case, we will do the same for Bayes pnets.




In Fig.\ref{fig-petrify-five-ars},
we give an example
of petrifying a bnet with 5 arrows,
instead of just one arrow
as in Fig.\ref{fig-petrify-ar}.

Henceforth in this chapter,
the conditioned nodes of a bnet will
be colored yellow.


The {\bf firing rules} for a Bayes pnet 
come directly from Pearl's d-separation rules
which are discussed in Chapter 
\ref{ch-dsep}. 
Figs.\ref{fig-firing-downstream},
\ref{fig-firing-downstream-cond-par}
and \ref{fig-firing-upstream}
are 3 cases that 
must be considered in the d-separation 
motivated firing rules.
Since the bnet nodes 
and pnet transition nodes
of a Bayes pnet are the same,
one natural way to
fire the pnet nodes 
is to fire them
one at a time, in topological order.


\begin{figure}
$$
\xymatrix@R=3pc@C=3pc{
\rva\petriarDR{dr}{}{1}
&&\rvb\petriar{dl}{2}{}
\\
&\rvx
\petriarDR{dr}{4}{}
\petriarDR{dl}{1}{}
\\
\rvc
&&\rvd
}
\;\;\;
\xymatrix@R=3pc@C=3pc{
\rva\petriarDR{dr}{}{1}
&&\rvb\petriarUR{dl}{2}{}
\\
&*++[o][F*:yellow]{\rvx}
\petriar{dr}{4}{}
\petriar{dl}{1}{}
\\
\rvc
&
&\rvd
}
$$
\caption{
What happens to an \ul{incoming downstream} message
impinging on bnet node $\rvx$,
with $\rvx$ conditioned on or not conditioned on.
}
\label{fig-firing-downstream}
\end{figure}

\begin{figure}
$$
\xymatrix@R=3pc@C=3pc{
\rva\petriarDR{dr}{}{1}
&&\rvb\petriarUR{dl}{2}{}
\\
&{\rvx}
\ar[d]
\petriar{dr}{4}{}
\petriar{dl}{1}{}
\\
\rvc
&
*++[o][F*:yellow]{\rve}
&\rvd
}
$$
\caption{What happens to an \ul{incoming downstream} message
impinging on bnet node $\rvx$,
in case
$\rvx$ isn't conditioned on
but one of its descendants is.}
\label{fig-firing-downstream-cond-par}
\end{figure}

\begin{figure}
$$
\xymatrix@R=3pc@C=3pc{
\rva\petriarUR{dr}{}{1}
&&\rvb\petriarUR{dl}{2}{}
\\
&{\rvx}
\petriarDR{dr}{4}{}
\petriarUR{dl}{1}{}
\\
\rvc
&&\rvd
}
\;\;\;
\xymatrix@R=3pc@C=3pc{
\rva\petriar{dr}{}{1}
&&\rvb\petriar{dl}{2}{}
\\
&*++[o][F*:yellow]{\rvx}
\petriar{dr}{4}{}
\petriarUR{dl}{1}{}
\\
\rvc
&
&\rvd
}
$$
\caption{What happens to an \ul{incoming upstream} message
impinging on bnet node $\rvx$,
with $\rvx$ conditioned on or not conditioned on.}
\label{fig-firing-upstream}
\end{figure}

For more information
about Bayes pnets,
check out my software
called \\ \qt{Bayes\_Petri\_Net}
(Ref.\cite{tucci-bayes-petri}) that implements
some of the concepts about Bayes pnets presented in this section.





