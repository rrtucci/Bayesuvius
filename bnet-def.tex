\chapter*{Definition of a Bayesian Network}
\addcontentsline{toc}{chapter}{Definition of a Bayesian Network}

\label{ch0-bnet-def}

A {\bf directed graph} $G=(V,E)$
consists of two sets, $V$
and $E$. $V$ contains
the {\bf vertices (nodes)}
and $E$ contains the {\bf edges (arrows)}.
An arrow $\rva\rarrow \rvb$ is an
ordered pair
$(\rva,\rvb)$ where $\rva, \rvb\in V$.

The {\bf parents}
of a node $\rvx$ are
those nodes $\rva$
such that there are arrows
$\rva\rarrow \rvx$.
The {\bf children} of a node
$\rvx$
are those nodes $\rvb$
such that there are arrows $\rvx\rarrow \rvb$.
A {\bf root node}
is a node with no parents.
A {\bf leaf node}
is a node with no children.
The {\bf neighbors}
of a node $\rvx$
is the set of parents and
children of $\rvx$.

A {\bf path} is a
set of nodes that
are connected
by arrows, so
that all nodes
have 1 or 2 neighbors,
but only two nodes ({\bf open path})
or zero nodes ({\bf closed path})
have only one neighbor.
A {\bf directed path}
is a path in
which all the arrows point
in the same direction.
A {\bf loop}
is a closed path;i.e.,
a path in which all
nodes have exactly 2 neighbors.
A {\bf cycle} is a directed loop.
A {\bf Directed Acyclic Graph (DAG)}
is a directed graph that has no
cycles.


A {\bf fully connected directed graph}
is
a directed graph
in which
every node has all other
nodes as neighbors.
Figs.\ref{fig-full-conn-4-line}
and
\ref{fig-full-conn-4-square}
show 2 different
ways of drawing
the same directed graph,
a fully connected graph with 4 nodes.
Note that a convenient
way
to label
the nodes of a fully
connected directed
graph
with $N$ nodes
is to point
arrows
from
$\rvx_k$ to $\rvx_j$
where $j=0, 1, 2,\ldots, N-1$
and
$k=j-1, j-2, \ldots, 0$.


\begin{figure}[h!]
$$
\xymatrix{
\rvx_3
&\rvx_2\ar[l]
&\rvx_1\ar[l]\ar@/_1pc/[ll]
&\rvx_0\ar[l]\ar@/_2pc/[ll]\ar@/_2pc/[lll]
}
$$
\caption{Fully
connected directed  graph with 4 nodes,
drawn as a line.}
\label{fig-full-conn-4-line}
\end{figure}

\begin{figure}[h!]
$$
\xymatrix{
\rvx_0\ar[r]\ar[d]\ar[rd]&\rvx_1\ar[d]\ar[ld]
\\
\rvx_2\ar[r]&\rvx_3
}
$$
\caption{Fully
connected directed  graph with 4 nodes,
drawn as a square.}
\label{fig-full-conn-4-square}
\end{figure}

A {\bf connected graph}
is a graph for which there
is no way of separating
the nodes into
two sets so that there is no arrow
from one set to the other.
A {\bf tree}
is a directed graph
in which all nodes
have a single parent
except for a single
node called the ``root" node
which has no parents.
A {\bf polytree}
is a DAG with
no loops.





\hrule

A {\bf Bayesian network (bnet)}
consists of a DAG
and a
{\bf Transition
Probability Matrix (TPM)}
associated
with each node
of the graph.
A TPM is often
called a {\bf Conditional Probability
Table
(CPT)}.
The {\bf structure} of a bnet
is its DAG alone, sans the TPMs.
The
{\bf skeleton} of a bnet is the
undirected graph beneath the bnet's
DAG.



In
this book,
random  variables are
 indicated by
underlined letters and their values
by non-underlined letters.
We use $S_\rvx$
to denote the set of states (i.e., values)
that a random variable $\rvx$ can assume.
 Each node of a bnet is
 labelled by a random variable.
 Thus, $\rvx=x$ means that node
$\rvx$ is in state $x$.

\hrule\noindent
{\bf Some sets of nodes
associated
with each node $\rva$
of a bnet}
\begin{itemize}
\item
$ch(\rva)=$ children of $\rva$.
\item
$pa(\rva)=$ parents of $\rva$.
\item
$nb(\rva)=pa(\rva)\cup ch(\rva)=$
neighbors of $\rva$.
\item
$de(\rva)=\cup_{n=1}^{\infty}ch^n(\rva)=$
$ch(\rva)\cup ch\circ ch(\rva)\cup\ldots$,
descendants of $\rva$.
\item
$an(\rva)=\cup_{n=1}^{\infty}pa^n(\rva)=$
$pa(\rva)\cup pa\circ pa(\rva)\cup\ldots$,
ancestors of $\rva$.
\end{itemize}
\hrule
In this book,
we will use
$\rva.$
to indicate
a {\bf multi-node (node set,
node array)} $\rva.=
(\rva_j)_{j=0, 1, \ldots , na-1}$.
We will often
treat multinodes as if
they were sets, and
combine them with
the usual
set
operators.
For instance,
for two
multinodes $\rva.$
and $\rvb.$,
we define
$\rva.\cup\rvb.$,
$\rva.\cap\rvb.$,
$\rva.-\rvb.$
and
$\rva.\subset\rvb.$
in the obvious way.
We
will indicate
a singleton set (single
node multi-node) $\rva.=\{\rva\}$
simply by $\rva.=\rva$.
For instance,
$\rva.-\rvb=\rva.-\{\rvb\}$.
\hrule

The TPM of a node
$\rvx$ of a bnet
is a matrix of
probabilities
$P(\rvx=x|pa(\rvx)=a.)$,
where $x\in S_\rvx$ and
$a.\in S_{pa(\rvx)}$.

A {\bf deterministic node}
is a node such that its TPM
is of the form

\beq
P(\rvx=x|pa(\rvx)=a.)=
\delta(x, f(a.))
\eeq
for some function $f:S_{pa(\rvx)}\rarrow S_\rvx$,
where $\delta(x,y)$
is the Kronecker delta function.



A bnet
with nodes $\rvx.$
represents
a probability
distribution

\beq
P(x.)=
\prod_j
P(\rvx_j=x_j|
(\rvx_k=x_k)_{k: \rvx_k\in pa(\rvx_j)})
\;.
\label{eq-chain-rule-bnet}
\eeq

Note that
for a fully connected bnet
with $N$ nodes,
Eq.(\ref{eq-chain-rule-bnet})
becomes

\beq
P(x.)=
\prod_{j=0}^{N-1}
P(x_j|
(x_k)_{k=j-1, j-2, \ldots, 0})
\;.
\label{eq-chain-rule-cond-probs}
\eeq
For example, if $N=4$,
Eq.(\ref{eq-chain-rule-cond-probs})
 becomes

\beq
P(x_0, x_1,x_2, x_3)=
P(x_3|x_2, x_1, x_0)
P(x_2|x_1, x_0)
P(x_1|x_0)
P(x_0)
\;.
\eeq
We see that
Eq.(\ref{eq-chain-rule-cond-probs})
is just the chain rule for
conditional probabilities.
\hrule
In this book,
we often use the following conventions
for bnet instantiations:

\bnetInstantiations

\hrule
In this book, \hiddenNodes

\hrule
In  Chapter \ref{ch-good-causal-fit},
we define a measure
of goodness of causal fit (CF)
for each bnet of a finite set of bnets.

Given a dataset of samples for
the random variables
$(\rvx_i)_{i=0, 1, \ldots, N-1}$,
there may be
several bnets (differing
in the direction
of some arrows) that
fit the data well. However,
the one with the highest
CF is most likely to be used
by Nature.
In this book, we will refer to that
single one as the
{\bf best CF bnet}.
We will also refer to
a bnet that has a high (resp., low) value
for its goodness
of CF, though not
necessarily the highest (resp., lowest),
as a {\bf good (resp., bad) CF bnet}.\footnote{
In  this book, the term ``causal bnet"
will mean a good CF bnet. Pearl is fond
of using the term ``causal bnet",
but he uses it in a different
sense that does not
allude to a measure of goodness of causal fit.}

It's important to realize
that bnets that are not a good CF
are far from useless; they
are frequently used
as intermediate calculational
tools. They are incorrect causally,
but they aren't incorrect numerically.
For instance, in Bayes rule,
we switch from a good CF bnet
$P(x|\theta)P(\theta)=x\larrow\theta$
to a bad CF bnet
$P(\theta|x)P(x)=x\rarrow\theta$,
where $x$ is the data
and $\theta$ are the parameters.
