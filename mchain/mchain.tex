\chapter{Markov Chains}
\label{ch-mchain}

A Markov Chain is simply
a bnet with the graph structure 
of a chain. For example,
Fig.\ref{fig-mchain}
shows a chain with $n=4$ nodes.

\begin{figure}[h!]
\centering
$$\xymatrix{
\rvx_0\ar[r]
&\rvx_1\ar[r]
&\rvx_2\ar[r]
&\rvx_3
}$$
\caption{Markov chain with $n=4$ nodes.}
\label{fig-mchain}
\end{figure}

Because of its
 graph structure,
the TPM of each node
only depends on the state of the previous 
node:

\beq
P(x_t|(x_a)_{a\neq t})=P(x_t|x_{t-1})
\;,
\eeq
where $(x_a)_{a\neq t}$ are all
 the nodes except $x_t$ itself and
$t=1, 2, \dots, n-1$.

If there
exists a single
TPM $P_{\rvx_1|\rvx_0}$
such that

\beq
P(x_t|x_{t-1})=P_{\rvx_1|\rvx_0}
(x_t|x_{t-1})
\;
\eeq
for $t=1, 2,\dots, n-1$, 
then
we say 
that the Markov chain
is {\bf time homogeneous}.

\begin{claim} (Data Processing Inequality (DPI))

Consider a Markov chain $\rvx_0\rarrow\rvx_1\cdots\rarrow\rvx_{n-1}$.
Suppose $0\leq a<m<b\leq n-1$. Then
\beq
H(\rvx_a:\rvx_b)\leq \min[H(\rvx_a:\rvx_m), H(\rvx_m:\rvx_b)]
\eeq
\end{claim}
See Ref.\cite{wiki-data-pro} for references where the DPI
is proven.
This inequality
confirms our intuitive expectations
that the information transmitted (i.e., the mutual
information(MI))
from $\rva$ to $\rvb$ (or vice versa
since MI is symmetric)
is smaller or equal to the
one transmitted from $\rva$ to $\rvm$
or from $\rvm$ to $\rvb$
because 
$\rva$ and $\rvb$ are ``farther apart"
and ``some info can get lost during
transmission through the mediator node $\rvm$".