\chapter{Potential Outcomes}
\label{ch-po}
This chapter
is based on Ref.\cite{book-mixtape},
a book by Stephen Cunningham entitled 
``Causal inference: the mixtape".

The theory of potential
outcomes (PO) was for the most part
invented in a seminal
1974 paper by Donald B. Rubin. Rubin
has also
made important extensions
to PO theory since 1974. However, he refuses to
use Pearl's causal DAGs to discuss PO theory. 
Pearl has shown that PO theory
can be substantially clarified
and extended by using
the language of causal DAGs.
The d-separation theorem 
that we discuss in  Chapter \ref{ch-dsep}
is especially
useful in this regard.


In this chapter, we stress the
connection
of PO theory to bnets,
and, in particular, to 
the do and imagine operators 
defined in Chapter \ref{ch-counterf}. Hence,
before reading this chapter,
the reader is expected to have at least
skimmed  Chapter \ref{ch-counterf},
so that he/she understands
the definition
of do and imagine operators.

\begin{table}[h!]
\centering
\begin{tabular}{|l|l|l|l|l|}
\hline
\rowcolor[HTML]{ECF4FF} 
$\s$ & $\rvd^\s$ & $\rvy^\s$ & $(1-\rvd^\s)\rvy^\s$ & $\rvd^\s\rvy^\s$ \\ \hline
Andy & \cellcolor[HTML]{FFFFC7}1 & 10 & . & 10 \\ \hline
Ben & \cellcolor[HTML]{FFFFC7}1 & 5 & . & 5 \\ \hline
Chad & \cellcolor[HTML]{FFFFC7}1 & 16 & . & 16 \\ \hline
Daniel & \cellcolor[HTML]{FFFFC7}1 & 3 & . & 3 \\ \hline
Edith & 0 & 5 & 5 & . \\ \hline
Frank & 0 & 7 & 7 & . \\ \hline
George & 0 & 8 & 8 & . \\ \hline
Hank & 0 & 10 & 10 & . \\ \hline
\end{tabular}
\caption{Dataset describing whether
individual $\s$
took a treatment dose ($d^\s=1$)
or didn't ($d^\s=0$).
The 
treatment outcome
is measured by the real number $y^\s$.}
\label{tab-pot-out-missing}
\end{table} 

Suppose a {\bf population
of individuals} $\s=0,1,2, \ldots, nsam-1$
is given ($d^\s=1$) or
not given ($d^\s=0$)
a {\bf treatment drug dose} $d^\s$,
and that
the 
 {\bf treatment outcome (i.e., response)}
is measured by
a real number $y^\s$.
Table \ref{tab-pot-out-missing}
gives a possible dataset
for this scenario.
As you
can see from
that table,
each individual 
either takes a drug
dose or
doesn't,
but not both.
PO theory
can be viewed as a
 missing
data (MD) problem. MD problems are 
discussed in
 Chapter \ref{ch-missing-d}.
However, the PO MD problem 
is much more specialized
than the generic MD problems
discussed in Chapter \ref{ch-missing-d}.
In the PO MD
problem, we can
fill
in the blank cells
by matching
each individual
that took
the drug with
another {\it similar} 
individual that didn't.
We will have much
more to say about
this matching
strategy later in this chapter.

One can define
similar
individuals as 
individuals that have the same
value
for $nx$ features $x^\s=(x^\s_i)_{i=0, 1, \ldots, nx-1}$.
One
can add to Table \ref{tab-pot-out-missing}
 $nx$ extra columns
giving the value of
the feature vector $x^\s$
for each individual.
Members
of a population with
the same $x^\s$ 
are referred to as 
a
{\bf subpopulation or stratum (ie., layer)}.

In a {\bf randomized clinical trial (RCT)},
the effect 
of the variable $x^\s$ on 
the value
of $d^\s$
is eliminated by
randomizing
the population
and therefore
making the effect of $x^\s$
average out  to zero.
However,
there are many situations
in which carrying out an RCT is not
possible. PO theory is
a way of predicting the
result
of an RCT in situations where
doing a real RCT is not physically possible.

In this chapter, $x^\s$
will be called the confounders.
Implicit throughout this chapter
is the assumption that there are {\bf 
no unmeasured confounders}.
Because if 
there are some unmeasured confounders,
those can
send secret messages 
that influence the value 
that $d^\s$ takes.
This would ruin
the
predictions
of someone trying
to predict the results of an RCT
without
being privy to those secret 
messages.
When there are {\bf some
unmeasured confounders},
it might still be
possible
to
predict the effect of an RCT.
This might be possible
using instrumental variables. See Chapter
\ref{ch-instrumental}
for a discussion
of {\bf instrumental
variables}.


\section{$G$ and $G_{den}$,
bnets,
the starting point bnets}


\begin{figure}[h!]
$$
\begin{array}{ccc}
\xymatrix{
&\rvx\sqsig\ar[dl]\ar[dr]
\\
\rvd\sqsig\ar[rr]&&\rvy\sqsig
}
&&
\xymatrix{
u_\rvd\ar[dd]&u_\rvx\ar[d]&u_\rvy\ar[dd]
\\
&\rvx\ar[dl]\ar[dr]
\\
\rvd\ar[rr]&&\rvy
}
\\
\\
G&&G_{den}
\end{array}
$$
\caption{Bnets
$G$ and $G_{den}$
are 
our starting
point in discussing PO theory. 
 $G$ is for 
a single individual $\s$ of the 
population.
Bnet $G_{den}$ is the 
DEN counterpart 
to $G$.
DEN (Deterministic with
External Noise) bnets are discussed in Chapter
\ref{ch-linear-sys}.} 
\label{fig-po-G-start}
\end{figure}

In this chapter, we will
abbreviate
$\rvX\sqsig=\rvX^\s$
for
$X\in \{d, x, y\}$ 
and for $\s=\{0,1,2, \ldots, nsam-1\}$.


For each individual (aka unit, sample) 
$\s=0, 1, 2, \ldots nsam-1$, let:

$\rvd^\s\in\bool$: treatment discrete drug dose,  1 if treated and 0 if untreated

$\rvy^\s\in \RR$:
 treatment potential outcome

$\rvx^\s$: column vector of treatment 
confounders 
(aka covariates, because they
are often used as covariates (i.e., 
independent
variables) in linear regression.)

Consider bnets $G$ and $G_{den}$
in 
 Fig.\ref{fig-po-G-start}.
$G$ reflects the language
used in Ref.\cite{book-mixtape}
to discuss PO theory. And
$G_{den}$ reflects
the language that Judea Pearl 
prefers to use to discuss PO theory.
Both languages are equivalent. To go from
one language to the other, one need only
perform the following
swaps, where $\rvu$
is the external noise of the DEN bnet.

$\rvX^\s\leftrightarrow \rvX(\rvu)$
for $X\in \{d, x, y\}$.

$P(\s)=\frac{1}{nsam}\leftrightarrow P(u)$

$\sum_\s P(\s) (\cdot)
\leftrightarrow \sum_uP(u) (\cdot)$




The TPMs, printed in blue,
for the bnet
$G$
in Fig.\ref{fig-po-G-start},
are as follows:


\beq\color{blue}
P(x^\s)=
P_{\rvx}(x^\s)
\eeq

\beq\color{blue}
P(d^\s|x^\s)=
P_{\rvd|\rvx}(d^\s|x^\s)
\eeq


\beq\color{blue}
P(y^\s|x^\s, d^\s)=
P_{\rvy|\rvx, \rvd}(y^\s|x^\s, d^\s)
\eeq




Now let:

$\rvd\in\bool$: treatment discrete drug dose,  1 if treated and 0 if untreated

$\rvy\in \RR$:
 treatment potential outcome

$\rvx$: column vector of 
treatment
confounders (aka covariates)


$\rvu=(\rvu_\rvd, \rvu_\rvx, \rvu_\rvy)$:
external noise

The TPMs, printed in blue,
for the bnet
$G_{den}$
in Fig.\ref{fig-po-G-start},
are as follows:


\beq \color{blue}
P(x|u_\rvx)= \indi(\;\;x=u_\rvx\;\;)
\eeq

\beq\color{blue}
P(d|x, u_\rvd)=
\indi( \;\; d= f_\rvd(x, u_\rvd)
\;\;)
\eeq

\beq\color{blue}
P(y|d,x, u_\rvy)=
\indi( \;\; y= f_\rvy(d,x, u_\rvy)
\;\;)
\label{eq-y-is-fy}
\eeq

If we linearize
 $f_\rvy$ in Eq.(\ref{eq-y-is-fy}),
we get

\beqa
\rvy =
\delta \rvd + \beta \rvx + \rvu_\rvy
\;,
\label{eq-y-is-lin}
\eeqa
where $\delta, \beta\in \RR$.
Assuming
that $\rvx, \rvy\in \RR$
and $\rvd\in \bool$,
Eq.(\ref{eq-y-is-lin}) can be plotted.
The resulting plot
is given in Fig.\ref{fig-po-two-parallel-lines}.
This plot
is a very special
case of the PO problem,
but it gives a crude idea
of the ``effects" $\delta
= y(1)-y(0)$ that PO theory 
gives estimates for.
Any 
individual in the experiment
experiences either $y(1)$
or $y(0)$,
but not both.



\begin{figure}[h!]
\centering
\includegraphics[width=2in]
{pot-out/two-parallel-lines.png}
\caption{Plot  of
Eq.(\ref{eq-y-is-lin})} 
\label{fig-po-two-parallel-lines}
\end{figure}




\section{$G_{do+}$  bnet}
\begin{figure}[h!]
$$
\begin{array}{ccccc}
\xymatrix{
&\rvx^\s\ar[dr]\ar[dl]
\\
\rvd^\s\ar[rr]&&\rvy^\s
}
&&
\xymatrix{
&\rvx^\s\ar[dr]
\\
\rho\rvd^\s=\td^\s\ar[rr]&&\rvy^\s
}
&&
\xymatrix{
&\rvx^\s\ar[dr]
\\
\rho\rvd^\s\ar[rr]&&\rvy^\s
}
\\
\\
G&&G_{do}= \rho_{\rvd^\s}
(\td^\s)G&& G_{do+}
\end{array}
$$
\caption{Bnet $G_{do}= \rho_{\rvd^\s}
(\td^\s)G$
is obtained by applying 
the do operator to node $\rvd^\s$
of bnet $G$. Bnet $ G_{do+}$
is obtained
by adding a prior
probability distribution $P(\td^\s)$
to node $\rho\rvd^\s$ of
bnet $G_{do}$.}
\label{fig-po-G-do}
\end{figure}

Fig.\ref{fig-po-G-do}
shows how bnet $G_{do}$
is obtained by applying 
the do operator to bnet $G$,
and
how
bnet $G_{do+}$
is obtained by adding
a prior
probability distribution
 to one of the nodes
of $G_{do}$.
In bnet $G_{do}$,
node  $\rvd^\s$ has been
stripped of all outside 
influences and fixed to a
specific state $\td^\s$.
This is what an RCT does.

The TPMs, printed in blue,
for the bnets $G_{do}$
and $G_{do+}$,
are as follows.
Note that the TPMs
for bnets  $G_{do}$ and $G_{do+}$
are defined in terms
of the TPMs of bnet $G$.

\beq\color{blue}
P(x^\s)=
P_{\rvx}(x^\s)
\eeq

\beq
P_{\rho\rvd}(d)=\sum_x P_{\rvd|\rvx}
(d|x)P_\rvx(x)
\eeq

\beq\color{blue}
P(\td^\s)=
\left\{
\begin{array}{ll}
\delta(\td^\s, (\td^\s)')& \text{for $G_{do}$}
\\
P_{\rho\rvd}(\td^\s)
& \text{for $G_{do+}$}
\end{array}
\right.
\eeq

\beq\color{blue}
P(y^\s|x^\s, \td^\s)=
P_{\rvy|\rvx, \rvd}(y^\s|x^\s, \td^\s)
\eeq


It is convenient
to define
the following
expected values of
$\rvy^\s$
in terms of the TPMs of
bnet $G_{do+}$:

\beq
\caly_{|\td,x}=E_\s E_{|\td,x,\s}[\rvy^\s(\td)]
\rarrow  E_{|\td,x}[\rvy(\td)]
=
\sum_y yP(y|\td,x)
\eeq


\beq
\caly_{|\td}=E_\s E_{|\td, \s}[\rvy^\s(\td)]
\rarrow E_{|\td}[\rvy(\td)]
=
\sum_{ c} \caly_{|\td,x}P(x)
\eeq

\beq
\caly_{|x}=E_\s E_{|x, \s}[\rvy^\s(\td)]
\rarrow E_{|x}[\rvy(\td)]
=
\sum_{\td} \caly_{|\td,x}P(\td)
\eeq


\beq
\caly=E_\s[\rvy^\s]
\rarrow E[\rvy]=\sum_{\td,x}
 \caly_{|\td,x}P(\td)P(x)
\eeq

\section{$G_{im+}$ bnet}


\begin{figure}[h!]
$$
\begin{array}{ccccc}
\xymatrix{
&\rvx^\s\ar[dl]\ar[dr]
\\
\rvd^\s\ar[rr]&&\rvy^\s
}
&&
\xymatrix{
&\rvx^\s\ar[dl]\ar[dr]
\\
\rvd^\s&\rvtd^\s=\td^\s\ar[r]&\rvy^\s
}
&&
\xymatrix{
&\rvx^\s\ar[dl]\ar[dr]
\\
\rvd^\s&\rvtd^\s\ar[r]&\rvy^\s
}
\\
\\
G&&G_{im}= \kappa_{\rvd^\s\rarrow\rvy^\s}
(\td^\s)G&&G_{im+}
\end{array}
$$
\caption{Bnet 
$G_{im}= \kappa_{\rvd^\s\rarrow\rvy^\s}
(\td^\s)G$
is obtained by applying 
the imagine operator to arrow 
$\rvd^\s\rarrow\rvy^\s$
of bnet $G$. Bnet $ G_{im+}$
is obtained
by adding a prior
probability distribution $P(\td^\s)$
to node $\rvtd^\s$ of
bnet $G_{im}$.
} 
\label{fig-po-G-im}
\end{figure}

Fig.\ref{fig-po-G-im}
shows how bnet $G_{im}$
is obtained by applying 
an imagine operator to bnet $G$,
and how bnet $G_{im+}$
is obtained from bnet 
$G_{im}$ by adding
a prior
probability distribution to
one of the nodes of $G_{im}$.
$\rvd\in \bool$ represents the
dose that a patient 
is told to take by a doctor, and
$\rvtd\in \bool$ represents the 
dose he actually takes.
If $\rvd=\rvtd$, the
patient is compliant,
and if $\rvd\neq\rvtd$, he is
non-compliant.


It is convenient to define
$\rvy^\s(\td^\s)$ so that

\beq
\rvy^\s(\td^\s)=
\rvy_{G_{im}}^\s
\eeq
and

\beq
\rvy^\s=
\rvy^\s(\td^\s)=
\rvy^\s(1) \td^\s
+
\rvy^\s(0) (1-\td^\s)
\;.
\eeq 

The TPMs, printed in blue,
for the nodes of bnets $G_{im}$ and $G_{im+}$,
are as follows.
Note that the TPMs
for bnets  $G_{im}$ and $G_{im+}$
are defined in terms
of the TPMs of bnet $G$.
Note that
the prior
$P(\td)$ is not arbitrary;
it's calculated from
the TPMs of bnet $G$.


\beq\color{blue}
P(x^\s)=
P_{\rvx}(x^\s)
\eeq

\beq\color{blue}
P(d^\s|x^\s)=
P_{\rvd|\rvx}(d^\s|x^\s)
\eeq

\beq
\pi_\td=P(\td)=\sum_x P_{\rvd|\rvx}
(\td|x)P_\rvx(x)
\eeq

\beq\color{blue}
P(\td^\s)=
\left\{
\begin{array}{ll}
\delta(\td^\s, (\td^\s)')& \text{for $G_{im}$}
\\
\pi_{\td^\s}
& \text{for $G_{im+}$}
\end{array}
\right.
\eeq


\beq\color{blue}
P(y^\s(\td^\s)|x^\s, \td^\s)=
P_{\rvy|\rvx, \rvd}(y^\s(\td^\s)|x^\s, \td^\s)
\eeq


It is convenient
to define
the following
expected values of
$\rvy^\s$
in terms of the TPMs of
bnet $G_{im+}$:



\beq
\caly_{d|\td,x}
=
E_\s E_{|d,\td,x, \s}[\rvy^\s(\td)]
\rarrow
E_{|d,\td,x} [\rvy(\td)]
=\sum_{y} y P(y|\td,x)P(d|x)
\eeq

\beq
\caly_{d|\td}
=
E_\s E_{| d,\td, \s}[\rvy^\s(\td)]
\rarrow
E_{|d, \td} [\rvy(\td)]
=\sum_x \caly_{d|\td,x}P(x)
\eeq

\beq
\caly_{d|x}
=
E_\s E_{| d,x, \s}[\rvy^\s(\td)]
\rarrow
E_{|d,x} [\rvy(\td)]
=\sum_\td \caly_{d|\td,x}P(\td)
\eeq

\beq
\caly_{d}
=
E_\s E_{| d, \s}[\rvy^\s(\td)]
\rarrow
E_{|d} [\rvy(\td)]
=\sum_{\td,x}\caly_{d|\td,x} P(\td)P(x)
\eeq


$\caly_{0|0}, \caly_{1|1}$
are {\bf factual} 
(for compliant patients)
whereas 
$\caly_{0|1}, \caly_{1|0}$
are {\bf counterfactual} 
(for non-compliant patients).

\section{Translation Dictionary}

\begin{table}[h!]
\renewcommand{\arraystretch}{1.5}
\centering
\begin{tabular}{|l|l|}
\hline
\rowcolor[HTML]{ECF4FF} 
In standard PO notation&
In our notation 
(for $G_{im+}$, unless otherwise specified)\\
\hline
$i$ (unit, sample index)& $\s$ \\ 
\hline 
$D_i$ (treatment dose) & $\rvd^\s$\\
\hline 
$Y_i=y_i$ (treatment outcome)& $\rvy^\s=y^\s$ \\ 
\hline 
$X_i=x_i$ (treatment confounders)& $\rvx^\s=x^\s$ \\ 
\hline 
$Y_i(\td)$ & $\rvy^\s=\rvy^\s(\td)$ for $G_{im}$\\
\hline 
$Y_i$ & $\rvy^\s$ for $G_{im+}$\\
\hline
$E[Y_i(\td)]$ & 
$E_{\s}[\rvy^\s(\td)]=\caly_{|\td}$ for $G_{im}$\\
\hline
$E[Y_i|D_i=d]$ & 
$E_\s E_{|d, \s}[\rvy^\s]=\caly_{d}$\\
\hline
$E[Y_i(\td)|D_i=d]$ & 
$E_\s E_{|d,\td, \s}[\rvy^\s]=\caly_{d|\td}$\\
\hline
$E[Y_i(\td)|D_i=d, X=x]$ & 
$E_\s E_{|d,\td,x, \s}[\rvy^\s]=\caly_{d|\td,x}$\\
\hline
\end{tabular}
\caption{Dictionary for 
translating
from standard PO notation
of Ref.\cite{book-mixtape} to our notation.
}
\label{tab-pot-out-dict}
\end{table}
\renewcommand{\arraystretch}{1}

Table \ref{tab-pot-out-dict}
gives a dictionary for 
translating
from the standard PO notation 
of Ref.\cite{book-mixtape}
to our notation. $d,\td\in \bool$.
PO rarely uses $\caly_{|\td}$, so
$E[Y_i(d)]$ and 
$E[Y_i|D_i=d]$ are often  both used to mean $\caly_d$.
I find
 the standard PO notation 
confusing because it often uses $D_i$
to represent two different nodes, 
$\rvd^\s$ and $\rvtd^\s$ in $G_{im+}$. This confusion
becomes particularly distressing
when we are told in PO notation that

\beq Y_i=D_i Y_i(0) + (1
-D_i)Y_i(1)\;.\eeq
This could mean that

\beq \rvy^\s= \rvtd^\s y^\s(1) 
+ (1-\rvtd^\s)\rvy^\s(0)\eeq 
which is always true, or
it could mean 
\beq \rvy^\s= \rvd^\s \rvy^\s(1) +
 (1-\rvd^\s)\rvy^\s(0)\eeq
which is not necessarily true.


\section{${\cal Y}_{d|\tilde{d}}$
differences (aka treatment effects)}


Note the 
 $\caly_d$ and $\caly_{|\td}$
are not the same thing.
\beq
\caly_{|\td}=
\sum_d \caly_{d|\td}
=
\caly_{0|\td} +
 \caly_{1|\td}
\eeq
whereas

\beq
\caly_d=
\sum_\td \caly_{d|\td}P(\td)
= \caly_{d|0}\pi_0
+
\caly_{d|1}\pi_{1}
\;.
\eeq


$\caly_{|\td}$
is connected to the do operator as follows.

\beq
P(\rvy=y|\rho \rvd=\td)=
\sum_x P(y|\td,x)P(x)
\eeq
so

\beq
\caly_{|\td}=
\sum_y y P(\rvy=y|\rho \rvd=\td)
\;.
\label{eq-y-bar-td}
\eeq
In particular, when
$\rvy$ is binary 
(i.e.,  $\rvy\in\bool$), 
Eq.(\ref{eq-y-bar-td}) becomes

\beq
\caly_{|\td}=
P(\rvy=1|\rho\rvd=\td)
\;.
\eeq

It is convenient to
define the following effects. Note
that we use  the
word {\bf ``effect"} to
refer to 
a difference of two  $\caly_{d|\td}$.

\begin{itemize}

\item average controlled effect  
 (ACE), used when doing a CRT.

\beq
{\color{red}ACE}=
\caly_{|1}-
\caly_{|0}
\eeq

\item average treatment effect\footnote{
Note that effects in which $\td$ varies
are called
``controlled",
whereas those in which $d$ varies instead,
 are called simply ``treatments".
$y$ is averaged over
in both cases.}
 (ATE).
\beq
{\color{red}ATE}=
\caly_{1}-
\caly_{0}= \delta
\eeq

\item average treatment effect 
of the treated (ATT)
\beq
{\color{red}ATT}=
\caly_{1|1}-\caly_{0|1}
\eeq

\item average
treatment effect of the untreated (ATU)
\beq
{\color{red}ATU}=
\caly_{1|0}-\caly_{0|0}
\eeq

\item simple difference in outcomes (SDO)
\beq
{\color{red} SDO}= \caly_{1|1}-\caly_{0|0}
\eeq

\item selection bias (SB)
\beq
{\color{red}SB}=\caly_{0|1}-\caly_{0|0}
\eeq
\end{itemize}

Note that some
of these effects  are
linearly related

\beq
\underbrace{\caly_{1|1}-\caly_{0|0}}_{SDO}
=
\underbrace{(\caly_{1|1}-\caly_{0|1})}_{ATT}
+
\underbrace{\caly_{0|1}-\caly_{0|0}}_{SB}
\eeq

\beq
\underbrace{\caly_1-\caly_0}_
{ATE}=
 \underbrace{(\caly_{1|1}-\caly_{0|1})}_{ATT}\pi_1+
 \underbrace{(\caly_{1|0}-\caly_{0|0})}_{ATU}\pi_0
\eeq

\beqa
\underbrace{\caly_{1|1}-\caly_{0|0}}_{SDO}
&=&
\underbrace{(\caly_{1|1}-\caly_{0|1})\pi_1 +
(\caly_{1|0}-\caly_{0|0})\pi_0 }_{ATE} 
\\
&&+
\underbrace{\caly_{0|1}-\caly_{0|0}}_{SB}
\\
&&+
\underbrace{(\caly_{1|1}-\caly_{0|1})}_{ATT}\pi_0
\\
&&-
\underbrace{(\caly_{1|0}-\caly_{0|0})}_{ATU}\pi_0
\eeqa

\section{Ignorability}

Confounders are said to be {\bf
ignorable} (i.e., there is independence
from confounders)
 if

\beq
\rvy^\s(\rvtd^\s)\perp_P \rvd^\s
\;.
\eeq
This is satisfied by $G_{do}$.
To prove so,
check that

\beq
\rvy^\s(\rvtd^\s)\perp_{G_{do}} \rvd^\s
\;
\eeq
and then invoke
the d-separation theorem 
(see Chapter \ref{ch-dsep}).
When confounders are ignorable, we have

\beq
\caly_{d|\td}=
\caly_{d|y(\td)}=
\caly_{d}
\;,
\eeq
or, equivalently, 


\beq
\caly_{d|\td=0}=
\caly_{d|\td=1}
\;.
\eeq
Therefore,

\begin{subequations}
\label{eq-ignorability}
\beq
ATE=ATT=ATU=SDO
\eeq
and

\beq
SB=0
\;.
\eeq
\end{subequations}

Let $\cale =\{ACE,ATE,ATT,ATU,SDO, SB\}$.
$\cale$ can be 
defined for a fixed stratum $x$
by replacing $\caly_{d|\td}$
with  $\caly_{d|\td, x}$. 
We will denote such
an extension by $\cale_x$,
or, sometimes, simply by $\cale$.


Confounders are said to be {\bf conditionally
ignorable} (i.e., there is
conditional independence
from confounders)
 if

\beq
\rvy^\s(\rvtd^\s)\perp_P \rvd^\s|\rvx^\s
\;.
\eeq
This is satisfied by $G_{im}$. To
prove this, check that

\beq
\rvy^\s(\rvtd^\s)\perp_{G_{im}} \rvd^\s|\rvx^\s
\;
\eeq
and then invoke
the d-separation theorem 
(see Chapter \ref{ch-dsep}).
When confounders are 
conditionally ignorable, we have

\beq
\caly_{d|\td,x}=
\caly_{d|y(\td),x}=
\caly_{d|x}
\;,
\eeq
or, equivalently, 


\beq
\caly_{d|\td=0, x}=
\caly_{d|\td=1, x}
\;.
\eeq
If conditional
ignorability holds, then
Eqs.(\ref{eq-ignorability}) 
are valid at fixed $x$.

\section{
Hypothesis testing for sharp null}
In this section, we assume
no $x$ dependence, or else
we assume that the whole discussion 
refers to  a single stratum $x$.
Hence, we will omit the $x$ subscript
in this section.




Assume $\rvd^\s=\rvtd^\s$.
In this section, we will
discuss hypothesis testing between the
following two opposite hypotheses:

\beq
\begin{array}{l}
H_0: \rvy^\s(1)= \rvy^\s(0) \;\;\forall \s\\
 H_1 =\;\; !H_0 
\end{array}
\;.
\eeq
$H_0$ is called the {\bf sharp null
hypothesis}.


\begin{table}[h!]
\centering
\begin{tabular}{|l|l|l|l|l|}
\hline
\rowcolor[HTML]{ECF4FF} 
$\s$ & $\rvd^\s$ & $\rvy^\s$ & $(1-\rvd^\s)\rvy^\s$ & $\rvd^\s\rvy^\s$ \\ \hline
Andy & \cellcolor[HTML]{FFFFC7}1 & 10 & 10 & 10 \\ \hline
Ben & \cellcolor[HTML]{FFFFC7}1 & 5 & 5 & 5 \\ \hline
Chad & \cellcolor[HTML]{FFFFC7}1 & 16 & 16 & 16 \\ \hline
Daniel & \cellcolor[HTML]{FFFFC7}1 & 3 & 3 & 3 \\ \hline
Edith & 0 & 5 & 5 & 5 \\ \hline
Frank & 0 & 7 & 7 & 7 \\ \hline
George & 0 & 8 & 8 & 8 \\ \hline
Hank & 0 & 10 & 10 & 10 \\ \hline
\end{tabular}
\caption{
Table \ref{tab-pot-out-missing}
with blank cells
filled according to the
sharp null hypothesis $H_0$.
Note that
$\vec{d}_0=(1,1,1,1, 0,0,0,0)$. 
If  $\xi(\vec{d})$
is defined by Eq.(\ref{eq-xi-d-def}), then
$\xi=|34-30|/4=1$
}
\label{tab-pot-out-missing2}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{|l|l|l|l|l|}
\hline
\rowcolor[HTML]{ECF4FF} 
$\s$ & $\rvd^\s$ & $\rvy^\s$ & $(1-\rvd^\s)\rvy^\s$ & $\rvd^\s\rvy^\s$ \\ \hline
Andy & \cellcolor[HTML]{FFFFC7}1 & 10 & 10 & 10 \\ \hline
Ben & 0 & 5 & 5 & 5 \\ \hline
Chad & \cellcolor[HTML]{FFFFC7}1 & 16 & 16 & 16 \\ \hline
Daniel & \cellcolor[HTML]{FFFFC7}1 & 3 & 3 & 3 \\ \hline
Edith & 0 & 5 & 5 & 5 \\ \hline
Frank & \cellcolor[HTML]{FFFFC7}1 & 7 & 7 & 7 \\ \hline
George & 0 & 8 & 8 & 8 \\ \hline
Hank & 0 & 10 & 10 & 10 \\ \hline
\end{tabular}
\caption{Table \ref{tab-pot-out-missing2}
after permuting the entries of column $\rvd^\s$.
Note that
$\vec{d}_1=(1,0,1,1,0,1,0,0)$.
If  $\xi(\vec{d})$
is defined by Eq.(\ref{eq-xi-d-def}), then
$\xi=|36-28|/4=2$}
\label{tab-pot-out-missing3}
\end{table}

Table \ref{tab-pot-out-missing} becomes
Table \ref{tab-pot-out-missing2}
if we fill the blank cells
according to the
sharp null hypothesis $H_0$.
And Table \ref{tab-pot-out-missing2}
becomes  Table \ref{tab-pot-out-missing3}
by permuting
the entries of column $\rvd^\s$.

Define
\beq
N_1 =\sum_\s d^\s
,\;\;\; \pi_1 = \frac{N_1}{nsam}
\;,
\eeq

\beq
N_0= \sum_\s (1-d^\s)=nsam-N_1,
\;\;\
\pi_0 = \frac{N_0}{nsam}
\;,
\eeq

 
\beq
\vec{d}=(d^\s)_{\s=0 1, 2, \ldots, nsam-1}
\;,
\eeq
and


\beq
\xi(\vec{d}) =\left|
\frac{ \sum_\s d^\s y^\s}
{N_1}
-
\frac{ \sum_\s (1-d^\s)y^\s}
{N_0}
\right|
\rarrow |\caly_1-\caly_0|
\label{eq-xi-d-def}
\;.
\eeq
$\xi(\vec{d})$
is the statistic
that we will use to 
test the sharp null hypothesis $H_0$.
There are other
possible functions $\xi(\vec{d})$
that are commonly used 
to test $H_0$; for example,
the Kolgomorov-Smirmov statistic 
(see Ref.\cite{book-mixtape})

According
to Tables
\ref{tab-pot-out-missing2}
and
\ref{tab-pot-out-missing3}, 
$\xi(\vec{d})=1$
for the true (i.e., $H_0$-satisfying) 
 vector $\vec{d}=\vec{d}_0$,
and $\xi(\vec{d})=2$ for
one possible
$H_1$-satisfying vector  
$\vec{d}=\vec{d}_1$.
Let $\cald$ be the
set of all permutations of $\vec{d}_0$,
and define $F:\RR\rarrow [0,1]$ by


\beqa
F(\xi)&=&\frac{1}{|\cald|}
\sum_{\vec{d}\in \cald}
\indi(\xi(\vec{d})\leq \xi)
\\
&=&
E_{\vec{d}}[\indi(\xi(\vec{d})\leq \xi)]\;\;\;
\text{ where }P(\vec{d})=\frac{1}{|\cald|}
\eeqa
The function $F(\xi)$ is monotonically 
increasing
from $F(-\infty)=0$ to $F(+\infty)=1$
so it can be interpreted to be a cumulative
distribution for $\xi$. Then one can
define a $p$ value
for the sharp null
hypothesis by

\beq
p=1-F(\xi(\vec{d}_0))
\;.
\eeq
$p\in[0,1]$ measures the 
likelihood that $H_0$ is true. The smaller it is, 
the less likely $H_0$ is.

Often, $|\cald|=$ 
the number of permutations of $\vec{d}_0$,
is too large to average over all 
the elements of $\cald$. In that
case, one can use random sampling methods.
For example, one can choose a $\vec{d}$
at random
from $\cald$, 
and calculate a step function $F_i(\xi)$
from that. Do this $ni$ times. Then
average all the $ni$ step functions
to obtain an estimate of $F(\xi)$.


\section{Matching Strata}

For a situation
described by
the bnet $G_{im+}$,
we can match {\it similar}
individuals to fill the blank cells of
 Table \ref{tab-pot-out-missing}.
By ``similar", we mean that
they have the same or almost the same
value of $\rvx^\s$.


\subsection{1-1 strata-match}

In 1-1 strata-match,
we match each individual with
$d^\s=1$
with
exactly
one individual
with $d^\s=0$.

Recall that

\begin{subequations}
\label{eq-to-estimate}
\beq
SDO=\caly_{1|1,x}-\caly_{0|0,x}
\eeq

\beq
ATT=
\caly_{1|1,x}-\caly_{0|1,x}
\eeq

\beq
ATU=
\caly_{1|0,x}-\caly_{0|0,x}
\eeq

\beq
ATE = ATT \pi_1 + ATU \pi_0
\eeq
\end{subequations}
Note that $\pi_0, \pi_1$
do not depend on $x$.
This
can be justified by  looking at 
the bnet $G_{im+}$,
for which $\pi_\td=P(\td)$
is the prior of node $\rvtd$.

Eqs.(\ref{eq-to-estimate})
can be estimated from the data
via the following estimators.
In these estimators,
$s(\s)$ is the single
match for individual $\s$.


\beq
\widehat{SDO}=
\frac{1}{N_1}\sum_\s d^\s y^\s
-
\frac{1}{N_0}\sum_\s (1-d^\s) y^\s
\eeq


\beqa
\widehat{ATT}
&=&
\frac{1}{N_1}\sum_\s d^\s y^\s - 
\frac{1}{N_1}\sum_\s d_\s y^{s(\s)}
\\
&=&
\frac{1}{N_1}\sum_\s d^\s (y^\s - y^{s(\s)})
\eeqa

\beqa
\widehat{ATU}
&=&
\frac{1}{N_0}\sum_\s (1-d^\s) ( y^{s(\s)}-y^\s)
\eeqa

\beqa
\widehat{ATE}&=&
\widehat{ATT} \pi_1 + 
\widehat{ATU} \pi_0
\\
&=&
\frac{1}{nsam}[
\widehat{ATT}N_1 + 
\widehat{ATU}N_0]
\\
&=&
\frac{1}{nsam}
\left[\sum_\s d^\s (y^\s - y^{s(\s)})+
\sum_\s(1-d^\s) ( y^{s(\s)}-y^\s)
\right]
\\
&=&
\frac{1}{nsam}\sum_\s (2d^\s-1) (y^\s - y^{s(\s)})
\eeqa

\subsection{Exact and approximate strata-match}

It is very often
the case that
one can't
find for a given
individual $\s$
another individual that has 
exactly the same value of $x^\s$.
In such cases, one can discard all
matchless individuals.
But that would entail a loss 
of precious information.
Instead of discarding orphans, 
a better way is to
relax our demands and
match individual $\s$
with another individual $s$
such that $x^s$
and $x^\s$ are very
close in some metric.

More precisely, 
for some arbitrary
parameter $\eps>0$,
and an individual $\s$
with $d^\s=1$,
define
the {\bf strata-matching set} 
$\calm_{\eps}(\s)$ by

\beq
\calm_{\eps}(\s)=
\{s: d^\s=1, d^s=0, 
dist(x^\s, x^s)\leq \eps \}
\;,
\eeq
where

\beq
dist(x^\s, x^s)=
[x^\s]^T [\Sigma]^{-1} x^s
\;,
\eeq
where $\Sigma = \av{\rvx^\s, [\rvx^s]^T}$.
 This
metric $dist(x^\s, x^s)$ is
called the {\bf Mahalanobis distance}.
We will call
the case $\eps=0$ an {\bf  exact strata-match},
and
the case
$\eps\neq 0$ 
 an {\bf approximate strata-match.}.
To do an approximate strata-match,
replace $y^{s(\s)}$ 
by
$\av{y}^\s$ 
in 
the estimators 
given above 
for a 1-1 strata-match.
$\av{y}^\s$ 
is defined by

\beq
\av{y}^\s=
\frac{1}{|\calm_{\eps}(\s)|}
\sum_{s\in \calm_{\eps}(\s)}
y^\s
\;.
\eeq

Ref.\cite{book-mixtape}
calculates the mean and variance
of estimator $\widehat{ATT}$. 
The mean is biased,
but one can define a new
bias-corrected estimator.


\subsection{Positivity}

{\bf Positivity} is defined as the
requirement that for all layers $x$,
\beq
0<P(d^\s=1|x^\s=x)<1
\eeq
or, equivalently, 
\beq
P(d^\s=1|x^\s=x)>0\text{\;\;\;and
\;\;\;}P(d^\s=0|x^\s=x)>0
\;.
\eeq
In other words, 
for each layer $x$,
there is
a non-zero
probability of being both treated 
and untreated.

If positivity is violated,
then 
for some 
layer $x$, 
 $\caly_{0|\td,x}$ or $\caly_{1|\td,x}$ 
is zero,
so we can't calculate effect
estimators such as $\widehat{ATE}$.
If $\widehat{ATE}$ can be calculated,
one says it is identifiable (i.e.,
calculable). Positivity is a requirement
for identifiability
of $\widehat{ATE}$ .

 

When 
$P(d^\s|x^\s=x)$ 
becomes 0 or 1 for some $x$,
the arrow
$\rvx\rarrow\rvd$
becomes deterministic
for some $x$.
This situation
is
the very 
antithesis
of RCTs,
wherein 
the influence
exerted by $\rvx^\s$ on 
$\rvd^\s$ is uniformly
random and therefore ignorable.
Hence, it is perhaps 
not too surprisingly
that a violation
of positivity makes
$\widehat{ATE}$
non-identifiable.



\section{Propensity Score}

It is often the case
that the discrete vector $\rvx^\s$
has
too many possible values to make
matching possible.
In such cases, it
is convenient to 
map the space
of vectors
$\rvx^\s$
to the real line.
One very  
convenient choice
for that map
is the 
{\bf propensity score},
which is defined as

\beq
g(x^\s)=P(d^\s=1|x^\s)
\;.
\eeq
The
propensity
score
is usually
approximated
by a sigmoid
function
using logistic regression\footnote{
The sigmoid function is defined
in Chapter \ref{ch-not-cons}
to be $\sig (x) = 1/(1-e^{-x})$.}

\beq
g(x^\s)= \sig(\alp + \beta x^\s)
\eeq


\begin{figure}[h!]
$$
\xymatrix{
\rvg^\s\ar[d]
&\rvx^\s\ar[dr]\ar[l]
\\
\rvd^\s&\rvtd^\s\ar[r]&\rvy^\s
\\
&G_{ps}
}
$$
\caption{Bnet $G_{ps}$
used when doing propensity scoring.} 
\label{fig-po-G-ps}
\end{figure}
To use the 
propensity score,
one replaces the bnet $G_{im+}$
by the bnet $G_{ps}$
shown in Fig.\ref{fig-po-G-ps}.
The TPMs, printed in blue,
for the 2 nodes of $G_{ps}$
that differ from the nodes
of $G_{im+}$,
are as follows:


\beq\color{blue}
P(g^\s|x^\s)= 
\delta(g^\s, g(x^\s))
\eeq

\beq\color{blue}
P(d^\s|g^\s)= 
g^\s d^\s + (1-g^\s)(1-d^\s)
\eeq

Note that
these TPMs are self-consistent because

\beqa
P(d|x)&=&
\sum_g P(d|g)P(g|x)
\\
&=&
g(x)d + [1-g(x)](1-d)
\\
&=&
P(d=1|x)d + [1-P(d=1|x)](1-d)
\\
&=&
P(d|x)
\eeqa


We would like to do
{\bf propensity score strata-matching} by
matching g-strata instead of x-strata.
 PO calculations
for x-strata matching
use the TPMs
for $P(d|x)$, $P(x)$
and $P(y|d,x)$.
To do g-strata matching
using the same
equations, but 
with $x$ replaced by $g$,
we would need to solve for
$P(d|g)$, $P(g)$
and $P(y|d,g)$
in terms of
$P(d|x)$, $P(x)$
and $P(y|d,x)$.
We solve for those next.

From the TPMs
for $G_{ps}$, one has

\beq
\boxed{
P(d|g)= 
g d + (1-g)(1-d)}
\eeq
and

\beq
\boxed{
P(g)=\sum_x \overbrace{
\delta(g,g(x))}^{P(g|x)}
P(x)}
\;.
\eeq
Next, note that


\beq
P(y| d,g)=
\sum_x P(y|d,x)P(x|g)
\eeq
so we need to find $P(x|g)$. Since

\beqa
P(x|g)&=&\frac{P(g|x)P(x)}{P(g)}
\\
&=&
\frac{\delta(g, g(x))P(x)}{P(g)}
\eeqa
we finally get

\beq
\boxed{
P(y| d,g)=
\sum_x P(y|d,x)
\frac{\delta(g, g(x))P(x)}{P(g)}
}
\;.
\eeq