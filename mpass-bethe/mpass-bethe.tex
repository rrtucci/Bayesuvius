\chapter{Message Passing and 
Bethe Free Energy}
\label{ch-mpass-bethe}

\newcommand{\ttheta}[0]{\TIL{\theta}}
\newcommand{\tP}[0]{\TIL{p}}

This chapter is based
on Refs. \cite{WainJordan}
and \cite{yedidia}.

\section{2MRFs}



\begin{figure}[h!]
$$\xymatrix{
&*+[F]{\Delta}
&
&*+[F]{\Delta}
&
&*+[F]{\Delta}
\\
*++[o][F]{x_{11}}
\ar@{-}[d]\ar@{-}[r]\ar@{-}[ru]
&*+[F]{\Delta}\ar@{-}[r]
&*++[o][F]{x_{12}}\ar@{-}[d]\ar@{-}[r]
\ar@{-}[ru]
&*+[F]{\Delta}\ar@{-}[r]
&*++[o][F]{x_{13}}\ar@{-}[d]
\ar@{-}[ru]
\\
*+[F]{\Delta}\ar@{-}[d]
&*+[F]{\Delta}
&*+[F]{\Delta}\ar@{-}[d]
&*+[F]{\Delta}
&*+[F]{\Delta}\ar@{-}[d]
&*+[F]{\Delta}
\\
*++[o][F]{x_{21}}\ar@{-}[r]\ar@{-}[ru]
&*+[F]{\Delta}\ar@{-}[r]
&*++[o][F]{x_{22}}\ar@{-}[r]\ar@{-}[ru]
&*+[F]{\Delta}\ar@{-}[r]
&*++[o][F]{x_{23}}\ar@{-}[ru]
}$$
\caption{Example
of factor graph
for a 2MRF.
In this figure,
a boxed $\Delta$
between variables $x_i$
and $x_j$,
denotes the propagator  function
$\Delta(x_i, x_j)$.
Also, a boxed
$\Delta$
connected to a
single variable $x_i$,
denotes the function
$\Delta(x_i, k)$,
where $k$
is a fixed number.
If we view this factor graph
as a representation of a 3 dimensional device
with the leaf nodes $\Delta$
in the back part and everything else in the front part,
then the back part can represent the light inputs from a scene, and the front part
can represent the analysis being done 
with the light inputs.
}
\label{fig-paiwise-mrf}
\end{figure}

Factor graphs
are discussed in Chapter \ref{ch-factor-g}.

A pairwise Markov Random Field (2MRF)
is a statistical model 
whose probability
distribution
is of the form

\beq
p(x)=\caln(!x)\prod_{i-j}\Delta(x_i, x_j)
\label{eq-2mrf}
\eeq
where $x=(x_1, x_2,
\ldots, x_n )$,
where $i-j$
represents the edge
that connects
nodes $i$ and $j$
in an undirected graph $G$,
and where  the product 
is over all edges of $G$.
$p(x)$ can be represented
graphically
by a factor graph.

Fig.\ref{fig-paiwise-mrf}
shows an example
of a 2MRF
represented as a
factor graph.

The {\bf Ising model} is
another prototypical
example of a 2MRF. 
Let $T$ be the temperature
of a system, 
\beq
\theta=\frac{1}{T}\;,
\eeq
and 
$x= (x_1, x_2, \ldots, x_{nx})
\in\{-1, 1\}^{nx}$.
The Ising model
is defined 
for $J,h(x_i)\in \RR$ by

\beq
\eps(x)= -J\sum_{i-j}x_i x_j -\sum_i h(x_i)
\eeq

\beq
P(x) =
\frac{e^{\frac{-\eps(x)}{T}}}{Z_\theta}
\eeq


Note that a bnet can
be easily converted to an
equivalent
2MRF.
You just
set $\Delta(x_i, k)=p(x_i)$
if $x_i$ is a root node
of the bnet,
and $\Delta(x_i, x_j)=p(x_i|x_j) $
if $x_i$ isn't a root node.
One 
must then choose
the arrow directions
of the bnet.
A natural choice
is to choose the
bnet arrows so 
that they all point from
an $x_i$ to a $\Delta$.



In what follows,
we shall use
{\bf messages (a.k.a. cavity fields)}
 $m_{a\rdart i}(x_i)$,
Note
that in our
notation
for messages, the letter $i$
appears twice,
and both occurrences
are next to each other.
This is always the case in
our notation for messages.

Henceforth, we will
refer to the functions
$\Delta(x_i, x_j)$
as {\bf propagators},
to make contact with 
Physics.

Henceforth,
we shall use the notation
$\partial i$  
to denote all nodes
$j$ that
are neighbors of node $i$.
$\partial i$ is called
the {\bf neighborhood or boundary} of $i$.


\section{Message Passing Intuition}
Next we present
some results that will
be derived later in the chapter,
but which we will first
present  now without proof,
and explain how they
make sense intuitively.
Assume that the functions $\Delta(x_i, x_j)$ 
are known
for all edges $i-j$, and that
the full
probability distribution $p(x)$
of the model
is given by
Eq.(\ref{eq-2mrf}).

\begin{enumerate}
\item Calculate 
message 
$m_{j\rdart i}(x_i)$ 
in terms of messages 
leaving $x_j$.
This is called 
the {\bf message updating rule}.

One has that
\beq
m_{j\rdart i}
(x_i) =
\sum_{x_j}
\Delta(x_i, x_j)
\prod_{b\in
\partial j\setminus i}
m_{b\rdart j} (x_j) 
\eeq
This can be represented graphically by

\beq
\xymatrix@C=.7pc{
\\
*++[o][F]{x_i}\ar@{-}[r]
&*+[F]{m_{j\rdart i}(x_i)}
&=
}
\xymatrix@C.7pc{
&&&*+[F]{m_{b_1\rdart j}(x_j)
}
\\
*++[o][F]{x_i}\ar@{-}[r]
&*+[F]{\Delta}\ar@{-}[r]
&*++[o][F]{\sum x_j}\ar@{-}[r]
\ar@{-}[ru]\ar@{-}[rd]
&*+[F]{
m_{b_2\rdart j}(x_j)
}
\\
&&&*+[F]{
m_{b_3\rdart j}(x_j)
}
}
\eeq

\item Calculate $\tP (x_i
, x_j )$ for any edge $i-j$ 
in terms of messages entering $x_i$
and leaving $x_j$

One has that
\beq
\tP (x_i
, x_j ) 
=\caln(!x)
 \Delta(x_i
, x_j )
\prod_{a\in \partial i \setminus j}
m_{a\rdart i}(x_i)
\prod_{b\in \partial j \setminus i}
m_{b\rdart j}(x_j)
\eeq
This can be represented graphically by

\beq
\xymatrix@C=.7pc{
\\
*++[o][F]{x_i}\ar@{-}[r]
&
*+[F]{\tP}
\ar@{-}[r]
&*++[o][F]{x_j}
&=
\caln(!x)
}
\xymatrix@C.7pc{
*+[F]{m_{a_1\rdart i}(x_i)
}\ar@{-}[dr]
&&&&*+[F]{m_{b_1\rdart j}(x_j)
}
\\
*+[F]{m_{a_2\rdart i}(x_i)
}\ar@{-}[r]
&*++[o][F]{x_i}\ar@{-}[r]
&*+[F]{\Delta}\ar@{-}[r]
&*++[o][F]{x_j}\ar@{-}[r]
\ar@{-}[ru]\ar@{-}[rd]
&*+[F]{
m_{b_2\rdart j}(x_j)
}
\\
*+[F]{m_{a_3\rdart i}(x_i)
}\ar@{-}[ur]
&&&&*+[F]{
m_{b_3\rdart j}(x_j)
}
}
\eeq
\item
Calculate $\tP(x_i)$
in terms of messages entering $x_i$

One has that
\beq
\tP(x_i)=\sum_{x_j}\tP(x_i, x_j)
\eeq
This can be represented graphically by

\beq
\xymatrix@C=.7pc{
\\
*++[o][F]{x_i}\ar@{-}[r]
&
*+[F]{\tP}
&=
\caln(!x))
}
\xymatrix@C.7pc{
*+[F]{m_{a_1\rdart i}(x_i)
}\ar@{-}[dr]
\\
*+[F]{m_{a_2\rdart i}(x_i)
}\ar@{-}[r]
&*++[o][F]{x_i}\ar@{-}[r]
&*+[F]{m_{j\rdart i}(x_i)}
\\
*+[F]{m_{a_3\rdart i}(x_i)
}\ar@{-}[ur]
}
\eeq
\end{enumerate}

Why it works?

Suppose we approximate
the propagator $\Delta(x_i, x_j)$ by
\beq
\Delta(x_i, x_j)
\approx m_{j\rdart i}(x_i)
m_{i\rdart j}(x_j)
\eeq
This can be represented graphically by

\beq
\xymatrix@C=.7pc{
*++[o][F]{x_i}\ar@{-}[r]
&*+[F]{\Delta}\ar@{-}[r]
&*++[o][F]{x_j}
&\approx
&*++[o][F]{x_i}\ar@{-}[r]
&*+[F]{m_{j\rdart i}(x_i)}
&*+[F]{m_{i\rdart j}(x_j)}\ar@{-}[r]
&*++[o][F]{x_j}
}
\eeq
Then the results 1,2,3 above
can be justified as follows.


\begin{enumerate}


\item 

\beqa
m_{j\rdart i}
(x_i) &\approx&
\sum_{x_j}
\underbrace{
m_{j\rdart i}(x_i)
m_{i\rdart j}(x_j)
}_{\Delta(x_i, x_j)}
\prod_{b\in
\partial j\setminus i}
m_{b\rdart j} (x_j)
\\
&=&
 m_{j\rdart i}(x_i)
 \sum_{x_j}
 \prod_{b\in \partial j}
 m_{b\rdart j} (x_j)
 \\
 &=&
 \caln(!x)
 m_{j\rdart i}(x_i)
\eeqa
\item

\beqa
\tP(x_i, x_j) 
&=&
\sum_{x\setminus x_i, x_j}\tP(x)
\\
&\approx&
\caln(!x)
\sum_{x\setminus x_i, x_j}
\prod_{i-j}
\underbrace{
m_{j\rdart i}(x_i)
m_{i\rdart j}(x_j)
}_{\Delta(x_i, x_j)}
\\
&\approx &
\caln(!x)
 \Delta(x_i
, x_j )
\prod_{a\in \partial i \setminus j}
m_{a\rdart i}(x_i)
\prod_{b\in \partial j \setminus i}
m_{b\rdart j}(x_j)
\eeqa
\item

\begin{align}
\tP(x_i) 
&=
\sum_{x_j}\tP(x_i, x_j)
\\
&\approx 
\caln(!x)
\sum_{x_j}
\underbrace{
m_{j\rdart i}(x_i)
m_{i\rdart j}(x_j)
}_{\Delta(x_i, x_j)}
\prod_{a\in \partial i \setminus j}
m_{a\rdart i}(x_i)
\prod_{b\in \partial j \setminus i}
m_{b\rdart j}(x_j)
\\
&\approx 
\caln(!x)
\prod_{a\in \partial i}
m_{a\rdart i}(x_i)
\end{align}
\end{enumerate}

Henceforth,
we will ocassionally use
the following more
 compressed notation
for the factor graph of a 2MRF.
Instead of using
circles for variables
and squares for functions,
we will use arrows pointing
from $x_1$ to $x_2$
and vice versa.
\beq
\xymatrix@C=.7pc{
*++[o][F]{x_1}\ar@{-}[r]
&*+[F]{m_{2\rdart 1}(x_1)}
&*+[F]{m_{1\rdart 2}(x_2)}\ar@{-}[r]
&*++[o][F]{x_2}
}
=\quad
\xymatrix@C=4pc{
x_1\ar@<1ex>[r]^{m_{1\rdart 2}(x_2)}
&x_2\ar@<1ex>[l]^{m_{2\rdart 1}(x_1)}
}
\eeq


Next, we give an 
example
of how to use message 
passing
to find
the marginal $\tP(x_i)$
for any node $x_i$. Refer to
Fig\ref{fig-mp-3node-example}
for this example.

\begin{figure}[h!]
$$\xymatrix@C=5pc{
&&x_3\ar@<1ex>[ld]^{m_{3\rdart 2}(x_2)}
\\
x_1\ar@<1ex>[r]^{m_{1\rdart 2}(x_2)}
&x_2\ar@<1ex>[l]^{m_{2\rdart 1}(x_1)}
\ar@<1ex>[ru]^{m_{2\rdart 3}(x_3)}
\ar@<1ex>[rd]^{m_{2\rdart 4}(x_4)}
\\
&&x_4\ar@<1ex>[lu]^{m_{4\rdart 2}(x_2)}
}$$
\caption{
Example of compressed factor graph 
for a 2MRF.}
\label{fig-mp-3node-example}
\end{figure}

Using the message
updating rule
successively, we get
\begin{align}
\tP(x_1)
&=
\caln(!x)m_{2\rdart 1}(x_1)
\\
&=
\caln(!x)
\sum_{x_2}
\Delta(x_1,x_2)
m_{3\rdart 2}(x_2)
m_{4\rdart 2}(x_2)
\\
&=
\caln(!x)
\sum_{x_2}
\Delta(x_1,x_2)
\sum_{x_3}
\Delta(x_2,x_3)
\sum_{x_4}
\Delta(x_2, x_4)
\\
&=
\xymatrix@R=.5pc@C=.7pc{
&&&&
*++[o][F]{\sum x_3}
\\
&&&
*+[F]{\Delta}\ar@{-}[ru]
\\
*++[o][F]{x_1}\ar@{-}[r]
&*+[F]{\Delta}\ar@{-}[r]
&*++[o][F]{\sum x_2}
\ar@{-}[ru]\ar@{-}[rd]
\\
&&&*+[F]{\Delta}\ar@{-}[rd]
\\
&&&&
*++[o][F]{\sum x_4}
}
\end{align}

This 
algorithm
is guaranteed to work
only for trees. In practice,
one starts by calculating the 
messages pointing up from
the leaf nodes
of the tree. Then one calculates
the messages pointing up from the parents 
of the leaf nodes of the tree.
And so forth until
one calculates
all the messages 
pointing up from the leaf nodes
to the root node
of the tree.
Then one goes in the opposite
direction, first calculating
messages pointing down from the 
root node to its children, from the
children  of the root node 
to their children. And so forth.
By the end, 
all upward and downward
pointing 
messages have been calculated.
From this, $\tP(x_i)$
for all $i$ can be calculated.
$\tP(x_i)$
is the product of all
messages pointing into $x_i$.
.


\section{ $-\ln Z_\theta$=
Free Energy (FE)}

Suppose $\theta, \eps(x)\in \RR^{ni}$
and $x=(x_1,x_2, \ldots, x_{nx})\in \RR^{nx}$.
Define the {\bf partition function} $Z_\theta$ by
\beq
Z_\theta = \sum_x   e^{-\theta^T \eps(x)}
\eeq
and the probability
distribution $P(x|h)$ by


\beqa
\underbrace{P(x|\theta)
}_{e^{LL_\theta(x)}} &=& 
\exp( -\theta^T \eps (x) - \ln Z_\theta)
\label{eq-px-at-theta}
\\
&=&
\frac{
e^{-\theta^T \eps(x)}}{Z}
\eeqa
$\eps(x)$ is called a {\bf sufficient
statistic } for $\rvx$ because
$P(x|\theta)$
is a functional (i.e.,
a function of a function) of $\eps(x)$.
Note that 
by taking  
first and higher order derivatives
of $\ln Z_\theta$
with respect to $\theta_i$,
we can calculate
the statistics
of $\eps_i(x)$.


\beqa
-\partial_{\theta_i} \ln Z_\theta
&=&
\frac{1}{Z_\theta}
\sum_x   
\eps_i(x)
e^{-\theta^T \eps(x)}
\\
&=&
E_{\rvx|\theta}[\eps_i(\rvx)]=\av{\eps_i}
\eeqa


\begin{align}
\partial_{\theta_j}
\partial_{\theta_i} \ln Z_\theta
&=
\partial_{\theta_j}\frac{1}{Z_\theta}
\sum_x   
-\eps_i(x)
e^{-\theta^T \eps(x)}
\\
&=
\left\{
\begin{array}{l}
\frac{1}{Z_\theta}
\sum_x   
\eps_j(x)\eps_i(x)
e^{-\theta^T \eps(x)}
\\
+\frac{-1}{Z_\theta^2}
\left[
\sum_x   
-\eps_j(x)
e^{-\theta^T \eps(x)}
\right]
\left[
\sum_x   
-\eps_i(x)
e^{-\theta^T \eps(x)}
\right]
\end{array}
\right.
\\
&=
\av{\eps_j\eps_i}
-\av{\eps_j}\av{\eps_i}
\\
&=
\av{\eps_j, \eps_i}
\end{align}

Define the log likelihood $LL_\theta(x)$
by


\beq
LL_\theta(x)= -\theta^T \eps(x) 
-\ln Z_\theta
\eeq
and the {\bf entropy} $S$ by
\footnote{In Thermodynamics,
the entropy is denoted by the letter
$S$. In Shannon Information
Theory, and elsewhere in this
book, it is denoted by the letter $H$.}

\beqa
S &=& -\sum_x
 P(x|\theta)LL_\theta(x)
 \\
 &=&
 -\sum_x P(x|\theta)\ln P(x|\theta)
\eeqa
Define the {\bf internal energy} $U$ by 
\beq
U = \sum_x P(x|\theta)\eps_\theta(x)
\eeq

\beq
-S=-\theta^T U - \ln Z_\theta
\eeq

\beq
\partial_{U_i} S= \theta_i
\eeq
$S$ is concave.
$S$ and $-\ln Z_\theta=F/T$
are concave dual functions.\footnote{
Concave dual functions
are discussed in Chapter \ref{ch-var-bay-medical}}.

\begin{mdframed}[hidealllines=true,backgroundcolor=blue!10]
{\bf Relationship to Thermodynamics}

In Thermodynamics,
$U$ is the internal energy
and $S$ is the entropy
of a system
at {\bf temperature} $T$.
Define $\theta\in\RR$ to be
the inverse temperature 
\beq
\theta = \frac{1}{T}
\eeq
Define the {\bf free energy} $F$ by\footnote{
In this chapter,
we 
call $-\ln Z$
the free energy too.}
\beqa
F&=& -T \ln Z_\theta
\\
&=&
-T\ln 
\sum_x e^{-\frac{\eps(x)}{T}}
\eeqa
Then

\beq
U-TS = F
\eeq
So the free energy equals
the internal energy minus
the energy held in disordered form.
\end{mdframed}

\section{$-\ln Z_{\theta^*}$=
Minimum FE}

Suppose we consider
$P(x|\theta)$ 
for two 
different parameter
$\theta$ and $\ttheta$. Define
\beq
p(x) = P(x|\theta), 
\;
\tP(x)=P(x|\TIL{\theta})
\eeq
The Kullback-Leibler 
divergence is always non-negative so:

\beqa
0&\leq& D_{KL}(\tP(x))
\parallel p(x))
\\
&=&
\sum_x \tP(x)\ln
\frac{\tP(x)}
{p(x)}
\\
&=&
-\TIL{S}
-\sum_x \tP(x)
\left[
-(\theta)^T\eps(x)-\ln Z_{\theta}
\right]
\\
&=&
-\TIL{S}
+(\theta)^T\TIL{U} + \ln Z_{\theta}
\quad\text{($\TIL{S}, \TIL{U}$ correspond to parameter $\ttheta$)}
\eeqa

\beq
-\ln Z_\theta\leq 
-\TIL{S}
+(\theta)^T\TIL{U}
\eeq

Let

\beq
\theta^*=
\argmin_\theta\left[
-\TIL{S}
+(\theta)^T\TIL{U}\right]
\eeq
Henceforth,
we will refer to
$-\ln Z_\theta$ as the 
{\bf FE (Free Energy)}
and to 
$-\ln Z_{\theta^*}$
as the
{\bf minimum FE}.

Relationship to 
convex/concave dual functions\footnote{
Convex/concave dual functions are discussed
in Chapter \ref{ch-var-bay-medical}}

\beq
\TIL{S} = \min_{\theta}\left[
(\theta)^T\TIL{U} + \ln Z_{\theta}\right]
\eeq

\beq
-\ln Z_{\theta}=
\min_{\TIL{U}}\left[
(\theta)^T\TIL{U} -\TIL{S}
\right]
\eeq
$\TIL{S}$ and $-\ln Z_\theta$ are
concave dual functions.

\beq
-\ln Z_{\theta^*}=
(\theta^*)^T\TIL{U} -\TIL{S}
\label{eq-duality-theta-star}
\eeq

\section{$-\ln Z^{tree}_\theta$=Tree FE
(a.k.a. Bethe FE)}

$\tP()x)$
is said to satisfy 
{\bf Mean Field Approximation (MFA)
or independent variables approximation (IVA)}
if its
variables $x_i$
are independently
distributed: 
\beq
\tP^{ind}(x)=
\prod_
k
\tP(x_k)
\eeq
In the MFA,
the entropy is

\beqa
\TIL{S}^{ind}
&=&
-\sum_x\tP(x)\ln \prod_i \tP(x_i)
\\
&=&
-\sum_{x_i} \tP(x_i)\ln \tP(x_i)
\\
&=&
\sum_i \TIL{H}(\rvx_i)
\eeqa

A slightly more 
complicated case than the
MFA is when $\tP(x)$
is defined on
a tree graph $G^{tree}$
with edges $i-j$.
In such a case,
it will take just
a few examples of trees
 $G^{tree}$
to convince the reader 
that in general, for 
any tree  $G^{tree}$,
$\tP(x)$
must have the following form:


\beq
\tP^{tree}(x)=
\tP^{ind}(x)
\prod_
{i-j}
\frac{
\tP (x_i
, x_j )}{
\tP(x_i)\tP (x_j )}
\label{eq-tP-tree}
\eeq
Hence,

\beqa
\TIL{S}^{tree}
&=&-\sum_i\sum_{x_i}
\tP^{tree}(x_i)\ln \tP^{tree}(x_i)
\\
&=&
\sum_i\TIL{H}(\rvx_i)
-
\sum_{i-j}\sum_{x_i, x_j}
\tP(x_i,x_j)
\ln \frac{\tP(x_i, x_j)}
{\tP(x_i)\tP(xj)}
\\
&=&
\sum_i \TIL{H}(\rvx_i)
-
\sum_{i-j} \TIL{H}(\rvx_i:\rvx_j)
\eeqa

Note that $\TIL{S}^{tree}$
can be written in terms of the
joint entropy $\TIL{H}(\rvx_i, \rvx_j)$
instead of the mutual entropy
$\TIL{H}(\rvx_i:\rvx_j)$.

\beq
\TIL{S}^{tree}=
-\sum_i(d_i-1)\TIL{H}(\rvx_i)
+ \sum_{i-j}\TIL{H}(\rvx_i, \rvx_j)
\eeq
where $d_i$
is the number of neighbors of node $i$.

The following
approximation
is often called the {\bf Bethe approximation}

\beq
-\ln Z_{\theta^*}
\approx -\ln Z_{\theta^*}^{tree}
\eeq



\section{ 
$-\ln Z^{tree}_{\theta^*}$=
Tree Minimum FE,
and message passing}
In this section, we 
will evaluate
$ -\ln Z_{\theta^*}^{tree}$
exactly
using a message passing ansatz (an
ansatz is an initial guess).

If we replace $\TIL{S}$ by
$\TIL{S}^{tree}$ in Eq.(\ref{eq-duality-theta-star}),
we get 

\beq
-\ln Z^{tree}_{\theta^*}=\min_{\TIL{U}}\left[
(\theta)^T\TIL{U} -\TIL{S}^{tree}
\right]
\label{eq-ln-z-tree-star}
\eeq
But note that

\beqa
(\theta)^T\TIL{U} &=&
\sum_i \theta_i\sum_x \tP(x)\eps_i(x)
\\
&=&
\sum_x\tP(x)
\underbrace{\sum_i \theta_i \eps_i(x)}
_{\text{ call } \Theta(x)}
\eeqa
so Eq.(\ref{eq-ln-z-tree-star}) becomes

\beq
-\ln Z^{tree}_{\theta^*}=
\min_{\tP}\left[
\sum_x \tP(x)\Theta(x)
-\sum_i\TIL{H}(\rvx_i)
+\sum_{i-j}\TIL{H}(\rvx_i:\rvx_j)
\right]
\eeq
subject to $\sum_x \tP(x)=1$
and $\tP(x)\geq 0$ for all $x$.

\begin{claim}
$-\ln Z^{tree}_{\theta^*}$
is achieved if

\beq
\tP(x) = \caln(!x)e^{-\Theta(x)}
\eeq

\beq
\Theta(x)
=
\sum_i \Theta(x_i)
+\sum_{i-j}\Theta(x_i, x_j)
\eeq
(This form for $\tP(x)$ and 
$\Theta(x)$ agrees with Eq.(\ref{eq-tP-tree}))


\beq
m_{t\rdart s}(x_s)=
e^{\lam_{t\rdart s}(x_s)}
\eeq

\beq
\tP(x_i)
=\caln(!x)
e^{-\Theta(x_i)}
\prod_{a\in \partial i}
m_{a\rdart i}(x_i)
\eeq

\beq
\tP(x_i, x_j)=
\caln(!x)
e^{-\Theta(x_i, x_j) 
- \Theta(x_i)
-\Theta(x_j)}
\left[\prod_{a\in \partial i
\setminus j} m_{a\rdart i}(x_i)
\right]
\left[\prod_{b\in \partial j
\setminus i} m_{b\rdart j}(x_j)
\right]
\eeq
\end{claim}
\proof

We want to minimize
the following Lagrangian
with respect to variations
$\delta\tP(x_i)$ 
of $\tP(x)$,
for each $i$.


\beq
\call=
\left\{
\begin{array}{l}
\sum_x \tP(x)\Theta(x)
\\
+\sum_i (1-d_i) 
\sum_{x_i}
\tP(x_i)
\ln
\tP(x_i)
\\
+\sum_{i-j}\sum_{x_i, x_j}\tP(x_i, x_j)
\ln \tP(x_i, x_j)
\\
+\lam\left[
\sum_x\tP(x)-1
\right]
\end{array}
\right\}
\eeq
The term proportional to
the Lagrange multiplier $\lam$
enforces the constraint $\sum_x \tP(x)=1$.
Note that in general,

\beq
\delta\tP(x)=
\sum_i \delta\tP(x_i)
\eeq
so if we only vary $\tP(x_i)$,

\beqa
\delta F(\tP(x))
&=&
\pder{F(\tP(x))}{\tP(x)}\delta\tP(x)
\\
&=&
\pder{F(\tP(x))}{\tP(x)}\delta\tP(x_i)
\eeqa
for any
well behaved function $F:\RR\rarrow \RR$.
Hence


\beq
\delta\call=
\sum_{x }\delta\tP(x _i)
\left\{
\begin{array}{l}
\Theta(x )
\\
+\sum_i (1-d_i) 
\left[1+\ln
\tP(x_i )\right]
\\
+\sum_{i-j}
\left[1+
\ln \tP(x_i , x_j )\right]
\\
+\lam
\end{array}
\right\}
\eeq
for any variation $\delta\tP(x_i)$.
Setting the coefficient of 
$\delta\tP(x_i)$ to zero now yields

\beq
0=
\left\{
\begin{array}{l}
\Theta(x)
\\
+\sum_i  
\left[1+\ln
\tP(x_i)\right]
\\
+\sum_{i-j}
\left[1+
\ln \frac{\tP(x_i, x_j)}
{\tP(x_i)\tP(x_j)}
\right]
\\
+\lam
\end{array}
\right\}
\eeq
for each $i$.
If we now substitute
the equations
that are hypotheses to this claim,
we get




\beq
0=
\left\{
\begin{array}{l}
\Theta(x)
\\
-\sum_i \Theta(x_i)
\\
-\sum_{i-j}\Theta(x_i, x_j)
\end{array}
\right\}
\eeq

\beqa
0&=&
\left\{
\begin{array}{l}
+\sum_i \ln 
\prod_{a\in \partial i}m_{a\rdart i}(x_i)
\\
+\sum_{i-j}\ln 
\frac{\left[\prod_{a\in \partial i
\setminus j} m_{a\rdart i}(x_i)
\right]
\left[\prod_{b\in \partial j
\setminus i} m_{b\rdart j}(x_j)
\right]
}{
\prod_{a\in \partial i}m_{a\rdart i}(x_i)
\prod_{b\in \partial j}m_{b\rdart j}(x_j)
}
\\
+\lam
\end{array}
\right\}
\\
&=&
\left\{
\begin{array}{l}
\sum_i
\sum_{a\in \partial i} \lam_{a\rdart i}(x_i)
\\
-
\sum_{i-j}\left[
\lam_{j\rdart i}(x_i)+ \lam_{i\rdart j}(x_j)
\right]
\\
+\lam
\end{array}
\right\}
\\
&=&
\lam
\eeqa
So the hypotheses to this claim
indeed do satisfy $\delta \call=0$
for all variations $\delta\tP(x_i)$,
for each $i$,
with Lagrange multiplier $ \lam=0$.
\qed








