\chapter{Frisch-Waugh-Lovell (FWL) theorem}
\label{ch-fwl-theo}

The Frisch-Waugh-Lovell (FWL) theorem
(see Ref.\cite{wiki-fwl-theo})
(mnemonic: FoWL Theorem)
is a method used in Linear Regression (LR).
It allows us to
calculate,
for LR with
two features $\rvx_1$ and $\rvx_2$,
the regression coefficient 
of feature $\rvx_2$
by conditioning
on feature $\rvx_1$.

As in Chapter \ref{ch-linear-reg}
on LR, we will consider
two cases: $x^\s$ non-random, and $x^\s$ random i.i.d..

\section{FWL, assuming $x^\s$ are non-random}
Suppose
\beq
y=  X_1\beta_1 + X_2\beta_2 + \eps
\label{eq-y-x1-x2-eps}
\eeq
where

$y, \eps\in \RR^{nsam}$

$X_a\in \RR^{nsam\times k_a}$,
$\beta_a\in \RR^{k_a}$ for $a=1,2$

Define the matrices $U_1$ and $A_1$ by

\beq
U_1 = X_1(X_1^TX_1)^{-1}X_1^T
\eeq
and

\beq
A_1 = 1-U_1
\;.
\eeq
Note that

\beq
U_1X_1=X_1\;,\;\;A_1X_1=0
\eeq
(mnemonic: $A_1$ Annihilates $X_1$,
and $U_1$ acts like Unity on $X_1$).

Applying $A_1$ to Eq.(\ref{eq-y-x1-x2-eps}) gives

\beq
A_1 y = A_1X_2\beta_2  + A_1\eps
\eeq
so we can estimate $\beta_2$ by

\beq
\HAT{\beta}_2=
(A_1X_2)^{-1}A_1 y
\label{eq-fwl-nonrand}
\;.
\eeq

\section{FWL, assuming $x^\s$ are random}

Assume for simplicity that 
$k_1=k_2=1$
in Eq.(\ref{eq-y-x1-x2-eps}). Let
$\beta_1,\beta_2\in\RR$.
When the $x^\s$ are random and i.i.d.,
the
$X_1, X_2$ are
replaced by 
the random variables
$\rvx_1, \rvx_2\in \RR$,
and
Eq.(\ref{eq-y-x1-x2-eps})
becomes

\beq
\rvy = \beta_1 \rvx_1 + 
\beta_2\rvx_2 + \rveps_\rvy
\label{eq-y-x1-x2-rand}
\eeq

Fig.\ref{fig-fwl-ab}
shows two LDEN bnets in
which Eq.(\ref{eq-y-x1-x2-rand})
can arise.


\begin{figure}[h!]
$$
\begin{array}{cc}
\xymatrix{
\rveps_{\rvx_2}\ar[dd]
&\rveps_{\rvx_1}\ar[d]
&\rveps_{\rvy}\ar[dd]
\\
&\rvx_1\ar[dr]^{\beta_1}
\ar[dl]_\alp
\\
\rvx_2\ar[rr]_{\beta_2}
&&\rvy
}
&
\xymatrix{
\rveps_{\rvx_2}\ar[dd]
&\rveps_{\rvx_1}\ar[d]
&\rveps_{\rvy}\ar[dd]
\\
&\rvx_1\ar[dr]^{\beta_1}
\\
\rvx_2\ar[rr]_{\beta_2}
\ar[ur]^\lam
&&\rvy
}
\\
(a)&(b)
\end{array}$$
\caption{LDEN bnets for discussing the FWL theorem.
$(a)$ and $(b)$ only
differ in the direction of the 
arrow between $\rvx_1$
and $\rvx_2$.
}
\label{fig-fwl-ab}
\end{figure}

The structural equations,
printed in blue,
for the 2 bnets of Fig.\ref{fig-fwl-ab},
are as follows:

For $(a)$,
\beq
\color{blue}
\left\{
\begin{array}{l}
\rvx_1 = \rveps_{\rvx_1}
\\
\rvx_2 = \alp\rvx_1 +\rveps_{\rvx_2}
\\
\boxed{\rvy = \beta_1 \rvx_1 + 
\beta_2\rvx_2 + \rveps_\rvy}
\end{array}
\right.
\eeq

For $(b)$,
\beq
\color{blue}
\left\{
\begin{array}{l}
\rvx_1 = \lam\rvx_2+\rveps_{\rvx_1}
\\
\rvx_2 = \rveps_{\rvx_2}
\\
\boxed{\rvy = \beta_1 \rvx_1 + 
\beta_2\rvx_2 + \rveps_\rvy}
\end{array}
\right.
\eeq
What we are going to say next
depends only on the boxed
equation, so it applies
equally to cases $(a)$ and $(b)$.

Assume
\beq
\av{\rveps_\rvy}=0
\eeq
Note that

\beq
\av{\rvx_j, \rveps_\rvy}=0
\eeq
because the path from 
$\rvx_j$ to $\rveps_\rvy$ is 
blocked by a collider.
Hence

\beq
\av{\rvx_2, \rvy}^{|x_1}= \beta_2 \av{\rvx_2, \rvx_2}^{|x_1}
\eeq

\beq
\beta_2 = 
\left[\frac{\av{\rvx_2, \rvy}}
{\av{\rvx_2, \rvx_2}}\right]^{|x_1}
=
\left[\pder{}{\rvx_2}\right]^{|x_1}
\rvy
\eeq


