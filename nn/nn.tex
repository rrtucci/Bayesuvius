\chapter{Neural Networks}\label{ch-nn}

In this chapter, we discuss
 Neural Networks (NNs) of the
feedforward kind,
which is the most popular kind. In their
 plain, vanilla form, NNs only
have deterministic nodes.
But the nodes of a bnet can
be deterministic too, because
the TPM
of a node
can reduce to a delta function.
Hence, NNs should be expressible
as bnets. We will confirm this
in this chapter.

Henceforth in this chapter,
if we replace an index of an
indexed quantity by a dot,
it will mean the collection
of the indexed quantity
for all values of that
index. For example, $x.$
will mean the
array of $x_i$ for all $i$.


\begin{figure}[h!]
\centering
$$\xymatrix{
\rvx_0\ar@/^1pc/[rr]
\ar@/^2pc/[rrr]
\ar[d]\ar[dr]\ar[drr]\ar[r]&
\rvx_1\ar@/^1pc/[rr]
 \ar[dl]\ar[d]\ar[dr]\ar[r]&
\rvx_2\ar[dll]\ar[dl]\ar[d]\ar[r]&
\rvx_3\ar[dlll]\ar[dll]\ar[dl]\\
\rvh_0^0\ar[d]\ar[dr]&
\rvh_1^0\ar[dl]\ar[d]&
\rvh_2^0\ar[dll]\ar[dl]\\
\rvh_0^1\ar[d]\ar[dr]&
\rvh_1^1\ar[dl]\ar[d]\\
\rvY_0&
\rvY_1
}$$
\caption{Neural Network (feed forward)
with 4 layers: input layer $\rvx.$,
2 hidden layers $\rvh^0.$,
$\rvh^1.$ and
output layer $\rvY.$ }
\label{fig-nn}
\end{figure}

Consider Fig.\ref{fig-nn}.

$\rvx_i\in
\{0,1\}$ for
$i=0, 1, 2, \dots,nx-1$
is the \textbf{input layer}.

$\rvh_i^\lam\in \RR$ for
$i=0, 1, 2, \dots,nh(\lam)-1$
is the $\lam$\textbf{-th hidden layer}.
$\lam=0, 1, 2, \ldots, \Lambda-2$.
A NN is said to be {\bf deep} if
$\Lambda>2$; i.e., if it has
more than one hidden layer.

$\rvY_i\in \RR$ for
$i=0, 1, 2, \dots,ny-1$
is the \textbf{output layer}.
We use a upper case y
here because in the training phase,
we will use pairs $(x.[\sigma],y.[\sigma])$ where
$y_i[\sigma]\in \{0,1\}$
for $i=0, 1, \ldots, ny-1$.
$Y=\HAT{y}$
is an estimate of $y$.
Note that lower case y is
either 0 or 1,
but upper case y may be
any real. Often, the
activation
functions are chosen so that
$Y\in[0,1]$.


The number of nodes in each layer
and the number of layers are arbitrary.
Fig.\ref{fig-nn} is fully connected
(a.k.a. dense), meaning that every node
of a layer is impinged
arrow coming
from every node of the preceding
layer. Later on in this chapter,
we will
discuss non-dense layers.

Let
  $w^\lam_{i|j}, b_i^\lam\in \RR$
be given,
for $i\in\ZZ_{[0, nh(\lam))}$,
$j\in\ZZ_{[0, nh(\lam-1))}$,
and $\lam\in\ZZ_{[0, \Lambda)}$.

The TPMs,
printed in blue, for
 bnet
Fig.\ref{fig-nn},
are as follows.


\beq\color{blue}
P(x_i\cond x_{i-1},
x_{i-1},\dots, x_0)\text{ = given}
\eeq

\beq\color{blue}
P(h^{\lam}_i\cond h^{\lam-1}_.)=
\delta\left(h^{\lam}_i,
\cala_i^\lam(\sum_j w^{\lam}_{i|j}
h^{\lam-1}_j + b^{\lam}_i)\right)
\;,
\eeq
where $P(h^0_i|h^{-1})=P(h^0_i|x)$.

\beq\color{blue}
P(Y_i\cond h^{\Lambda-2}_.)=
\delta \left(Y_i,
\cala_i^{\Lambda-1}(\sum_j w^{\Lambda-1}_{i|j}
h^{\Lambda-2}_j + b^{\Lambda-1}_i)\right)
\;.
\eeq


\section{Activation Functions
$\cala_i^\lam:\RR\rarrow \RR$}
\label{sec-activation-fun}

Activation functions must be
nonlinear. Why? Because if
they  were all linear,
the NN mapping  would be a bijection (1-1 onto map), and its
domain and
range
would be the same.
That is not what you want for
a classifier.
For a classifier, you want the range
to be much smaller than the domain.


\begin{itemize}
\item {\bf Step function (Perceptron)}

\beq\
\cala(x)=\indi(x>0)
\eeq
Zero for $x\leq 0$, one for $x>0$.

\item {\bf Sigmoid function}

\beq
\cala(x)=\frac{1}{1+e^{-x}}=\smoid(x)
\eeq
Smooth, monotonically increasing
function.
$\smoid(-\infty)=0$,$\smoid(0)=1/2$,
$\smoid(\infty)=1$.

\item {\bf Hyperbolic tangent}
\beq
\cala(x)=\tanh(x)=\frac{e^x-e^{-x}}
{e^x+e^{-x}}
\eeq
Smooth,
 monotonically increasing function.
$\tanh(-\infty)=-1$,$\tanh(0)=0$,
$\tanh(\infty)=1$.

Odd function:
\beq
\tanh(-x)=-\tanh(x)
\eeq

Whereas $\smoid(x)\in [0,1]$,
$\tanh(x)\in[-1,1]$.


\item {\bf ReLU (Rectified Linear Unit)}

\beq
\cala(x)=\underbrace{x\indi(x>0)}_{x_+}= \max(0, x)
\;.
\eeq
Compare this to the step function
$\indi(x>0)$.

\item {\bf Swish}
\beq
\cala(x)=x\;\smoid(x)
\eeq
\item {\bf Softmax}

\beq
\cala(x_i
|x.)=\frac{e^{x_i}}{\sum_i e^{x_i}}=
\softmax(x.)(i)
\eeq

The softmax definition implies
that the bnet nodes
 within a softmax layer
are fully connected by arrows
to form a \qt{clique}.

\end{itemize}

\section{Weight
optimization via
supervised training and
gradient descent}

The bnet of Fig.\ref{fig-nn}
is used for classification
of a single data point $x.$.
It assumes that the
weights $w^\lam_{i|j}, b_i^\lam$
are given.

To find the optimum
weights via supervised
training and gradient descent,
one uses the bnet Fig.\ref{fig-nn-ext}.

In Fig.\ref{fig-nn-ext},
the nodes in
Fig.\ref{fig-nn} become
sampling space vectors.
For example, $\rvx.$ becomes
$\ranvec{x.}$, where the
components of
$\ranvec{x.}$ in sampling space are
$\rvx.[\sigma]\in \{0,1\}^{nx}$
for $\sigma=0, 1, \ldots, nsam(\vecx)-1$.
$nsam(\vecx)$
is the number of
samples in the whole dataset.


To train a NN bnet with a dataset,
the standard procedure
is to split the dataset into 3 parts
(I like to call them the {\bf ttt datasets}):
\begin{enumerate}
\item
{\bf training dataset},
\item
{\bf tuning (a.k.a. validation) dataset}, for
tuning
of hyperparameters
like $nsam(\vecx)$,  $\Lambda$,
and $nh(i)$
for each $i$.
\item
{\bf testing dataset}
\end{enumerate}

Weights only change while training on the training dataset.
While the model is being trained, its performance is 
periodically tested on the tuning dataset. Training 
continues until performance on the tuning dataset 
no longer improves. After that happens, the model 
is finally applied to the testing dataset.

The training dataset is
split into batches.
An {\bf epoch} is a pass through all
the batches in the training dataset.

Define
\beq
W^\lam_{i|j}=[w^\lam_{i|j}, b^\lam_i]
\;.
\eeq

The
TPMs,
printed in blue, for
 bnet
Fig.\ref{fig-nn-ext},
are as follows.

\begin{figure}[h!]
\centering
$$\xymatrix{
&&\ranvec{x.}\ar[d]\ar[r]&
\ranvec{y.}\ar[dddd]\\
&\rvW^0_{.|.}, \ar[r]&
\ranvec{h^0_.}\ar[d]\\
&\rvW^1_{.|.}\ar[r]&
\ranvec{h^1_.}\ar[d]\\
&\rvW^2_{.|.}\ar[r]&
\ranvec{h^2_.}\ar[d]\\
\rvW^._{.|.}\ar[ru]\ar[ruu]\ar[ruuu]
\ar@/_2pc/[rrrr]
&&
\vec{\rvY}.\ar[r]&\cale\ar[r]&
(\rvW')^._{.|.}
}$$
\caption{bnet
for
finding optimum
weights of the bnet
Fig.\ref{fig-nn} via
supervised training
and gradient descent.
}
\label{fig-nn-ext}
\end{figure}

\beq\color{blue}
P(x.[\sigma])
\text{ = given}
\;.
\eeq

\beq\color{blue}
P(y.[\sigma]\cond x.[\sigma])
\text{ = given}
\;.
\eeq

\beq\color{blue}
P(h^{\lam}_i[\sigma]\cond h^{\lam-1}_.[\sigma])=
\delta\left(h^{\lam}_i[\sigma],
\cala_i^\lam(\sum_j w^{\lam}_{i|j}
h^{\lam-1}_j[\sigma] + b^{\lam}_i)\right)
\eeq

\beq\color{blue}
P(Y_i[\sigma]\cond h^{\Lambda-2}_.[\sigma])=
\delta\left(Y_i[\sigma],
\cala_i^{\Lambda-1}(\sum_j w^{\Lambda-1}_{i|j}
h^{\Lambda-2}_j[\sigma] + b^{\Lambda-1}_i)\right)
\eeq

\beq\color{blue}
P(W^._{.|.})\text{ = given}
\eeq
The first time it is used,
 $W^._{.|.}$ is arbitrary.
After the first time, it is determined
by previous stage.



\beq\color{blue}
P(W^\lam_{.|.}|W^._{.|.})
=
\delta(W^\lam_{.|.},
(W^._{.|.})^\lam)
\eeq

\beq\color{blue}
P(\cale|\vec{y}., \vec{Y}.)=
\frac{1}{nsam(\vecx)}
\sum_\sigma\sum_i d(y_i[\sigma], Y_i[\sigma])
\;,
\eeq
where

\beq
d(y,Y)=|y-Y|^2
\;.
\eeq
If $y, Y\in [0,1]$,
one can use this instead

\beq
d(y,Y)=XE(y\rarrow Y)=
-y\ln Y - (1-y)\ln (1-Y)
\;.
\eeq

\beq\color{blue}
P((W')^\lam_{i|j}|\cale, W^._{.|.})
=
\delta((W')^\lam_{i|j},
W_{i|j}^\lam -\eta
\partial_{W_{i|j}^\lam} \cale
)
\eeq
$\eta>0$ is called the learning rate.
This method of minimizing the
error $\cale$ is called
gradient descent.
$W'-W=\Delta W =-\eta  \partial_W \cale$
so $\Delta \cale=\frac{-1}{\eta}
(\Delta W)^2<0$.

\section{Non-dense layers}


The TPM for
a non-dense layer is of the
form:

\beq\color{blue}
P(h^\lam_i[\sigma]\cond h^{\lam-1}_.[\sigma])=
\delta(h^\lam_i[\sigma],H^\lam_i[\sigma])
\;,
\eeq
where
$H^\lam_i[\sigma]$ will
be specified below for each type of
non-dense layer.

\begin{itemize}
\item{\bf Dropout Layer}

The dropout layer was
invented in Ref.\cite{dropout}.
To dropout nodes from a fixed
layer $\lam$:
For all $i$ of layer $\lam$,
define a new node $\rvr^\lam_i$
with an arrow
$\rvr^\lam_i\rarrow\rvh^\lam_i$.
For $r\in \{0,1\}$,
and some $p\in (0,1)$, define

\beq\color{blue}
P(r^\lam_i=r)=[p]^r
[1-p]^{1-r}
\text{ (Bernoulli dist.)}
\;.
\eeq
Now one has

\beq \color{blue}
P(h^\lam_i[\sigma]\cond h^{\lam-1}_.[\sigma], r^\lam_i)=
\delta(h^\lam_i[\sigma],H^\lam_i[\sigma])
\;,
\eeq
where

\beq
H^\lam_i[\sigma]=
\cala^\lam_i(
r^\lam_i\sum_j w^\lam_{i|j}h^{\lam-1}_j[\sigma]
+b^\lam_i
)
\;.
\eeq

This reduces overfitting.
Overfitting might
occur if the weights follow too closely
several similar batches.
This dropout procedure adds a random
component to each batch
making groups of similar batches
less likely.

The random $\rvr^\lam_i$ nodes
that induce dropout are
only used in the training bnet Fig.\ref{fig-nn-ext},
not in the classification bnet Fig.\ref{fig-nn}.
We prefer to remove the
$\rvr^\lam_i$ stochasticity from classification
and for Fig.\ref{fig-nn} to act as an average
over sampling space of Fig.\ref{fig-nn-ext}.
Therefore,
if weights $w^\lam_{i|j}$ are obtained
for a dropout layer $\lam$ in Fig.\ref{fig-nn-ext},
then that layer is used in Fig.\ref{fig-nn} with
no $\rvr^\lam_i$ nodes but
with weights $\av{r^\lam_i}w^\lam_{i|j}=
pw^\lam_{i|j}$.


Note that dropout adds non-deterministic
nodes to a NN,
which in their vanilla form only have
deterministic nodes.


\item {\bf Convolutional Layer}

\begin{itemize}
\item 1-dim

Filter function $\calf:\{0, 1, \ldots,
nf-1\}\rarrow \RR$.

$\sigma$=stride length

For $i\in \{0,1,\dots,nh(\lam)-1\}$,
let

\beq
H^\lam_i[\sigma]=
\sum_{ j=0}^{nf-1}
h^{\lam-1}_{j+i\sigma}[\sigma] \calf(j)
\;.
\label{eq-conv1}
\eeq
For the indices not to
go out of bounds in Eq.(\ref{eq-conv1}),
we must have

\beq
nh(\lam-1)-1=nf-1 +
(nh(\lam)-1)\sigma
\;
\eeq
so
\beq
nh(\lam)=\frac{1}{\sigma}[nh(\lam-1)-
nf] + 1
\;.
\eeq
\item 2-dim

$h_i^\lam[\sigma]$ becomes
$h_{(i,j)}^\lam[\sigma]$.
Do 1-dim convolution
along both $i$ and $j$ axes.

\end{itemize}
\item{\bf Pooling Layers
(MaxPool, AvgPool)}

Here each node $i$
of layer $\lam$ is impinged by
arrows from  a subset $Pool(i)$
of the set of all
nodes of the previous layer $\lam-1$.
Partition set
$\{0,1,\dots,nh(\lam-1)-1\}
$ into $nh(\lam)$ mutually
disjoint, nonempty sets
called $Pool(i)$, where
$i\in \{0, 1, \ldots,nh(\lam)-1\}$.

\begin{itemize}
\item AvgPool
\beq
H^\lam_i[\sigma]=\frac{1}
{|Pool(i)|}
\sum_{j\in Pool(i)}h^{\lam-1}_j[\sigma]
\eeq
\item MaxPool
\beq
H^\lam_i[\sigma]=
\max_{j\in Pool(i)}h^{\lam-1}_j[\sigma]
\eeq

\end{itemize}


\end{itemize}

\section{Autoencoder NN}


If the sequence

\beq
nx, nh(0), nh(1), \ldots,
nh(\Lambda-2),ny
\eeq
first decreases monotonically
up to layer $\lam_{min}$, then
increases monotonically until
$ny=nx$, then
the NN is called an {\bf
autoencoder NN}.
Autoencoders
are  useful for unsupervised learning
and
feature reduction. In this case,
$Y$ estimates $x$.
The layers before
layer $\lam_{min}$
are called the {\bf encoder},
and those after
$\lam_{min}$ are called the
{\bf decoder}.
Layer $\lam_{min}$
is called the {\bf code}.
