\chapter{Neural Networks}

In this chapter, we discuss
 Neural Networks (NNs) of the
feedforward kind,
which is the most popular kind. In their
 plain, vanilla form, NNs only
have deterministic nodes.
But the nodes of a bnet can
be deterministic too, because
the transition probability matrix
of a node
can reduce to a delta function.
Hence, NNs should be expressible
as bnets. We will confirm this
in this chapter.

Henceforth in this chapter,
if we replace an index of an
indexed quantity by a dot, 
it will mean the collection
of the indexed quantity
for all values of that
index. For example, $x.$
will mean the 
array of $x_i$ for all $i$.


\begin{figure}[h!]
\centering
$$\xymatrix{
\rvx_0\ar@/^1pc/[rr]
\ar@/^2pc/[rrr]
\ar[d]\ar[dr]\ar[drr]\ar[r]&
\rvx_1\ar@/^1pc/[rr]
 \ar[dl]\ar[d]\ar[dr]\ar[r]&
\rvx_2\ar[dll]\ar[dl]\ar[d]\ar[r]&
\rvx_3\ar[dlll]\ar[dll]\ar[dl]\\
\rvh_0^0\ar[d]\ar[dr]&
\rvh_1^0\ar[dl]\ar[d]&
\rvh_2^0\ar[dll]\ar[dl]\\
\rvh_0^1\ar[d]\ar[dr]&
\rvh_1^1\ar[dl]\ar[d]\\
\rvY_0&
\rvY_1
}$$
\caption{Neural Network (feed forward)
with 4 layers: input layer $\rvx.$,
2 hidden layers $\rvh^0.$,
$\rvh^1.$ and
output layer $\rvY.$ }
\label{fig-nn}
\end{figure}

Consider Fig.\ref{fig-nn}.

$\rvx_i\in 
\{0,1\}$ for 
$i=0, 1, 2, \dots,numx-1$
is the \textbf{input layer}.

$\rvh_i^\lam\in \RR$ for 
$i=0, 1, 2, \dots,numh(\lam)-1$
is the $\lam$\textbf{-th hidden layer}.
$\lam=0, 1, 2, \ldots, \Lambda-1$.
A NN is said to be {\bf deep} if
$\Lambda>1$; i.e., if it has 
more than one hidden layer.

$\rvY_i\in \RR$ for 
$i=0, 1, 2, \dots,numy-1$
is the \textbf{output layer}.
We use a upper case y
here because in the training phase,
we will use pairs $(x.[s],y.[s])$ where
$y_i[s]\in \{0,1\}$ 
for $i=0, 1, \ldots, numy-1$.
$Y=\hat{y}$
is an estimate of $y$.
Note that lower case y is 
either 0 or 1, 
but upper case y may be 
any real. Often, the
activation
functions are chosen so that
$Y\in[0,1]$. 
 

The number of nodes in each layer 
and the number of layers are arbitrary.
Fig.\ref{fig-nn} is fully connected 
(aka dense), meaning that every node
of a layer is impinged 
arrow coming 
from every node of the preceding
layer. Later on in this chapter,
we will
discuss non-dense layers.

Let
  $w^\lam_{i|j}, b_i^\lam\in \RR$
be given,
for $i\in\ZZ_{[0, numh(\lam))}$,
$j\in\ZZ_{[0, numh(\lam-1))}$, 
and $\lam\in\ZZ_{[0, \Lambda)}$.

These are the
transition probability matrices,
printed in blue, for 
the nodes of the bnet 
Fig.\ref{fig-nn}:
 

\beq\color{blue}
P(x_i\cond x_{i-1},
x_{i-1},\dots, x_0)\text{ = given}
\eeq

\beq\color{blue}
P(h^{\lam}_i\cond h^{\lam-1}_.)=
\delta\left(h^{\lam}_i,
\cala_i^\lam(\sum_j w^{\lam-1}_{i|j}
h^{\lam-1}_j + b^{\lam-1}_i)\right)
\eeq

\beq\color{blue}
P(Y_i\cond h^{\Lambda-1}_.)=
\delta\left(Y_i,
\cala_i^\Lambda(\sum_j w^{\Lambda-1}_{i|j}
h^{\Lambda-1}_j + b^{\Lambda-1}_i)\right)
\eeq

\section*{Activation Functions
$\cala_i^\lam:\RR\rarrow \RR$}

Activation functions must be
nonlinear.

\begin{itemize}
\item {\bf Step function (Perceptron)}

\beq
\cala(x)=\indi(x>0)
\eeq
Zero for $x\leq 0$, one for $x>0$.

\item {\bf Sigmoid function}

\beq
\cala(x)=\frac{1}{1+e^{-x}}
\eeq
Smooth, monotonically increasing 
function.
$\cala(-\infty)=0$,$\cala(0)=0.5$,
$\cala(\infty)=1$

\item {\bf Hyperbolic tangent}
\beq
\cala(x)=\tanh(x)
\eeq
As with sigmoid: smooth,
 monotonically increasing function,
goes
from 0 to 1 as $x$ goes 
from $-\infty$ to
$\infty$.
\item {\bf ReLU (Rectified Linear Unit)}

\beq
\cala(x)=x\indi(x>0)= \max(0, x)
\;.
\eeq
Compare this to the step function.

\item {\bf Swish}
\beq
\cala(x)=x*sigmoid(x)
\eeq
\item {\bf Softmax}

\beq
\cala(x_i
|x.)=\frac{e^{x_i}}{\sum_i e^{x_i}}
\label{eq-softmax}
\eeq
It's called softmax because if we 
approximate the exponentials,
 both in the numerator and denominator
of Eq.(\ref{eq-softmax}),
by the largest one,
we get

\beq
\cala(x_i|x.)\approx \indi(x_i=\max_k x_k)
\;.
\eeq

The softmax definition implies
that the bnet nodes
 within a softmax layer
are fully connected by arrows
to form a ``clique".

\end{itemize}

\section*{Weight 
optimization via
supervised training and
gradient descent}

The bnet of Fig.\ref{fig-nn}
is used for classification
of a single data point $x.$.
It assumes that the
weights $w^\lam_{i|j}, b_i^\lam$
are given.

To find the optimum
weights via supervised
training and gradient descent,
one uses the bnet Fig.\ref{fig-nn-ext}.

In Fig.\ref{fig-nn-ext},
the nodes in
Fig.\ref{fig-nn} become 
sampling space vectors.
For example, $\rvx.$ becomes
$\ranvec{x.}$, where the
components of 
$\ranvec{x.}$ in sampling space are
$\rvx.[s]\in \{0,1\}^{numx}$
for $s=0, 1, \ldots, nsam(\vecx)-1$.


$nsam(\vecx)$
is the number of
samples used to calculate the
gradient
during each {\bf stage (aka iteration)} of
Fig.\ref{fig-nn-ext}.
We will also  refer to
$nsam(\vecx)$ as the {\bf mini-batch size}.
A {\bf mini-batch} is a subset 
of the training data set.



To train a bnet with a data
set (d-set),
the standard procedure
is to split the d-set into 3 parts:
\begin{enumerate}
\item
{\bf training d-set}, 
\item
{\bf testing1 d-set}, for
tuning
of hyperparameters 
like $nsam(\vecx)$,  $\Lambda$,
and $nunh(i)$
for each $i$. 
\item
{\bf testing2 d-set}, for measuring
how well the model
tuned with the testing1 d-set
performs.
\end{enumerate}

The training d-set is 
itself split into mini-batches.
An {\bf epoch} is a pass through all 
the training d-set.

Define
\beq
W^\lam_{i|j}=[w^\lam_{i|j}, b^\lam_i]
\;.
\eeq

These are the
transition probability matrices,
printed in blue, for 
the nodes of the bnet 
Fig.\ref{fig-nn-ext}:

\begin{figure}[h!]
\centering
$$\xymatrix{
&&\ranvec{x.}\ar[d]\ar[r]&
\ranvec{y.}\ar[dddd]\\
&\rvW^0_{.|.}, \ar[r]&
\ranvec{h^0_.}\ar[d]\\
&\rvW^1_{.|.}\ar[r]&
\ranvec{h^1_.}\ar[d]\\
&\rvW^2_{.|.}\ar[r]&
\ranvec{h^2_.}\ar[d]\\
\rvW^._{.|.}\ar[ru]\ar[ruu]\ar[ruuu]
\ar@/_2pc/[rrrr]
&&
\vec{\rvY}.\ar[r]&\cale\ar[r]&
(\rvW')^._{.|.}
}$$
\caption{bnet 
for 
finding optimum
weights of the bnet 
Fig.\ref{fig-nn} via
supervised training
and gradient descent.
}
\label{fig-nn-ext}
\end{figure}

\beq\color{blue}
P(x.[s])
\text{ = given}
\;.
\eeq

\beq\color{blue}
P(y.[s]\cond x.[s])
\text{ = given}
\;.
\eeq

\beq\color{blue}
P(h^{\lam}_i[s]\cond h^{\lam-1}_.[s])=
\delta\left(h^{\lam}_i[s],
\cala_i^\lam(\sum_j w^{\lam-1}_{i|j}
h^{\lam-1}_j[s] + b^{\lam-1}_i)\right)
\eeq

\beq\color{blue}
P(Y_i[s]\cond h^{\Lambda-1}_.[s])=
\delta\left(Y_i[s],
\cala_i^\Lambda(\sum_j
 w^{\Lambda-1}_{i|j}
h^{\Lambda-1}_j[s] + b^{\Lambda-1}_i)\right)
\eeq

\beq\color{blue}
P(W^._{.|.})\text{ = given}
\eeq
The first time it is used,
 $W^._{.|.}$ is arbitrary.
After the first time, it is determined 
by previous stage.



\beq\color{blue}
P(W^\lam_{.|.}|W^._{.|.})
=
\delta(W^\lam_{.|.},
(W^._{.|.})^\lam)
\eeq

\beq\color{blue}
P(\cale|\vec{y}., \vec{Y}.)=
\frac{1}{nsam(\vecx)}
\sum_s\sum_i d(y_i[s], Y_i[s])
\;,
\eeq
where 

\beq
d(y,Y)=|y-Y|^2
\;.
\eeq
If $y, Y\in [0,1]$, 
one can use 

\beq
d(y,Y)=XE(y\rarrow Y)=
-y\log Y - (1-y)\log (1-Y)
\eeq
instead.

\beq\color{blue}
P((W')^\lam_{i|j}|\cale, W^._{.|.})
=
\delta((W')^\lam_{i|j},
W_{i|j}^\lam -\alpha
\partial_{W_{i|j}^\lam} \cale
)
\eeq
$\alpha>0$ is called the learning rate.

\section*{Non-dense layers}


The transition
probability matrix for
a non-dense layer is of the
form: 

\beq\color{blue}
P(h^\lam_i[s]\cond h^{\lam-1}_.[s])=
\delta(h^\lam_i[s],H^\lam_i[s])
\;,
\eeq
where
$H^\lam_i[s]$ will
be specified below for each type of
non-dense layer.

\begin{itemize}
\item{\bf Dropout Layer}

The dropout layer was
invented in Ref.\cite{dropout}.
To dropout nodes from a fixed 
layer $\lam$:
For all $i$ of layer $\lam$, 
define a new node $\rvr^\lam_i$
with an arrow 
$\rvr^\lam_i\rarrow\rvh^\lam_i$.
For $r\in \{0,1\}$, 
and some $p\in (0,1)$, define

\beq\color{blue}
P(r^\lam_i=r)=[p]^r
[1-p]^{1-r}
\text{ (Bernouilli dist.)}
\;.
\eeq
Now one has

\beq \color{blue}
P(h^\lam_i[s]\cond h^{\lam-1}_.[s], r^\lam_i)=
\delta(h^\lam_i[s],H^\lam_i[s])
\;,
\eeq
where

\beq
H^\lam_i[s]=
\cala^\lam_i(
r^\lam_i\sum_j w^\lam_{i|j}h^{\lam-1}_j[s]
+b^\lam_i
)
\;.
\eeq

This reduces ovefitting.
Overfitting might 
occur if the weights follow too closely
several similar minibatches.
This dropout procedure adds a random
component to each minibatch
making groups of similar minibatches
less likely.

The random $\rvr^\lam_i$ nodes
that induce dropout are 
only used in the training bnet Fig.\ref{fig-nn-ext},
not in the classification bnet Fig.\ref{fig-nn}.
We prefer to remove the 
$\rvr^\lam_i$ stochasticity from classification 
and for Fig.\ref{fig-nn} to act as an average
over sampling space of Fig.\ref{fig-nn-ext}.
Therefore,
if weights $w^\lam_{i|j}$ are obtained
for a dropout layer $\lam$ in Fig.\ref{fig-nn-ext},
then that layer is used in Fig.\ref{fig-nn} with 
no $\rvr^\lam_i$ nodes but
with weights $\av{r^\lam_i}w^\lam_{i|j}=
pw^\lam_{i|j}$.


Note that dropout adds non-deterministic
nodes to a NN, 
which in their vanilla form only have
deterministic nodes.


\item {\bf Convolutional Layer}

\begin{itemize}
\item 1-dim

Filter function $\calf:\{0, 1, \ldots, 
numf-1\}\rarrow \RR$.

$\sigma$=stride length

For $i\in \{0,1,\dots,numh(\lam)-1\}$,
let

\beq
H^\lam_i[s]=
\sum_{ j=0}^{numf-1}
h^{\lam-1}_{j+i\sigma}[s] \calf(j)
\;.
\label{eq-conv1}
\eeq
For the indices not to
go out of bounds in Eq.(\ref{eq-conv1}),
we must have

\beq
numh(\lam-1)-1=numf-1 +
(numh(\lam)-1)\sigma
\;
\eeq
so
\beq
numh(\lam)=\frac{1}{\sigma}[numh(\lam-1)-
numf] + 1
\;.
\eeq
\item 2-dim

$h_i^\lam[s]$ becomes
$h_{(i,j)}^\lam[s]$.
Do 1-dim convolution
along both $i$ and $j$ axes.

\end{itemize}
\item{\bf Pooling Layers 
(MaxPool, AvgPool)}

Here each node $i$ 
of layer $\lam$ is impinged by
arrows from  a subset $Pool(i)$
of the set of all
nodes of the previous layer $\lam-1$.
Partition set
$\{0,,1,\dots,numh(\lam-1)-1\}
$ into $numh(\lam)$ mutually
disjoint, nonempty sets
called $Pool(i)$, where
$i\in \{0, 1, \ldots,numh(\lam)-1\}$.

\begin{itemize}
\item AvgPool
\beq
H^\lam_i[s]=\frac{1}
{size(Pool(i))}
\sum_{j\in Pool(i)}h^{\lam-1}_j[s]
\eeq
\item MaxPool
\beq
H^\lam_i[s]=
\max_{j\in Pool(i)}h^{\lam-1}_j[s]
\eeq

\end{itemize}


\end{itemize}

\section*{Autoencoder NN}


If the sequence 

\beq
numx, numh(0), numh(1), \ldots,
numh(\Lambda-1),numy
\eeq
first decreases monotonically 
up to layer $\lam_{min}$, then
increases monotonically until
$numy=numx$, then
the NN is called an {\bf
autoencoder NN}.
Autoencoders
are  useful for unsupervised learning
and
feature reduction. In this case,
$Y$ estimates $x$.
The layers before
layer $\lam_{min}$
are called the {\bf encoder},
and those after 
$\lam_{min}$ are called the 
{\bf decoder}.
Layer $\lam_{min}$
is called the {\bf code}.



