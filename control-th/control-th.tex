\chapter{Control Theory (linear, deterministic)}
\label{ch-control-th}

This chapter is
based on Ref.\cite{wikibooks-control-systems}
and \cite{wiki-signal-flow}.

We will assume that
the reader has read Section \ref{sec-laplace-trans}
on Laplace Transforms
and Section \ref{sec-z-transform}
on Z-transforms.

Control Theory (CT)
studies the optimal
control of systems
with feedback. 
The systems studied can
be 
\begin{itemize}
\item linear or non-linear,
\item deterministic or stochastic,
\item continuous time (analog)
or
discrete time (digital).
\end{itemize}
This chapter will deal
with linear deterministic
systems of either 
the analog or digital kind.

As explained in Chapter
\ref{ch-dyn-bnet},
dynamical bnets and feedback
are two ways
of viewing the same 
physical phenomenon.
Also, there are numerous examples
of dynamical bnets in this book:
Kalman filters, Hidden Markov Models,
Reinforcement Learning,
Recursive Neural Nets, to name a few.
Hence, a chapter 
on CT is very 
pertinent to this book.

Two acronyms
commonly 
used in CT books
are:
{\bf SISO} (single input single output)
and
{\bf MIMO} (multiple input multiple output). We will consider both
SISO and MIMO systems in this chapter.

Another distinction
commonly
made in CT books
is between {\bf time-variant}
and {\bf time-invariant } systems.
We will explain what those terms
mean later on in this chapter.
This chapter will consider
both types of systems.



\section{Basic feedback model}

CT uses feedback to
control
a {\bf system or process}.
Fig.\ref{fig-basic-feedback}
shows a very basic
feedback model,
represented
with 3 equivalent
diagrams.


\begin{figure}[h!]
$$
\begin{array}{ccc}
\xymatrix{
\ar[d]^{r(t)}
\\
*+[F]{\stackrel{\rm Controller}{F_1}}
\ar@{-}[d]^{u(t)}
&
\ar@{-}[d]\ar@{-}[l]
&
\\
*+[F]{\stackrel{\rm Prossess} {F_2}}
\ar@{-}[r]^{\quad y(t)}
&\ar@{-}[u]
}
&
\xymatrix{
\rvr^{[k]}\ar[d]
&
\rvr^{[k+1]}\ar[d]
\\
\rvu^{[k]}\ar[d]
&\rvu^{[k+1]}\ar[d]
\\
\rvy^{[k]}\ar[r]\ar[ur]
&
\rvy^{[k+1]}
}
&\quad\quad
\xymatrix{
\rvr(t)\ar[d]
\\
\rvu(t)\ar[d]
\\
\rvy(t)
\ar@(ul,ur)@{~>}[]
\ar@{~>}@/_2pc/[u]
}
\\
\\
(a)&(b)&\quad\quad(c)
\end{array}
$$
\caption{Basic feedback model
represented as: $(a)$ a
wired-boxes diagram, $(b)$ one
slice of a dynamical bnet 
(see Chapter \ref{ch-dyn-bnet}),
and $(c)$ a ``rolled" dynamical bnet
 with feedback cycles. 
}
\label{fig-basic-feedback}
\end{figure}

The diagrams of 
Fig.\ref{fig-basic-feedback}
represent
graphically
the following
system
of equations.
Here $t\in[0,\infty]$
is time and
$u,y:[0,\infty]$.

\beq
\left\{
\begin{array}{l}
u(t) = F_1(y(t), r(t), t)
\\
\partial_t y(t)= F_2(y(t), u(t), t)
\end{array}
\right.
\eeq
If we approximate the 
time derivative of $y(t)$ by

\beq
\partial_t y(t) \approx 
\frac{y(t+
\Delta t)-y(t)}{\Delta t}
\eeq
and we set

$t_k=t$, $t_{k+1}=t + \Delta t$, $0< \Delta t<< 0$, 

$f(t_k)=f^{[k]}$ for $f=y,u$,
\\then we get 
Fig.\ref{fig-basic-feedback}

The TPMs, 
printed in blue,
of the bnet
in Fig.\ref{fig-basic-feedback},
are as follows:

\beq \color{blue}
P(r^{[k]}) = \text{given}
\eeq

\beq\color{blue}
P(u^{[k]}|y^{[k]}, r^{[k]}) = 
\delta(\quad
u^{[k]}- F_1(y^{[k]}, r^{[k]}, t_k)
\quad)
\eeq


\beq\color{blue}
P(y^{[k+1]}|y^{[k]}, u^{[k]})
=
\delta(\quad
y^{[k+1]}
- y^{[k]} -\Delta t F_2(y^{[k]}, u^{[k]}, t_k)
\quad)
\eeq




\section{Classical model (analog)}


\begin{figure}[h!]
$$
\begin{array}{ccc}
\xymatrix{
\ar[d]_{r(t)}^{+}
\\
*+[F]{\sum}
\ar@{-}[d]_{e(t)}
&\ar@{-}[l]^{-}_{f(t)}
\\
*+[F]{\stackrel{\rm Controller}{C}}
\ar@{-}[d]_{u(t)}
&
*+[F]{\stackrel{\rm Filter} {F}}
\ar@{-}[u]\ar@{-}[d]
&
\\
*+[F]{\stackrel{\rm Prossess} {\Pi}}
\ar@{-}[r]^{y(t)}
\ar[d]^{y(t)}
&\ar@{-}[u]
\\
&
}
&
\xymatrix@C=3pc{
\rvf^{[k]}\ar@/_1.5pc/[dd]
&
\rvf^{[k+1]}\ar@/_1.5pc/[dd]
\\
\rvr^{[k]}\ar[d]
&
\rvr^{[k+1]}\ar[d]
\\
\rve^{[k]}\ar[d]\ar[rd]
&
\rve^{[k+1]}\ar[d]
\\
\rvu^{[k]}\ar[d]\ar[rd]
&\rvu^{[k+1]}\ar[d]
\\
\rvy^{[k]}\ar[ruuuu]
&
\rvy^{[k+1]}
}
&\quad\quad
\xymatrix{
\rvf(t)\ar@/_1.5pc/[dd]
\\
\rvr(t)\ar[d]
\\
\rve(t)\ar[d]\ar@{~>}@/_1pc/[d]
\\
\rvu(t)\ar[d]\ar@{~>}@/_1pc/[d]
\\
\rvy(t)\ar@{~>}@/_2pc/[uuuu]
}
\\
\\
(a)& (b) &\quad\quad (c)
\end{array}
$$
\caption{
Classical Model
represented
with
the same 3
types of diagrams
as Fig.\ref{fig-basic-feedback}.
Bnets $(b)$ and $(c)$
don't
show all the arrows.
In reality,
for any arrow $a^{[k]}\rarrow b^{[k+1]}$ that
points from the 
time slice $t_k$
to the time slice $t_{k+1}$,
there should 
be arrows 
$a^{[j]}\rarrow b^{[k+1]}$
for $j\in\{0,1,2, \cdots, k
\}$.
That's because
the classical model is defined
in terms of convolutions,
and a convolution
at time $t$ requires
memory for all times
between 0 and $t$.
If we take
the Laplace transform
of figures $(a)$ and $(c)$ (or 
the Z-transform of figure $(b)$),
then we get a
Signal Flow (SF) graph. SF graphs will
be discussed later on in this chapter.
In SF graphs,
the only
inter slice arrows are between 
adjacent slices; i.e., SF graphs have
no arrows that 
span more than 1 slice.}
\label{fig-classic-model}
\end{figure}
A {\bf classical
model} is a bunch of
 convolution boxes
 connected by wires.
 See Fig.\ref{fig-classic-model}
 for 3
 graphical
 representations of
 the classical model.
 
The diagrams of 
Fig.\ref{fig-classic-model}
represent
graphically
the following
system
of equations.
Here $t\in[0,\infty]$
is time and
$f,e,u,y:[0,\infty]\rarrow \CC$.

\beq
\left\{
\begin{array}{l}
f(t) = (F\circledast  y)(t)
\\
e(t)= r(t) - f(t)
\\
u(t) = (C\circledast  e)(t)
\\
y(t) = (\Pi\circledast  u)(t)
\end{array}
\right.
\label{eq-classic-analog}
\eeq
where $()f\circledast g)(t)$
denotes a covolution,
as defined in Section \ref{sec-laplace-trans}
on Laplace transforms.

The TPMs, 
printed in blue,
of the bnet
in Fig.\ref{fig-classic-model},
are as follows:

\beq\color{blue}
P(r(t)) = \text{given}
\eeq

\beq\color{blue}
P(f(t)|y(\cdot))
=\delta(\quad
f(t)-\Pi[y](t)
\quad)
\eeq

\beq\color{blue}
P(e(t)|
r(t), f(t))=
\delta(\quad
e(t)- [r(t)-f(t)]
\quad)
\eeq

\beq\color{blue}
P(u(t)|e(\cdot))=
\delta(\quad
u(t)-C[e](t)
\quad)
\eeq

\beq\color{blue}
P(y(t)|u(\cdot))
=\delta(\quad
y(t)-\Pi[u](t)
\quad)
\eeq


If we take
the Laplace transform
of Eqs.(\ref{eq-classic-analog},
we get

\beq
\left\{
\begin{array}{l}
\TIL{f}(s) =\TIL{F}(s)\TIL{y}(s)
\\
\TIL{e}(s)= \TIL{r}(s) - \TIL{f}(s)
\\
\TIL{u}(s) = \TIL{C}(s)\TIL{e}(s)
\\
\TIL{y}(s) = \TIL{\Pi}(s)\TIL{u}(s)
\label{eq-classic-analog-lap}
\end{array}
\right.
\eeq
Thus

\beq
\TIL{y}=\TIL{\Pi}\TIL{C}[\TIL{r}-
\TIL{F}\TIL{y}]
\eeq

\beq
[1+\TIL{\Pi}\TIL{C}\TIL{F}]\TIL{y}
=
\TIL{\Pi}\TIL{C}\TIL{r}
\eeq
From this,
we can solve for the 
{\bf transfer or gain function}
$\TIL{H}_{y|r}(s)$

\beqa
\TIL{H}_{y|r}(s)
&=&
\frac{\TIL{y}(s)}{\TIL{r}(s)}
\quad \text{(output/input)}
\\
&=&
\frac{
\overbrace{\TIL{\Pi}\TIL{C}}^{
H_0=
\TIL{H} \text{ with } F=0}
}
{1 + 
\underbrace{\TIL{\Pi}\TIL{C}\TIL{F}
}_{H_0\TIL{F}}
}
\eeqa

A special
and very common
type of controller
function $u(t)$
called the 
{\bf Proportional-Integral-Derivative (PID) Controller}
is defined as

\beq
u(t)=
K_\Pi e(t)
+K_I
\int_0^t
d\tau \;
e(\tau)
+
K_D\partial_t e(t)
\eeq
The Laplace transform
of the PID controller
is 


\beqa
\TIL{u}(s)
&=&
K_\Pi\TIL{e}(s)
+ K_I \frac{\TIL{e}}{s}
+
K_D (s\TIL{e}(s)-
e(0^+))
\\
&=&
\underbrace{
\left(
K_\Pi + \frac{K_I}{s}
+ K_D s
\right)
}_{\TIL{C}(s)}
\TIL{e}(s)
\quad\text{(assume $e(0^+)=0$)}
\eeqa


\begin{claim}
A PID controller
has unit gain ($\TIL{H}_{y|r}(s)=1$)
if its
defining parameters satisfy:

\beq
K_\Pi = 2K,\;
K_D=KT,\; K=\frac{K}{T}
\eeq
\end{claim}
\proof

\beq
\TIL{\Pi}(s)= \frac{1}{K(1+sT)}
\eeq
and

\beq
\TIL{F}(s) = \frac{1}{1+sT}
\eeq
so


\beqa
\TIL{C}
&=&\frac{K}{sT}\left(2sT +1 + (sT)^2\right)
\\
&=&
\frac{K}{sT}\left(1 +sT\right)^2
\eeqa
and

\beqa
1+\TIL{\Pi}\TIL{C}\TIL{F}&=&
1+\frac{1}{sT}
\\
&=&\frac{1}{sT}(1+sT)
\\
&=&
\TIL{\Pi}\TIL{C}
\eeqa
Hence,

\beq
\TIL{H}_{y|r}(s)=1
\quad \text{(output/input)}
\eeq
\qed






\section{Modern model (analog) }


 \begin{figure}[h!]
 $$
\begin{array}{ccc}
\xymatrix@R=2pc{
\ar[d]^{u(t)}
\\
\ar@/_2pc/@{-}[dd]_{u(t)}
\ar@{-}[d]^<<<{u(t)}
\\
*+[F]{F_{AB}}
\ar@{-}[d]^{x(t)}
\ar@(ul,ur)@{-}[]
\\
*+[F]{F_{CD}}
\ar[d]^{y(t)}
\\
&
}
&
\xymatrix@C=4pc{
\rvu^{[k]}\ar@/_2pc/[dd]_D
\ar[rd]^<<<{B}
& \rvu^{[k+1]}\ar@/_2pc/[dd]
\\
\rvx^{[k]}\ar[d]_C\ar[r]^<<<{A}
&\rvx^{[k+1]}\ar[d]
\\
\rvy^{[k]}
&\rvy^{[k+1]}
}
&\quad\quad
\xymatrix{
\rvu(t)\ar@/_2pc/[dd]
\ar@{~>}[d]
\\
\rvx(t)\ar[d]
\ar@(ul,ur)@{~>}[]
\\
\rvy(t)
}
\\
\\
(a)&(b)&\quad\quad(c)
\end{array}
$$
\caption{Modern Model
represented
with
the same 3
types of diagrams
as Fig.\ref{fig-basic-feedback}.}
\label{fig-modern-model}
\end{figure}

A {\bf modern
model (a.k.a. state space model)} is a bunch of
 time dependent
 boxes, some with 
 first order
 time derivatives, 
 connected by wires.
 See Fig.\ref{fig-modern-model}
 for 3
 graphical
 representations of
 the classical model.

The diagrams of 
Fig.\ref{fig-modern-model}
represent
graphically
the following
system
of equations.
Here $t\in[0,\infty]$
is time and

$u:[0, \infty]\rarrow \CC^{nu}$

$x:[0, \infty]\rarrow \CC^{nx}$

$y:[0, \infty]\rarrow \CC^{ny}$
\\ for some integers $nu, nx, ny$.
These
equations are
called the {\bf state space equations} and $x(t)$ is called the {\bf state} 
of the system.
\beq
\left\{
\begin{array}{l}
\partial_t x(t)= F_{AB}(x(t), u(t), t)
\\
y(t) = F_{CD}(x(t), u(t), t)
\end{array}
\right.
\label{eq-nonlinear-modern}
\eeq

The TPMs, 
printed in blue,
of the bnet
in Fig.\ref{fig-modern-model},
are as follows:


\beq\color{blue}
P(u^{[k]})=\text{given}
\eeq

\beq\color{blue}
P(x^{[k+1]}\cond x^{[k]}, u^{[k]})=
\delta(\quad
x^{[k+1]}- \Delta t F_{AB}(x^{[k]}, u^{[k]}, t_k)
\quad)
\eeq

\beq\color{blue}
P(y^{[k+1]}\cond x^{[k]}, u^{[k]})
=
\delta(\quad
y^{[k+1]}-F_{CD}(x^{[k+1]},
u^{[k+1]}, t_{k+1})
\quad)
\eeq




Henceforth, assume $F_{AB}$
and $F_{CD}$ are as follows. This is called
the {\bf linear} case.

\beq
\left\{
\begin{array}{l}
F_{AB}(x(t), u(t)) = A(t)x(t) + B(t)u(t)
\\
F_{CD}(x(t), u(t)) = C(t)x(t) + D(t)u(t)
\end{array}
\right.
\eeq
for some
matrices
$A(t), B(t), C(t), D(t)$.
In the linear case,
the modern model is
described by the following
equations:

\beq
 \left\{
 \begin{array}{l}
 \partial_t x(t) = A x(t) + B u(t)
 \\
 y(t) = C x(t) + D u(t)
 \end{array}
 \right.
 \label{eq-modern-linear-analog}
 \eeq
 
 

If the 
matrices  $A(t), B(t), C(t), D(t)$
depend on (resp., are independent of)
time $t$,
we say the system
is {\bf time-variant}
(resp., {\bf time-invariant}).
Next, we solve the 
differential
equation for
$x(t)$,
for both the time-invariant and variant cases.

\begin{itemize} 
\item time-invariant case


Taking the Laplace transform
of the first equation of
Eqs.(\ref{eq-modern-linear-analog}),
we get

\beq
s\TIL{x}(s)-x(0)=A\TIL{x}(s) + B\TIL{u}(s)
\eeq
Hence,
the Laplace transform
of Eqs.(\ref{eq-modern-linear-analog})
is
 
 \beq
 \left\{
 \begin{array}{l}
 \TIL{x}(s) = (sI-A)^{-1} x(0)
 + (sI-A)^{-1} B\TIL{u}(s)
 \\
 \TIL{y}(s)=C\TIL{x}(s)+ D\TIL{u}(s)
 \end{array}
 \right.
 \eeq

If we define the transfer function
 $\TIL{H}_{y|u}(s)$ by
  
 \beq
 \underbrace{\TIL{y}(s)}_{output}=
 \TIL{H}_{y|u}(s)
 \underbrace{\TIL{u}(s)}_{input}
 \eeq
 then,
 assuming $x(0)=0$,
  
 \beq
 \TIL{H}_{y|u}(s) =
 C(sI-A)^{-1}B
 +D
 \eeq
  
 
 \begin{claim}
 \beq
 x(t)=e^{A(t-t_0)}x(t_0)
 +
 e^{A(t-t_0)}
 \int_{t_0}^{t} d\tau\;
 e^{-A(\tau-t_0)}
 Bu(\tau)
 \label{eq-state-space-sol}
 \eeq
 where
 
 \beq
 e^{At}= \sum_{k=0}^\infty
 \frac{t^k}{k!}A^k
 \eeq
 Hence, setting $t\geq t_0=0$,
 \beq
  e^{At}=
  \call^{-1}[(sI-A)^{-1}]
  \eeq
  
  \beq
  e^{At}
    \int_0^t d\tau\;
    e^{-\tau}
    Bu(\tau)
    =
   \call^{-1}[
  (sI-A)^{-1}B\TIL{u}(s)]
   \eeq
   
 \end{claim}
 \proof
 To check Eq.(\ref{eq-state-space-sol}),
 just take the time derivative
 of both sides and 
 use $\partial_t\int^t d\tau\; f(\tau)= 
 f(t)$
 \qed
 
 \item time-variant case
 
 \begin{claim}
 \label{cl-modern-time-variant}
 
 \beq
 x(t)=
 \cale(t,t{_0})x(t_0)
 +
 \int_{t_0}^t
 d\tau\;
 \cale(t,\tau)
 B(\tau)u(\tau)
 \label{eq-time-var-sol}
 \eeq
 where
 the {\bf transition matrix
 (a.k.a. evolution matrix)} $\cale(t, t_0)$
 satisfies
 
 \beq
 \partial_t 
 \cale(t, t_0) = A(t)
 \eeq
and

\beq
\cale(t,t)=1
\eeq

 \end{claim}
 \proof 
 To prove Eq.(\ref{eq-time-var-sol}),
 just differentiate both sides of it
 with respect to $t$,
 and use $\partial_t\int^td\tau\; f(\tau) = f(t)$
 \qed
 
 In the time-invariant case,
 
 \beq 
 \cale(t,t_0)=e^{A(t-t_0)}
 \eeq
 
 
 
 \end{itemize}
 
 \section{Classical model (digital)} 
 In this section,
 we will define
 the classical model
 with discrete rather
 than continuous time.
 
 If we discretize time (i.e., 
 sample time
 at discrete times
 separated by a time
 interval $T$),
 then 
 the equivalent 
 of Eqs.(\ref{eq-classic-analog})
 is
 
 \beq
 \left\{
 \begin{array}{l}
 f^{[n]} = (F\circledast  y)^{[n]}
 \\
 e^{[n]}= r^{[n]} - f^{[n]}
 \\
 u^{[n]} = (C\circledast  e)^{[n]}
 \\
 y^{[n]} = (\Pi\circledast  u)^{[n]}
 \end{array}
 \right.
 \label{eq-classic-digital}
 \eeq
 where $(x\circledast y)^{[n]}$
denotes a 
discrete convolution, as 
defined in Section
 \ref{sec-z-transform}
 on Z-transforms.
 And
 if we take
 the Z-transform
 of Eqs.(\ref{eq-classic-digital}),
 we get
 
 \beq
 \left\{
 \begin{array}{l}
 \TIL{f}(z) =\TIL{F}(z)\TIL{y}(z)
 \\
 \TIL{e}(z)= \TIL{r}(z) - \TIL{f}(z)
 \\
 \TIL{u}(z) = \TIL{C}(z)\TIL{e}(z)
 \\
 \TIL{y}(z) = \TIL{\Pi}(z)\TIL{u}(z)
 \end{array}
 \right.
 \eeq
 
 
 
 \section{Modern model (digital)}
 
 In this section,
  we will define
  the modern model
  with discrete rather
  than continuous time.
We will do this
2 different ways. 
First, we will 
approximate the time derivates
in a differential equation
with a discrete approximation.
This approach is interesting
and instructive but
not perfect, because
it's an approximation.
Second, we will define
difference equations
and solve them exactly.
The second approach 
is better for most purposes,
because it gives exact results,
not approximations.
 
 
 \subsection{Discretizing analog model}
 
 
  
 Ibf we approximate
 $\partial_t x(t)$ by
 
 \beq
 \partial_t x(t)=
 \lim_{\Delta t\rarrow 0}
 \frac{x(t+\Delta t)-x(t)}{\Delta t}
 \eeq
 then Eqs.(\ref{eq-modern-linear-analog})
 reduce to
 
 \beq
 \left\{
 \begin{array}{l}
 x(t+\Delta t) = (1 +\Delta t A) x(t) +\Delta t B u(t)
 \\
 y(t) = C x(t) + D u(t)
 \end{array}
 \right.
 \eeq
 Using the new notation
 
 \beq
 \HAT{A} = 
 e^{A\Delta t}\approx 1+\Delta t A,
 \eeq
 
 \beq
 \HAT{B}\approx \Delta t B
 \eeq
 
 \beq
 \Delta t = T,
 \quad t = nT
 \eeq
 
 \beq
 x(t) = x(nT) = x^{[n]},
 \quad u(t) = u(nT)= u^{[n]}
 \eeq 
 we get
 
 \beq
 \left\{
 \begin{array}{l}
 x^{[n+1]} = 
 \HAT{A} x^{[n]} + \HAT{B} u^{[n]}
 \\
 y^{[n]} = C x^{[n]} + D u^{[n]}
 \end{array}
 \right.
 \label{eq-modern-difference-eqs}
 \eeq
 
 Setting
 $t=t_0+T$ in 
  Eq.(\ref{eq-state-space-sol}),
  we get
 
 \begin{align}
  x(t_0+T)&=
  e^{AT}x(t_0)
  +
  e^{AT}
  \int_{t_0}^{t_0+T} d\tau\;
  e^{-A(\tau-t_0)}
  Bu(\tau)
  \\
  &=
   e^{AT}x(t_0)
   +
   e^{AT}
   \int_{0}^{T} d\tau\;
   e^{-A\tau}
   Bu(\tau+t_0)
   \quad\text{(substitute $\tau\rarrow 
   \tau+t_0$)}
  \end{align}
  Now setting
  
  \beq
  t_0=nT
  \eeq
  and
  
  \beq
   u(\tau + t)\approx u(t)
  \quad\text{for $\tau\in [0, T]$}
  \eeq
  we get
  
  \beq
  x^{[n+1]}=
  \underbrace{e^{AT}}_{\HAT{A}}
  x^{[n]}
    +
    \underbrace{
    e^{AT}
    \left[
    \int_{0}^{T} d\tau\;
    e^{-A\tau}
    \right]
    B}_{\HAT{B}}
    \;u^{[n]}
 \eeq
 where
 
 \beq
 \HAT{A}=e^{AT}
 \eeq
 and
 
 \beqa
 \HAT{B}
 &=&
 e^{AT}
    \left[
    \int_{0}^{T} d\tau\;
    e^{-A\tau}
    \right]
    B
 \\
 &=&
 e^{AT}(-A)^{-1}(e^{-AT}-I)B
 \\
 &=&
 A^{-1}
 (\HAT{A}-1)B
 \eeqa
 When $T<<1$,
 $\HAT{A}-1\approx AT$
 so $A^{-1}
 (\HAT{A}-1)B\approx BT$.
 
 \subsection{Solving Difference Equation}
 
 \begin{itemize}
 \item time-invariant case
 
 Consider the following difference equation
 taken from
 Eqs.(\ref{eq-modern-difference-eqs})
 
 \beqa
 x^{[n+1]} &=& \HAT{A}x^{[n]}+\HAT{B}u^{[n]}
 \eeqa
 To solve
 this difference equation,
 we notice that
 
 
 \beqa
  x^{[1]} &=& \HAT{A}x^{[0]}+
  \HAT{B}u^{[0]}
  \eeqa
  
\beqa
x^{[2]}&=& \HAT{A}x^{[1]}
+\HAT{B}u^{[1]}
\\
&=&
\HAT{A}^2 x^{[0]}
+\HAT{A}\HAT{B}u^{[0]}
++\HAT{B}u^{[1]}
\eeqa
 
 \beqa
 x^{[2]}&=&\HAT{A}x^{[2]}+ \HAT{B}u^{[2]}
 \\
 &=&
 \HAT{A}^3 x^{[0]}
 + \HAT{A}^2 \HAT{B}u^{[0]}
 + \HAT{A}\HAT{B} u^{[1]}
 + \HAT{B} u^{[2]}
 \eeqa
 The general pattern is clear.
 In general,  
 
 \beq
 x^{[n]}=
 \HAT{A}^n x^{[n_0]}
 +\sum_{k=0}^{n-1}
 \HAT{A}^{n-k-1}
 \HAT{B} u^{[k]}
\eeq

It is also possible
to solve Eqs.(\ref{eq-modern-difference-eqs})
using Z-transforms (See Section
\ref{sec-z-transform}).
The Z-transform
of the first of
those two equations is

\beq
z(\TIL{x}(z) - x^{[0]})
= \HAT{A}\TIL{x}(z) + \HAT{B}\TIL{u}(z)
\eeq
Therefore, the
Z-transform of 
Eqs.(\ref{eq-modern-difference-eqs}) is\footnote{Notice the
$z$ factor multipying
$x^{[0]}$. There 
is no counterpart $s$
factor
multiplying $x(0)$
in the analog case.
That's because $z=e^{sT}$.}

\beq
\left\{
\begin{array}{l}
\TIL{x}(z) = (zI -\HAT{A})^{-1}z x^{[0]}
+
(zI -\HAT{A})^{-1}\HAT{B}\TIL{u}(z)
\\
\TIL{y}(z)= C\TIL{x}(z) + D\TIL{u}(z)
\end{array}
\right.
\eeq
From this
we can get the transfer matrix $\TIL{H}_{y|u}(z)$. Assuming $x^{[0]}=0$, 

\beq
\TIL{y}(z)=
\underbrace{
\left(
\HAT{C}(zI-\HAT{A})^{-1}\HAT{B}
+\HAT{D}
\right)}_{\TIL{H}_{y|u}(z)}
\TIL{u}(z)
\eeq


 
 \item time-variant case
 
 We can also give a
 discrete version of Claim
 \ref{cl-modern-time-variant}
 
 \begin{claim}
  \beq
  x^{[n]}=
  \cale^{[n,n_0]}x^{[n_0]}
  +\sum_{k=0}^{n-1}
  \cale^{[n, k+1]}
  B^{[k]} u^{[k]}
 \eeq
 where
 
 \beq
 \cale^{[n+1, n_0]}=\HAT{A}^{[n]}
 \cale^{[n,n_0]}
 \eeq
 and
 
 \beq
 \cale^{[n_0, n_0]} = I
 \eeq
 Hence
 
 \beqa
 \cale^{[n,n_0]}&=&
 \HAT{A}^{[n-1]}\HAT{A}^{[n-2]}
 \HAT{A}^{[n-3]}\cdots \HAT{A}^{[n_0]}
 \\
 &=&
 \prod_{k\in \ZZ^{[1, n-n_0]}}\HAT{A}^{[n-k]}
 \eeqa
 \end{claim}
 \proof Left to reader.
 \qed
 
 \end{itemize}
 
 
  \section{Higher than first order differential (or difference) equations}
  
If in the time-invariant
analog modern model, 
we express $x(t)$
in terms of $y(t)$
and $u(t)$

\beq
x(t)= C^{-1}[y(t) -Du(t)]
\eeq
and then we plug this into the
equation for $\partial_t x(t)$,
we get

\beq
\partial_t
\left(
\underbrace{
 C^{-1}[y(t) -Du(t)]}_{x(t)}
\right)
= A \underbrace{
 C^{-1}[y(t) -Du(t)]}_{x(t)} + Bu(t)
\eeq
Hence, it appears
that this model can
only accommodate a
first order time derivative
of the output $y(t)$.
Next we give
a transformation 
whereby a model
with
$y(t)$ derivatives that are higher
than 1st order,
can be re-expressed
in a form
that only 
has 1st order
time derivatives.
We will
also give
an analogous result for 
the time-invariant digital modern
model. We will show that those  
digital models 
can accommodate higher
than 1st order 
time differences.
  
  \subsection{Differential Equations}
 Let
 \beq
 \Omega = \partial_t^3
 + a_2\partial_t^2 + a_1\partial_t + a_0
 \eeq
 where $a_0, a_1, a_2\in \CC$ are 
 independent of time $t$,
 and consider the {\bf linear,
 constant
 coefficients (LCC) ordinary differential
 equation (ODE)}:
 
 \beq
 \Omega y(t)= u(t)
 \eeq
 Assume $f(t)$ satisfies
 
 \beq
 \Omega f(t)=0
 \eeq
Let

\beq
\underbrace{\partial_t^3 y(t)}_{
\partial_t x_2+\partial_t^3 f}
+ a_2\underbrace{\partial_t^2 y(t)}_{x_2
+\partial_t^2 f}
+ a_1 \underbrace{\partial_t y(t)}
_{x_1+\partial_t f}
+ a_0 \underbrace{y(t)}_{x_0+f}
 =u(t)
\eeq

\beq
x(t)=
\left[
\begin{array}{c}
x_0(t)
\\
x_1(t)
\\
x_2(t)
\end{array}
\right]
\eeq
Then

\beq
\partial_t x(t)=
\underbrace{\left[
\begin{array}{ccc}
0&1&0
\\
0&0&1
\\
-a_0&-a_1&-a_2
\end{array}
\right]
}_{A}
x(t)
+ \underbrace{\left[
\begin{array}{c}
0
\\
0
\\
1
\end{array}
\right]}_{B}
u(t)
\eeq

\beq
y(t) = x_0(t) + f(t)=
\underbrace{\left[
\begin{array}{ccc}
1&0&0
\end{array}
\right]
}_{C}
x(t)
+ f(t)
\eeq
If $\Omega u=0$, we can define $f=D u$.
Otherwise, define $f=0$.

What's going on here, from 
a dynamical bnet perspective,
is that we are defining the slices 
to have enough variables 
so that there only needs to be
memory from one slice to the previous 
slice instead of to the
previous 3 slices.

\subsection{Difference Equations}
Let 

\beq
\underbrace{y^{[n+3]}}_{x_3^{[n+1]}}
+ a_2 \underbrace{y^{[n+2]}}_{x_2^{[n]}}
+ a_1 \underbrace{y^{[n+1]}}_{x_1^{[n]}}
+ a_0 \underbrace{y^{[n]}}_{x_0^{[n]}}
=
u^{[n]}
\eeq
Then

\beq
x^{[n]}=
\left[
\begin{array}{c}
x_0^{[n]}
\\
x_1^{[n]}
\\
x_2^{[n]}
\end{array}
\right]
\eeq

\beq
x^{[n+1]}=
\underbrace{\left[
\begin{array}{ccc}
0&1&0
\\
0&0&1
\\
-a_0&-a_1&-a_2
\end{array}
\right]
}_{A}
x^{[n]}
+ \underbrace{\left[
\begin{array}{c}
0
\\
0
\\
1
\end{array}
\right]}_{B}
u^{[n]}
\eeq

\beq
y^{[n]} = x_0^{[n]}=
\underbrace{\left[
\begin{array}{ccc}
1&0&0
\end{array}
\right]
}_{C}
x^{[n]}
\eeq



\section{Causality, Stability, Controlability, Observability}

Suppose $x, u, y:\calt\rarrow \calx$.
 where $\calt=[0, \infty)$.

\subsection{Causality, Stability}


 

$y(\cdot)$ is {\bf stable}
if $x(\cdot)$ 
and $u(\cdot)$ bounded implies
$y(\cdot)$ is bounded.


\subsection{
Controllability,
Observability}
$a\in\calx$ is {\bf controllable
(resp., reachable) at time $t_0\in\calt$
}
if there exist a time
$t_1\in\calt$ and an input $u(\cdot)$ such that $x(t_0)=a$
and $x(t_1)=0$ (resp., $x(t_0)=0$ and $x(t_1)=a$)
$x(\cdot)$ is {\bf controllable 
(resp., reachable)
at time $t_0\in\calt$}
if it is controllable 
(resp., reachable)
at time $t_0\in\calt$ for all $a\in\calx$.


$a\in\calx$ is {\bf observable at time $t_0\in\calt$}
if there exists a $t_1\in \calt$
such that 
$x(t_0)=a$
and $a$
can be determined from
the values of 
$y(t)$ for $t\in [t_0, t_1]$.
$x(\cdot)$ is {\bf observable
at time $t_0\in\calt$ } 
if it is 
observable 
at time $t_0\in\calt$ for all $a\in \calx$.

\section{Signal Flow Graph}
This section is based on Ref.\cite{wiki-signal-flow}

Fig.\ref{fig-basic-flow-graph}
is 
the Laplace transform
of Figs.\ref{fig-classic-model} 
$(a)$ and $(c)$.
It shows 2
graphical representations 
(as a wired-boxes diagram and
as a bnet with feedback cycles) of the
Laplace transform of the
classical
model.
Note that in going 
from the wired-boxes diagram
to the bnet,
the wire labels become the node
labels,
and the box labels 
become
the arrow labels.
Note also
that node and arrow
labels are all
Laplace Transforms.



\begin{figure}[h!]
$$
\begin{array}{cc}
\xymatrix{
\ar[d]_{\TIL{r}}^{+}
\\
*+[F]{\sum}
\ar@{-}[d]_{\TIL{e}}
&\ar@{-}[l]^{-}_{\TIL{f}}
\\
*+[F]{\stackrel{\rm Controller}{\TIL{C}}}
\ar@{-}[d]_{\TIL{u}}
&
*+[F]{\stackrel{\rm Filter} {\TIL{F}}}
\ar@{-}[u]\ar@{-}[d]
&
\\
*+[F]{\stackrel{\rm Prossess} {\TIL{\Pi}}}
\ar@{-}[r]^{\TIL{y}}
\ar[d]^{\TIL{y}}
&\ar@{-}[u]
\\
&
}
&
\xymatrix{
\TIL{\rvf}\ar@/_1.5pc/[dd]_{-1}
\\
\TIL{\rvr}\ar[d]^{+1}
\\
\TIL{\rve}\ar[d]_{\TIL{C}}
\\
\TIL{\rvu}\ar[d]_{\TIL{\Pi}}
\\
\TIL{\rvy}\ar@/_2pc/[uuuu]_{\TIL{F}}
}
\end{array}
$$
\caption{This figure
is
the Laplace transform 
of Figs.\ref{fig-classic-model}
$(a)$ and
$(c)$.
}
\label{fig-basic-flow-graph}
\end{figure}

Fig.\ref{fig-basic-flow-graph}
$(c)$ is what is called a Signal
Flow (SF) graph. According
to Wikipedia Ref.\cite{wiki-signal-flow},
SF graphs were invented by Shannon
in 1942, and later 
studied and promoted by Mason.



The SF graph of Fig.\ref{fig-basic-flow-graph}
implies the following 
system of equations:

\beq
\left\{
\begin{array}{l}
\TIL{\rve}=\TIL{\rvr}-\TIL{\rvf}
\\
\TIL{\rvu}=\TIL{C}\TIL{\rve}
\\
\TIL{\rvy}=\TIL{\ul{\Pi}}\TIL{\rvu}
\\
\TIL{\rvf}=\TIL{F}\TIL{\rvy}
\end{array}
\right.
\eeq
From this system of
equations,
we find that

\beq
\TIL{y} =\TIL{\Pi}\TIL{C}
(\TIL{r}-
\underbrace{\TIL{f}}_{\TIL{F}\TIL{y}})
\eeq
Hence,
just as we found previously, the transfer function
$\TIL{H}_{y|r}(s)$ is given by

\beq
\TIL{H}_{y|r}(s)=
\frac{\TIL{y}}{\TIL{r}}
=
\frac{\TIL{\Pi}\TIL{C}}
{1+\TIL{\Pi}\TIL{C}\TIL{F}}
\eeq

SF graphs are 
the same as 
LDEN (Linear Deterministic
with External Noise) bnets discussed in
Chapter \ref{ch-linear-sys},
except for 2 important
differences: (1) SF graphs
have no external random nodes
(2)
unlike the LDEN considered in 
Chapter \ref{ch-linear-sys},
SF graphs can have feedback cycles.

In SF graphs, the
multiplicative factors
carried by the arrows  are called 
``gains".

Next, we shall discuss
some properties of the
feedback cycles of SG graphs.



An approach
that I like
is to 
re-express
an SF graph 
with feedback cycles
by one without them
that is easier to understand.

\begin{claim}
Getting rid of bubbles (i.e., self feedback cycles)
\beq
\begin{array}{ccc}
\xymatrix{
\rva\ar[dr]_{\alpha}
&
&\rvb\ar[dl]^{\beta}
\\
&\rvx\ar@(ul,ur)[]^\mu
}
&\xymatrix{\\=}&
\xymatrix{
\rva\ar[dr]_{\frac{\alpha}{1-\mu}}
&
&\rvb\ar[dl]^{\frac{\beta}{1-\mu}}
\\
&\rvx
}
\end{array}
\eeq
\end{claim}
\proof
From the left hand diagram,

\beq
\rvx=\mu\rvx + \alpha\rva + \beta\rvb
\eeq
Hence, 
\beq
\rvx =
\frac{\alpha}{1-\mu}\rva
+
\frac{\beta}{1-\mu}\rvb
\eeq
\qed

\begin{claim}
Getting rid of 2-node  cycles
\beq
\begin{array}{ccc}
\xymatrix{
\rva\ar[d]_\alpha
&\rvb\ar[d]^\beta
\\
\rvx\ar@/^1pc/[r]^\mu
&\rvy\ar@/^1pc/[l]^\nu
}
&
\xymatrix{\\=}
&
\xymatrix@C=6pc{
\rva\ar[d]
_{\frac{\alpha}{1-\mu\nu}}
\ar[rd]
^<<<<<{\frac{\mu\alpha}{1-\mu\nu}}
&\rvb\ar[d]
^{\frac{\beta}{1-\mu\nu}}
\ar[dl]
_<<<<<{\frac{\nu\beta}{1-\mu\nu}}
\\
\rvx
&\rvy
}
\end{array}
\eeq

\end{claim}
\proof
From the left hand  diagram,

\beq
\left\{
\begin{array}{l}
\nu \rvy + +\alp\rva =\rvx
\\
\beta\rvb + \mu\rvx = \rvy
\end{array}
\right.
\eeq
Hence,

\beq
\left[
\begin{array}{cc}
-1&\nu
\\
\mu&-1
\end{array}
\right]
\left[
\begin{array}{c}
\rvx
\\
\rvy
\end{array}
\right]
=
\left[
\begin{array}{c}
-\alpha \rva
\\
-\beta\rvb
\end{array}
\right]
\eeq
But

\beq
\left[
\begin{array}{cc}
-1&\nu
\\
\mu&-1
\end{array}
\right]^{-1}
=
\frac{1}{1-\mu\nu}
\left[
\begin{array}{cc}
-1&-\nu
\\
-\mu&-1
\end{array}
\right]
\eeq
so

\beq
\left[
\begin{array}{c}
\rvx
\\
\rvy
\end{array}
\right]
=
\frac{1}{1-\mu\nu}
\left[
\begin{array}{cc}
1&\nu
\\
\mu&1
\end{array}
\right]
\left[
\begin{array}{c}
\alpha\rva
\\
\beta\rvb
\end{array}
\right]
\eeq

\qed

We could continue by showing
how to eliminate cycles
with $3, 4\ldots$ cycle nodes,
but
the pattern is clear.
If there are $N$
cycle nodes $\rvl_1, \rvl_2,\cdots, \rvl_N$,
then
any arrow $\rva\rarrow \rvl_{k_0}$
in the feedback graph
is replaced by $N$ arrows
$\rva\rarrow \rvl_k$
for $k=1,2, \ldots,N$
in the non-feedback graph.
Let $G_{cycle}$
be the product
of the gains in the cycle.
If $\alpha_{k_0}$
is the gain of arrow
$\rva\rarrow\rvl_{k_0}$
in the feedback graph,
then the gain of $\rva\rarrow\rvl_k$
in the non-feedback graph equals
the product of the gains
in the path  $\rva\maparrow{\alpha_{k_0}}\rvl_{k_0}\rarrow\cdots\rarrow\rvl_k$
divided by $1-G_{cycle}$.


SG graphs 
with feedback cycles can be
used to represent a general
system of $N$ linear equation
with $N$ unknowns (i.e. $y=Cx$,
where $C$ is
an $N\times N$ matrix).
Fig.\ref{fig-flow-graph-3eqs}
shows an SF graph that
does this for $N=3$.


\begin{figure}[h!]
\centering
\includegraphics[width=3in]
{control-th/flow_graph_3eqs.png}
\caption{SF graph
that represents a general
system of 3 linear equations 
with 3 unknowns.}
\label{fig-flow-graph-3eqs}
\end{figure}

In Fig.\ref{fig-flow-graph-3eqs},
\beq
\begin{array}{l}
\rvx_1 = (c_{11}+1)\rvx_1 + c_{12}\rvx_2 
+c_{13}\rvx_3 -\rvy_1
\\
\rvx_2 = c_{21}\rvx_1 + (c_{22}+1)x_2
+c_{23}\rvx_3 -\rvy_2
\\
\rvx_3 = c_{31}\rvx_1 + c_{32}\rvx_2
+(c_{33}+1)\rvx_3 -\rvy_3
\end{array}
\eeq
so

\beq
\left[
\begin{array}{c}
\rvy_1
\\
\rvy_2
\\
\rvy_3
\end{array}
\right]
=
\left[
\begin{array}{ccc}
c_{11}&c_{12}&c_{13}
\\
c_{21}&c_{22}&c_{23}
\\
c_{31}&c_{32}&c_{33}
\end{array}
\right]
\left[
\begin{array}{c}
\rvx_1
\\
\rvx_2
\\
\rvx_3
\end{array}
\right]
\eeq
\beq
y=Cx
\eeq
Note that if 
we get rid of the cycles
by setting $c_{11}=c_{22}=c_{33}=-1$,
the {\it general}
system of 3 linear equations 
with 3 unknowns 
cannot be represented.
The cycles are necessary for
doing this.



\section{Laplace transform}
\label{sec-laplace-trans}

This section
is a watered down version
of the Wikipedia entry 
for
Laplace Transforms
(Ref.\cite{wiki-laplace-transform}), which we highly recommend.

Let $0^- = 0-\eps$,
$0^+=0+\eps$
for some $\eps\in \RR$
such that $0<\eps<<1$.

Let $s=\s + i\omega$
for $\s,\omega\in\RR$.
$\s$ is called
the {\bf decay 
constant}
and $\omega$
is called the
{\bf angular frequency})

The {\bf Laplace Transform (LT)} of $f:[0,\infty]\rarrow \CC$
is defined as
\beq 
\call[f](s)=\TIL{f}(s)=
\int_{0-}^\infty dt\; e^{-st} f(t)
\label{eq-def-lap-trans}
\eeq
Note that the LT is a linear functional\footnote{
A functional $\calf[f]$ is a function
of a function $f$,
or,
equivalently,
a function
of a vector
with possibly
infinitely
many components
given by $[f(x)]_{\forall x}$.}

\beq
\call[af+bg](t)
=
a\TIL{f}(s)
+ b\TIL{g}(s)
\eeq
for $f,g:[0, \infty]\rarrow \CC$
and $a,b\in\CC$.
For LTs,
we assume functions 
$f(t)$ that
vanish for $t<0^-$.
They can jump
to a finite value (with a step function) or
an infinite value 
(
with a Dirac delta function) 
at $t=0$, 
but must vanish for $t<0^-$.
LTs are ideally
suited for solving
ordinary
differential
equations
with {\bf initial conditions}
such as $x(0)=5$, $\partial_t x(0)=10$.




\begin{table}[h!]
\centering
\begin{tabular}{|l|l|l|}
\hline
\rowcolor[HTML]{ECF4FF} 
name & formula & comment \\ \hline
\begin{tabular}{l}
 {\bf Bilateral Laplace}\\ {\bf transform (BLT)} 
 \end{tabular}& $\calb[f](s)=\int_{-\infty}^\infty dt\; e^{-st} f(t)$ & \begin{tabular}[c]{@{}l@{}}same as LT but\\ with $-\infty<t<\infty$\\ instead of $t>0$\end{tabular} \\ \hline
{\bf Fourier transform (FT)} & $\calf[f](\omega)=\int_{-\infty}^\infty dt\; e^{-i\omega t} f(t)$ & \begin{tabular}[c]{@{}l@{}}Same as BLT but\\ with $s=i\omega\in i\RR$\end{tabular} 
\\ \hline
{\bf Star Transform (ST)} & $\call^*[f](s)=\sum_{n=0}^\infty  e^{-snT} f(nT)$ & \begin{tabular}[c]{@{}l@{}}Same as LT but\\ samples only\\ discrete points at
\\ $t=nT$\end{tabular} \\ \hline
\begin{tabular}{l}
{\bf Moment Generating}\\ 
{\bf Function}
\end{tabular} 
& 
\begin{tabular}{l}
$E_\rvx[e^{-s\rvx}]=\int_0^\infty
dx\;  e^{-sx} P(x)$\\
$\av{\rvx^n} = \left[(-\partial_\rvs)^n E_\rvt[e^{-s\rvx}]\right]_{s=0}$
\end{tabular}
 & \begin{tabular}[c]{@{}l@{}}Same as LT but\\ 
for probability \\ distribution
\\
$P:[0,\infty]\rarrow[0,1]$
\end{tabular} \\ \hline
\end{tabular}
\caption{Transforms that are akin to the Laplace transform.
Don't be intimidated by all these
transforms. 
They are all 
just 
fancy
dot products
like $\vec{a}\cdot\vec{b}$.}
\label{tab-akin-to-LT}
\end{table}
See Table
\ref{tab-akin-to-LT}.
If the function $f(t)$
does not vanish for $t<0$,
we can use the
{\bf Bilateral Laplace Transform (BLT)}. 
The BLT gives the
{\bf Fourier transform (FT)} when 
$s=i\omega$.
In this section,
we will only discuss the LT.

The following
intuition
about LTs might
be helpful to th reader.
A LT is like 
a dot product of two vectors,
$e^{-st}$ and $f(t)$,
except that
in this case
the index $t$ for
their components
is a non-countable set $[0,\infty)$.
As with all
dot products, its maximum
is achieved 
if the two vectors 
point in the same
direction (this is
what the Cauchy Schwartz
inequality 
$\vec{a}.\vec{b}=
|\vec{a}||\vec{b}|\cos\theta
\leq |\vec{a}||\vec{b}|$
says).
In this case, if we substitute
$f(t) =\TIL{f}(s_0)e^{s_0t}$
on the right hand side
of Eq.(\ref{eq-def-lap-trans}),
we get  

\beqa
\TIL{f}(s)
&=&
\TIL{f}(s_0)
\int_{0^-}^\infty
dt\; e^{-(s-s_0)t}
\\
&=&
\TIL{f}(s_0)\delta(s-s_0)
\eeqa
So our intuition
is this:
whenever you see
an equation
involving LTs, 
replace each $f(t)$
by the special case
$f(t)= e^{s_ot}\TIL{f}$
(this is called a {\bf phasor}
when $s_0 = i\omega_0$),
and convince
yourself that
that the equation
is valid 
in the special case of phasors.





The {\bf Inverse
Laplace Transform}
is defined so that

\beq \call^{-1}[\underbrace{\call[f]}_{\TIL{f}}](t)
= f(t)
\eeq

Next, we 
will list
some of the
many
properties of 
LTs.
Henceforth,
we will
use the following
notation:

$a, b\in\CC$

$f,g:[0,\infty]\rarrow \CC$

$f(t)\maparrow{\call} \TIL{f}(s)$

Define {\bf Dirac delta function} by

\beq
\delta(t) =\int_{-\infty}^{\infty}
d\omega\; e^{i\omega t}
=
\left\{
\begin{array}{ll}
\infty & \text{if $t=0$}
\\
0 &\text{otherwise}
\end{array}
\right.
\eeq
and the
{\bf Heaviside step function} by

\beq
step_a(t)=
\indi(t-a>0)
\eeq


\begin{itemize}

\item
Dirac delta function

\beq
\delta(t-a)
\maparrow{\call}
e^{-sa}
\eeq

\item 
Heaviside step function



\beq
step_a(t)
\maparrow{\call}
\frac{1}{s}
e^{-sa}\quad \text{(for $Re(s)>0$)}
\eeq

\item box

\beq
step_0(t) - step_a(t)
\maparrow{\call} \frac{1}{s}
(1-e^{-sa})\quad\text{(for $Re(s)>0$)}
\eeq

\item ramp

\beq
t\;step_0(t)
\maparrow{\call}
\frac{1}{s^2}
\quad\text{(for $Re(s)>0$)}
\eeq

\item sine, cosine

\beq
\sin(at)step_0(t) \maparrow{\call}
\frac{a}{s^2+a^2}
\eeq

\beq
\cos(at)step_0(t) \maparrow{\call}
\frac{s}{s^2+a^2}
\eeq

\item
polynomial rise, exponential drop

\beq
\frac{t^n}{n!}e^{-at}\;step_0(t)
\maparrow{\call}
\frac{1}{(s+a)^{n+1}}
\quad\text{(for $Re(s)>-a$)}
\eeq

\item Exponential approach to steady state

\beq
(1-e^{-at})step_0(t)
\maparrow{\call}
\frac{a}{s(s+a)}
\quad \text{(for $Re(s)>0, Re(s)>-a$)}
\eeq



\item Taylor series of $f(t)$

\beq
\int_0^\infty
dx\;e^{ -x} \frac{x^n}{n!}
=1
\eeq

\beq
\int_0^\infty dt\;e^{-st} \frac{t^n}{n!}
=\frac{1}{s^{n+1}}
\eeq

\beq
\frac{t^n}{n!}step_0(t)
\maparrow{\call}\frac{1}{s^{n+1}}
\eeq

\beqa
f(t)step_0(t)&=&
step_0(t)
\sum_{n=0}^\infty
\frac{t^n}
{n!}\partial_t^n f(0)
\\
&\maparrow{\call}&
\sum_{n=0}^\infty \frac{1}{s^{n+1}}
\partial_t^n f(0)
\eeqa

\item
derivatives of $\TIL{f}(s)$

\beq
(-t)f(t)
\maparrow{\call}
 \partial_s
\TIL{f}(s)
\eeq

\beq
(-t)^kf(t)
\maparrow{\call} (\partial_s)^k
\TIL{f}(s)
\eeq

\item
derivatives of $f(t)$

Define

\beq
f^{\geq 1}(t)
=f(t) - 
f(0^+)
\eeq


\beq
f^{\geq 2}(t)
=f(t) - 
f(0^+) - t f'(0^+)
\eeq

\beq
\partial_t f(t)
\maparrow{\call}
s
\TIL{f^{\geq 1}}(s)
\quad \text{(a.k.a. $f(t)$ {\bf differentiator})}
\eeq


\beq
(\partial_t)^2 f(t)
\maparrow{\call}
s^2
\TIL{f^{\geq 2}}(s)
\eeq

\item
integral of $f(t)$ (a.k.a. $f(t)$ 
{\bf integrator})

\beq 
\int_0^t d\tau\; f(\tau)
\maparrow{\call}
\frac{1}{s}\TIL{f}(s)
\eeq
Note that

\beqa
\int_0^t d\tau\; f(\tau)
&=&
\int_{-\infty}^\infty d\tau\; f(\tau)
step_0(\tau)step_0(t-\tau)
\\
&=&
((f step_0)\circledast  step_0)(t)
\eeqa


\item
integral of $\TIL{f}(s)$

\beq \frac{1}{t}f(t)
\maparrow{\call}
\int_s^\infty d\s\; \TIL{f}(\s)
\eeq

\item shifting $\TIL{f}(s)$ (frequency shifting)



\beq
e^{at}f(t)\maparrow{\call} \TIL{f}(s-a)
\quad\text{(for  $a>0$)}
\eeq



\item shifting $f(t)$ (time shifting).



\beq
f(t-a)\;step_a(t)\maparrow{\call}
e^{-as}\TIL{f}(s)
\quad\text{(for  $a>0$)}
\eeq


\item time scaling

\beq
f(at)
\maparrow{\call}
\frac{1}{a}
\TIL{f}
\left(\frac{s}{a}\right)
\quad\text{(for  $a>0$)}
\eeq

\item multiplication

\beq
f(t)g(t)
\maparrow{\call}
\frac{1}{i2\pi}
\lim_{T\rarrow \infty}
\int_{c-iT}^{c+iT}
ds'\;
\TIL{f}(s')
\TIL{g}(s-s')
\eeq


\item convolution $(f\circledast  g)(t)$

The convolution
of $f:\RR\rarrow\CC$
and $g:\RR\rarrow\CC$
is defined by
\beq
(f\circledast  g)(t) = \int_{-\infty}^\infty
d\tau\; f(\tau)g(t-\tau)
\eeq
If $f(t)=g(t)=0$
for $t<0$,

\beq
(f\circledast  g)(t) = \int_{0}^t
d\tau\; f(\tau)g(t-\tau)
\quad
\text{(see Fig.\ref{fig-convolution}.)}
\label{eq-conv-left-half-zero}
\eeq

\begin{figure}[h!]
\centering
\includegraphics[width=2.5in]
{conventions/convolution.png}
\caption{Pictorial
representation
of the convolution
$(f\circledast g)(t)$. }
\label{fig-convolution}
\end{figure}

It's not hard to show that

\beq
f\circledast g = g\circledast f
\eeq
and that

\beq
(f\circledast  g)(t) \maparrow{\call}\TIL{f}(s)\TIL{g}(s)
\label{eq-lt-conv}
\eeq
Eq.(\ref{eq-lt-conv})
is easy to check
with phasors.
Indeed, if we substitute
$f(\tau)=e^{i\omega_0\tau}\TIL{f}$
and
$g(t-\tau)=
e^{i\omega_0(t-\tau)}\TIL{g}$,
on the right hand side
of Eq.(\ref{eq-conv-left-half-zero}),
the right
hand side
becomes $e^{i\omega_0t}\TIL{f}
\TIL{g}$,
and the LT of that
is $\TIL{f}
\TIL{g}$.

A common question
is how does one 
evaluate
convolutions in practice.
If one can sample and remember
the waveforms $f(\tau)$
and $g(\tau)$
for all $\tau\in[0,t]$,
then it's just a matter 
of multiplication
and addition of samples.
Sometimes, even if we
have no memory resources,
it's possible to calculate a convolution. For example,
if $g(t)=e^{st}step_0(t)$

\beqa
(f\circledast g)(t)
&=&
\int_0^t d\tau\;
f(\tau)e^{s(t-\tau)}
\\
&=&
\underbrace{e^{st}}_
{g(t)}\TIL{f}(s)
\eeqa
so convolving this $g(\cdot)$
merely evaluates it at $t$
and multiplies it
by a constant $\TIL{f}(s)$.

\item circular convolution

$f_T, g_T$ 
periodic functions
with period $T$

\beq
(f_T\circledast_C g_T)(t)
=
\int_0^Td\tau\;
f_T(\tau)g_T(t-\tau)
\eeq

\beq
(f_T\circledast_C g_T)(t)
\maparrow{\call}
\TIL{f_T}(s) \TIL{g_T}(s)
\eeq

\item complex 
conjugation
\beq
f^*(t)\maparrow{\call}\TIL{f}^*(s^*)
\eeq

\item cross correlation

\beq
(f,g)_{CC}=
\int_0^\infty
d\tau\;
f^*(\tau)
g(t+\tau)
\eeq

\beq
(f,g)_{CC}
\maparrow{\call}
\TIL{f}^*(-s^*)\TIL{g}(s)
\eeq

\item $f_T(t)$ periodic 
with period $T$

\beq
\cali_a^b=
\int_a^b dt\;
e^{-st}f_T(t)
\eeq

\beqa
\TIL{f_T}(s)
&=& \cali_0^T + \cali_{T}^{2T}
+
\cali_{2T}^{3T}+\cdots
\\
&=&
\cali_0^T(1 + e^{-sT} + e^{-s2T} +\cdots)
\\
&=&
\frac{1}{1-e^{-sT}}\cali_0^T
\eeqa

\beq
f_T(t)\maparrow{\call}
\frac{1}{1-e^{-sT}}
\int_0^T dt\;
e^{-st}f_T(t)
\eeq

\item
periodic summation

\beq
\sum_{n=0}^\infty
f(t-nT)u_0(t-nT)
\maparrow{\call}
\frac{1}{1{\color{red}-}e^{-Ts}}
\TIL{f}(s)
\eeq

\beq
\sum_{n=0}^\infty
{\color{red}(-1)^n}
f(t-nT)u_0(t-nT)
\maparrow{\call}
\frac{1}{1{\color{red}+}e^{-Ts}}
\TIL{f}(s)
\eeq

\item limits of $f(t)$


\beqa
\lim_{s\rarrow \infty}s\TIL{f}(s)
&=&
\lim_{s\rarrow \infty}s\int_0^\infty dt\; e^{-st}f(t)
\\
&\approx& 
f(0^+)\lim_{s\rarrow \infty}
\underbrace{s\int_0^\infty dt\; e^{-st}
}_{=1}
\\
&=&
f(0^+)
\eeqa

\beq
\lim_{s\rarrow 0}s\TIL{f}(s)
= f(\infty)
\eeq

\item Inverse LT

The inverse 
LT of a function
$\TIL{f}(s)$
can be calculated 
by performing
the
following
complex contour integral:

\beq
\underbrace{\call^{-1}[\TIL{f}(s)](t)}_
{f(t)} = \frac{1}{2\pi i}
\lim_{T\rarrow \infty}
\int_{\gamma-iT}^{\gamma+iT}ds\;
e^{st}\TIL{f}(s)
\eeq
Another way of calculating
the inverse LT of $\TIL{f}(s)$, is
to express
$\TIL{f}(s)$ as a linear combination
of functions for which the inverse LT
is known from LT tables. For instance,

\begin{align}
\call^{-1}
\left[
\frac{1}{s(s-1)}
\right]
=&
\call^{-1}
\left[
\frac{1}{s} - \frac{1}{s+1}
\right] \text{(partial fractions expansion)}
\\
=&
\call^{-1}
\left[
\frac{1}{s}\right]
 - \call^{-1}
 \left[\frac{1}{s+1}\right]
 \\
 =&
 u_0(t)[1-e^{-t}]
\end{align}

\item {Bode, Nyquist plots}

$s=\s+i\omega$

{\bf Bode plot}: plot of
$$(\log_{10}(\omega),
|\TIL{f}(i\omega)|)$$ and,
right below it, plot of
$$(\log_{10}(\omega),
phase\{\TIL{f}(i\omega)\})$$.

{\bf Nyquist plot}: plot of
$$(Re \TIL{f}(i\omega), Im \TIL{f}(i\omega))$$
or, equivalently,
plot of

$$(|\TIL{f}(i\omega)|, phase(\TIL{f}(i\omega)))$$
on polar graph paper.

Usually, $\TIL{f}(i\omega)$
is a gain (i.e.,  LT
of output
divided by LT
of input).



\beq
\begin{array}{c|c|c|c}
f(t)&\TIL{f}(i\omega)
&|\TIL{f}(i\omega)|&
phase(\TIL{f}(i\omega))
\\ \hline\hline
\delta(t)
&1
&1
&0
\\
step_0(t)
&
\frac{1}
{i\omega}
&\frac{1}{|\omega|}
&-\frac{\pi}{2}
\\
t\;step_0(t)
&
\frac{1}
{(i\omega)^2}
&
\frac{1}{\omega^2}
&
-\pi
\\
\frac{t^2}{2} step_a(t)
&
\frac{1}
{(i\omega)^3 }
&
\frac{1}{|\omega|^3}
&
\frac{\pi}{2}
\end{array}
\eeq

the narrower $f(t)$ is,
the broader $|\TIL{f}(i\omega)|$ is.
The more 
$f(t)$ is a high pass filter,
the more 
$|\TIL{f}(i\omega)|$
is a low pass filter.



\end{itemize}

\section{Z-transform}
\label{sec-z-transform}

This section
is a watered down version
of the Wikipedia entry 
for
Z-transforms 
(Ref.\cite{wiki-z-transform}), which we highly recommend.
Before reading 
this section,
we recommend that the 
reader read Section \ref{sec-laplace-trans}
on Laplace transforms,
as those are usually
taught first.

Suppose $x^{[n]}\in\CC$
for all $n\in \ZZ^{\geq 0}$ ($\ZZ^{\geq 0}=$non-negative
integers), and $z\in \CC$. Then 
we define the {\bf Z-transform (ZT)} by
\beq
\calz{[x]}(z)=
\TIL{x}(z) =
\sum_{n=0}^\infty x^{[n]} z^{-n}
\eeq
Note that the ZT is a linear functional

\beq
\calz[a x^{[n]} + b y^{[n]}]=
a \TIL{x}(z) + b\TIL{y}(z)
\eeq
for $x^{[n]}, y^{[n]}\in\CC$
for all $n\in \ZZ^{\geq 0}$ non-negative
integers, and $a,b\in\CC$.

\begin{table}[h!]
\centering
\begin{tabular}{|l|l|l|}
\hline
\rowcolor[HTML]{ECF4FF} 
name & formula & comment \\ \hline
\begin{tabular}{l}
{\bf Bilateral}
\\{\bf ZT (BZT)}
\end{tabular} & $\TIL{x}(z)=\sum_{n=-\infty}^\infty x^{[n]} z^{-n}$ & \begin{tabular}[c]{@{}l@{}}Same as ZT but\\ with $n\in \ZZ$\\ instead of \\ $n\in\ZZ^{\geq 0}$\end{tabular} \\ \hline
\begin{tabular}{l}
{\bf Discrete time Fourier}\\
{\bf transform (DTFT)}
\end{tabular} & $\TIL{x}_{2\pi}(\omega)=\sum_{n=-\infty}^{\infty}x^{[n]}e^{-i\omega n}$ & \begin{tabular}[c]{@{}l@{}}same as BZT but \\ with $z = e^{i\omega}$\end{tabular} \\ \hline
\begin{tabular}{l}
{\bf Discrete Fourier}
\\
{\bf transform (DFT)}
\end{tabular} & $\TIL{x}^{[k]}=\sum_{n=0}^{N-1}x^{[n]} e^{-i \frac{2\pi kn}{N}}$ & \begin{tabular}[c]{@{}l@{}}Same as  ZT but a\\finite ($N$) number of\\ $x^{[n]}$ components, and\\ with $z=e^{i\frac{2\pi k}{N}}$
\\
(N roots of unity\\
on unit circle)\end{tabular} \\ \hline
\begin{tabular}{l}
{\bf Probability}\\
{\bf Generating Function}
\end{tabular} & $\TIL{P}(z)=\sum_{n=0}^{\infty}P^{[n]}
z^n$ & \begin{tabular}[c]{@{}l@{}}same as ZT but \\ with $s\rarrow -s$
and \\
$P^{[n]}:\ZZ^{\geq 0} \rarrow [0,1]$
\\
is a discrete prob.
\\
distribution. If \\
$z=e^{-T}$, get moment\\
genetating function.
\end{tabular} \\
\hline
\end{tabular}
\caption{Transforms that are akin to the Z-transform.
Don't be intimidated by all these
transforms. 
They are all 
just 
fancy
dot products
like $\vec{a}\cdot\vec{b}$.}
\label{tab-akin-to-ZT}
\end{table}
See Table \ref{tab-akin-to-ZT}.


\beq
e^{-st}=z^{-n}
\eeq

\beq
t=nT
\eeq

\beq
\boxed{
z= e^{sT}}
\eeq

\beq
s=\s +i\omega, \quad
z = re^{i\theta} 
\eeq

\beq
r =e^{\s T}
,\quad
\theta =\omega T
\eeq

\begin{figure}[h!]
\centering
\includegraphics[width=5in]
{conventions/s-z-planes.png}
\caption{Relationship
between the s-plane (for Laplace transform)
and z-plane (for Z-transform.)}
\label{fig-s-z-planes}
\end{figure}

As shown in Fig.\ref{fig-s-z-planes},
the map
from the s-plane (for
Laplace transforms)
to the z-plane (for
Z-transforms) maps:


left half plane $\rarrow$
inside of the unit
circle

real axis $\rarrow$ unit circle

right half plane $\rarrow$ outside
of unit circle

Discrete unit step
\beq
u_j^{[n]}=\indi(n\geq j)
\eeq

Kronecker delta function
\beq
\delta_j^{[n]} = \indi(n=j)
\eeq


Region of
convergence (ROC)
for a Z-transform is very important
(can't tell 
from the transform formula alone
without knowing the
time series it came from). Won't list it here, but it
can be found in Ref.\cite{wiki-z-transform}
along with proofs.

\begin{mdframed}[hidealllines=true,backgroundcolor=gray!10]
Note: ZT  formulae
can be checked by
replacing
$z=e^{sT}$
for $0<T<<1$
and checking that
\beq 
T \calz[x](e^{sT})\approx 
\call[f](s)
\eeq
\end{mdframed}



\begin{itemize} 

\item Kronecker delta function

\beq
\delta_{n_0}^{[n]}
\maparrow{\calz}
z^{-n_0}
\eeq

Compare this with

\beq
\delta(t-a)
\maparrow{\call} e^{-sa}
\eeq
with $a=n_0T$
and $z=e^{sT}$.


\item unit step

\beq
a^n u_0^{[n]}
\maparrow{\calz}
\frac{1}{1-az^{-1}}
\quad \text{for $|z|>|a|$}
\eeq


\beq
-a^n u_0^{[-n-1]}
\maparrow{\calz}
\frac{1}{1-az^{-1}}
\quad \text{for $|z|<|a|$}
\eeq

Compare this with

\beq
step_0(t)
\maparrow{\call}
\frac{1}{s}
\quad \text{\text{(for $Re(s)>0$)}}
\eeq
for $a=1$, $z=e^{sT}\approx 1 + sT$.


\item ramp

\beq
n a^n u_0^{[n]}
\maparrow{\calz}
\frac{az^{-1}}{(1-az^{-1})^2}
\quad \text{for $|z|>|a|$}
\eeq


\beq
-n a^n u_0^{[-n-1]}
\maparrow{\calz}
\frac{az^{-1}}{(1-az^{-1})^2}
\quad \text{for $|z|<|a|$}
\eeq

\item sine, cosine

\beq
a^n\sin(\omega_0n)u_0^{[n]}
\maparrow{\calz}
\frac{az^{-1}\sin\omega_0}
{1-2az^{-1}\cos\omega_0 + a^2 z^{-2}}
\eeq

\beq
a^n\cos(\omega_0n)u_0^{[n]}
\maparrow{\calz}
\frac{1-az^{-1}\cos\omega_0}
{1-2az^{-1}\cos\omega_0 + a^2 z^{-2}}
\eeq

\item time expansion

\beq
x^{[n/K]}\indi(n/K\in\ZZ)
\maparrow{\calz}
\TIL{x}(z^K)
\eeq

\item decimation
\beq
x^{[Kn]}
\maparrow{\calz}
\frac{1}{K}
\sum_{p=0}^{K-1}
\TIL{x}\left(
z^{\frac{1}{K}}
e^{-i 2\pi \frac{p}{K}}
\right)
\eeq

\item time delay

\beq
x^{[n-k]}u_0^{[n]}
\maparrow{\calz}
z^{-k}\TIL{x}(z) 
\quad \text{(for $k>0$)}
\eeq

\item time advance

\beq
x^{[n+k]}
\maparrow{\calz}
z^{k}
\left(
\TIL{x}(z)
-z^k\sum_{n=0}^{k-1}
x^{[n]}z^{-n}
\right)
\quad \text{(for $k>0$)}
\eeq

\item first difference backwards

\beq
x^{[n]}u_0^{[n]}
-x^{[n-1]}u_0^{[n-1]}
\maparrow{\calz}
(1-z^{-1})\TIL{x}(z)
\eeq

\item first difference forward

\beq
x^{[n+1]}u_0^{[n+1]}
-x^{[n]}u_0^{[n]}
\maparrow{\calz}
z\left( (1-z^{-1})
\TIL{x} -x^{[0]}
\right)
\eeq  

\item time reversal

\beq
x^{[-n]}
\maparrow{\calz}
\TIL{x}(z^{-1})
\eeq

\item scaling in z-domain

\beq
a^n x^{[n]}
\maparrow{\calz}
\TIL{x}(a^{-1}z)
\eeq

\item complex conjugation

\beq
(x^{[n]})^*
\maparrow{\calz} \TIL{x}^*(z^*)
\eeq

\beq
Re(x^{[n]})
\maparrow{\calz}
\frac{1}{2}
(\TIL{x}(z)+\TIL{x}^*(z^*))
\eeq

\beq
Im(x^{[n]})
\maparrow{\calz}
\frac{1}{2i}
(\TIL{x}(z)-\TIL{x}^*(z^*))
\eeq

\item $\TIL{x}(z)$ differentiation

\beq
nx^{[n]}
\maparrow{\calz}
-z\partial_z \TIL{x}(z)
\eeq

\item convolution

\beq
x_1^{[n]}\circledast x_2^{[n]}=
\sum_{k=0}^nx_1^{[k]}x_2^{[n-k]}
\eeq

\beq
x_1^{[n]}\circledast x_2^{[n]}
\maparrow{\calz}
\TIL{x}_1(z)\TIL{x}_2(z)
\eeq

\item cross-correlation

\beq
(x_1^{[-n]})^*\circledast x_2^{[n]}
\maparrow{\calz}
\TIL{x_1}^*\left(\frac{1}{z^*}\right))
\TIL{x}_2(z)
\eeq

\item accumulation

\beq
\sum_{k=-\infty}^\infty
x^{[k]}
\maparrow{\calz}
\frac{1}{1-z^{-1}}
\TIL{x}(z)
\eeq

\item multiplication

\beq
x_1^{[n]} x_2^{[n]}
\maparrow{\calz}
\frac{1}{i2\pi}
\oint_C \frac{dw}{w}\;
\TIL{x}_1(w)
\TIL{x}_2\left(
{\color{red}\frac{z}{w}}
\right)
\eeq

\item Parseval's theorem

\beq
\sum_{k=-\infty}^{\infty}
x_1^{[n]}(x_2^{[n]})^*
=
\frac{1}{i2\pi}
\oint_C \frac{dw}{w}\;
\TIL{x}_1(w)
\TIL{x}_2\left(
{\color{red}
\frac{1}{w^*}}
\right)
\eeq

\item limits of $x^{[n]}$

initial value theorem

\beq
x^{[0]}=\lim_{z\rarrow \infty}
\TIL{x}(z)
\eeq

final value theorem

\beq
x^{[\infty]}=\lim_{z\rarrow 1}
(z-1)\TIL{x}(z)
\eeq

\end{itemize}


